2 Problem Formulation  A Continual Learning Prediction (CLP) problem consists of an unending stream of samples  T = (X1, Y1), (X2, Y2), . . . , (Xt, Yt), . . . for inputs Xt and prediction targets Yt, from sets X and Y respectively.2 The random vector Yt is sampled according to an unknown distribution p(Y |Xt). We assume the process X1, X2, . . . , Xt, . . . has a marginal distribution µ : X → [0, ∞), that reﬂects how often each input is observed. This assumption allows for a variety of correlated sequences. For example, Xt could be sampled from a distribution  2This deﬁnition encompasses the continual learning problem where the tuples also include task descriptors Tt (Lopez-Paz and Ranzato, 2017). Tt in the tuple (Xt, Tt, Yt) can simply be considered as part of the inputs.  2  Representation Learning Network (RLN)x1x2xn...InputPrediction Learning Network (PLN)LearnedrepresentationyOutputMeta-parameters(Only updated in the outer loopduring meta-training)Adaptation Parameters(Updated in the inner loop and at meta-testing)...r1r2r3r4NetworkConnectionsCould be any diﬀerentiable layer e.g a conv layer + relu or fc layer + relurdNetworkConnectionsNetworkConnectionsNetworkConnectionsNetworkConnectionsNetworkConnectionsNetworkConnections Figure 2: Effect of the representation on continual learning, for a problem where targets are generated from three different distributions p1(Y |x), p2(Y |x) and p3(Y |x). The representation results in different solution manifolds for the three distributions; we depict two different possibilities here. We show the learning trajectory when training incrementally from data generates ﬁrst by p1, then p2 and p3. On the left, the online updates interfere, jumping between distant points on the manifolds. On the right, the online updates either generalize appropriately—for parallel manifolds—or avoid interference because manifolds are orthogonal.  potentially dependent on past variables Xt−1 and Xt−2. The targets Yt, however, are dependent only on Xt, and not on past Xi. We deﬁne Sk = (Xj+1Yj+1), (Xj+2Yj+2) . . . , (Xj+k, Yj+k), a random trajectory of length k sampled from the CLP problem T . Finally, p(Sk|T ) gives a distribution over all trajectories of length k that can be sampled from problem T . For a given CLP problem, our goal is to learn a function fW,θ that can predict Yt given Xt. More concretely, let (cid:96) : Y × Y → R be the function that deﬁnes loss between a prediction ˆy ∈ Y and target y as (cid:96)(ˆy, y). If we assume that inputs X are seen proportionally to some density µ : X → [0, ∞), then we want to minimize the following objective for a CLP problem: (cid:90) (cid:20)(cid:90)  (cid:21)  LCLP (W, θ) def= E[(cid:96)(fW,θ(X), Y )] =  (cid:96)(fW,θ(x), y)p(y|x)dy  µ(x)dx.  (1)  where W and θ represent the set of parameters that are updated to minimize the objective. To minimize LCLP , we limit ourselves to learning by online updates on a single k length trajectory sampled from p(Sk|T ). This changes the learning problem from the standard iid setting – the agent sees a single trajectory of correlated samples of length k, rather than getting to directly sample from p(x, y) = p(y|x)µ(x). This modiﬁcation can cause signiﬁcant issues when simply applying standard algorithms for the iid setting. Instead, we need to design algorithms that take this correlation into account.  A variety of continual problems can be represented by this formulation. One example is an online regression problem, such as predicting the next spatial location for a robot given the current location; another is the existing incremental classiﬁcation benchmarks. The CLP formulation also allows for targets Yt that are dependent on a history of the most recent m observations. This can be obtained by deﬁning each Xt to be the last m observations. The overlap between Xt and Xt−1 does not violate the assumptions on the correlated sequence of inputs. Finally, the prediction problem in reinforcement learning—predicting the value of a policy from a state—can be represented by considering the inputs Xt to be states and the targets to be sampled returns or bootstrapped targets.  3 Meta-learning Representations for Continual Learning  Neural networks, trained end-to-end, are not effective at minimizing the CLP loss using a single trajectory sampled from p(Sk|T ) for two reasons. First, they are extremely sample-inefﬁcient, requiring multiple epochs of training to converge to reasonable solutions. Second, they suffer from catastrophic interference when learning online from a correlated stream of data (French, 1991). Metalearning is effective at making neural networks more sample efﬁcient (Finn et al., 2017). Recently, Nagabandi et al. (2019); Al-Shedivat et al. (2018) showed that it can also be used for quick adaptation from a stream of data. However, they do not look at the catastrophic interference problem. Moreover,  3  Solution Manifold for Task 1Solution manifolds in arepresentation space notoptimized for continual learningJoint Training SoluionParameter SpaceSolution manifolds in arepresentation space ideal for continual learningWWp1p2p3p1p2p3 their work meta-learns a model initialization, an inductive bias we found insufﬁcient for solving the catastrophic interference problem (See Appendix C.1).  To apply neural network to the CLP problem, we propose meta-learning a function φθ(X) – a deep Representation Learning Network (RLN) parametrized by θ – from X → Rd. We then learn another function gW from Rd → Y, called a Prediction Learning Network (PLN). By composing the two functions we get fW,θ(X) = gW (φθ(X)), which constitute our model for the CLP tasks as shown in Figure 1. We treat θ as meta-parameters that are learned by minimizing a meta-objective and then later ﬁxed at meta-test time. After learning θ, we learn gW from Rd → Y for a CLP problem from a single trajectory S using fully online SGD updates in a single pass. A similar idea has been proposed by Bengio et al. (2019) for learning causal structures. For meta-training, we assume a distribution over CLP problems given by p(T ). We consider two meta-objectives for updating the meta-parameters θ. (1) MAML-Rep, a MAML (Finn et al., 2017) like few-shot-learning objective that learns an RLN instead of model initialization, and OML (Online aware Meta-learning) – an objective that also minimizes interference in addition to maximizing fast adaptation for learning the RLN. Our OML objective is deﬁned as:  (cid:88)  OML(W, θ) def= (cid:88)  minW,θ  Ti∼p(T )  Ti∼p(T )  j  k∼p(Sk|Ti) S  (cid:88)  (cid:104)  (cid:16)U (W, θ, S  j  k)(cid:105)  LCLPi  (2)  k = (X ij+1Y ij+1), (X ij+2Y ij+2), . . . , (X ij+kY ij+k). U (Wt, θ, Sj  where Sj k) = (Wt+k, θ) represents an update function where Wt+k is the weight vector after k steps of stochastic gradient descent. The jth update step in U is taken using parameters (Wt+j−1, θ) on sample (X it+j, Y it+j) to give (Wt+j, θ). MAML-Rep and OML objectives can be implemented as Algorithm 1 and 2 respectively, with the primary difference between the two highlighted in blue. Note that MAML-Rep uses the complete batch of data Sk to do l inner updates (where l is a hyper-parameter) whereas OML uses one data point from Sk for one update. This allows OML to take the effects of online continual learning – such as catastrophic forgetting – into account. Algorithm 1: Meta-Training : MAML-Rep Require: p(T ): distribution over CLP problems Require: α, β: step size hyperparameters Require: l: No of inner gradient steps 1: randomly initialize θ 2: while not done do 3: 4: 5: 6: W0 = W 7: 8: 9: 10: 11: 12: end while  The goal of the OML objective is to learn representations suitable for online continual learnings. For an illustration of what would constitute an effective representation for continual learning, suppose that we have three clusters of inputs, which have signiﬁcantly different p(Y |x), corresponding to p1, p2 and p3. For a ﬁxed 2-dimensional representation φθ : X → R2, we can consider the manifold of solutions W ∈ R2 given by a linear model that provide equivalently accurate solutions for each pi. These three manifolds are depicted as three different colored lines in the W ∈ R2 parameter space in Figure 2. The goal is to ﬁnd one parameter vector W that is effective for all three distributions by learning online on samples from three distributions sequentially. For two different representations, these manifolds, and their intersections can look very different. The intuition is that online updates from a W are more effective when the manifolds are either parallel—allowing for positive generalization—or orthogonal—avoiding interference. It is unlikely that a representation producing such manifolds would emerge naturally. Instead, we will have to explicitly ﬁnd it. By taking into account the effects of online continual learning, the OML objective optimizes for such a representation.  end for Sample Stest from p(Sk|Ti) Update θ ← θ − β∇θ(cid:96)i(fθ,Wl (Stest[:, 0]), Stest[:, 1])  randomly initialize W Sample CLP problem Ti ∼ p(T ) Sample Strain from p(Sk|Ti) for j in 1, 2, . . . , l do  Wj = Wj−1 − α∇Wj−1 (cid:96)i(fθ,Wl (Strain[:, 0]), Strain[:, 1])  We can optimize this objective similarly to other gradient-based meta-learning objectives. Early work on learning-to-learn considered optimizing parameters through learning updates themselves, though typically considering approaches using genetic algorithms (Schmidhuber, 1987). Improvements  4   in automatic differentiation have made it more feasible to compute gradient-based meta-learning updates (Finn, 2018). Some meta-learning algorithms have similarly considered optimizations through multiple steps of updating for the few-shot learning setting (Finn et al., 2017; Li et al., 2017; Al-Shedivat et al., 2018; Nagabandi et al., 2019) for learning model initializations. The successes in these previous works in optimizing similar objectives motivate OML as a feasible objective for Meta-learning Representations for Continual Learning.  4 Evaluation  In this section, we investigate the question: can we learn a representation for continual learning that promotes future learning and reduces interference? We investigate this question by meta-learning the representations ofﬂine on a meta-training dataset. At meta-test time, we initialize the continual learner with this representation and measure prediction error as the agent learns the PLN online on a new set of CLP problems (See Figure 1).  4.1 CLP Benchmarks  We evaluate on a simulated regression problem and a sequential classiﬁcation problem using real data.  Algorithm 2: Meta-Training : OML Require: p(T ): distribution over CLP problems Require: α, β: step size hyperparameters 1: randomly initialize θ 2: while not done do 3: 4: 5: 6: W0 = W 7: 8: 9: 10: 11: 12: 13: end while  randomly initialize W Sample CLP problem Ti ∼ p(T ) Sample Strain from p(Sk|Ti) for j = 1, 2, . . . , k do (Xj, Yj) = Strain[j] Wj = Wj−1 − α∇Wj−1(cid:96)i(fθ,Wj−1 (Xj), Yj)  end for Sample Stest from p(Sk|Ti) Update θ ← θ − β∇θ(cid:96)i(fθ,Wk (Stest[:, 0]), Stest[:, 1])  Incremental Sine Waves: An Incremental Sine Wave CLP problem is deﬁned by ten (randomly generated) sine functions, with x = (z, n) for z ∈ [−5, 5] as input to the sine function and n a one-hot vector for {1, . . . , 10} indicating which function to use. The targets are deterministic, where (x, y) corresponds to y = sinn(z). Each sine function is generated once by randomly selecting an amplitude in the range [0.1, 5] and phase in [0, π]. A trajectory S400 from the CLP problem consists of 40 mini-batches from the ﬁrst sine function in the sequence (Each mini-batch has eight elements), and then 40 from the second and so on. Such a trajectory has sufﬁcient information to minimize loss for the complete CLP problem. We use a single regression head to predict all ten functions, where the input id n makes it possible to differentiate outputs for the different functions. Though learnable, this input results in signiﬁcant interference across different functions.  Split-Omniglot: Omniglot is a dataset of over 1623 characters from 50 different alphabets (Lake et al., 2015). Each character has 20 hand-written images. The dataset is divided into two parts. The ﬁrst 963 classes constitute the meta-training dataset whereas the remaining 660 the meta-testing dataset. To deﬁne a CLP problem on this dataset, we sample an ordered set of 200 classes (C1, C2, C3, . . . , C200). X and Y, then, constitute of all images of these classes. A trajectory S1000 from such a problem is a trajectory of images – ﬁve images per class – where we see all ﬁve images of C1 followed by ﬁve images of C2 and so on. This makes k = 5 × 200 = 1000. Note that the sampling operation deﬁnes a distribution p(T ) over problems that we use for meta-training. 4.2 Meta-Training Details  Incremental Sine Waves: We sample 400 functions to create our meta-training set and 500 for benchmarking the learned representation. We meta-train by sampling multiple CLP problems. During each meta-training step, we sample ten functions from our meta-training set and assign them task ids from one to ten. We concatenate 40 mini-batches generated from function one, then function two and so on, to create our training trajectory S400. For evaluation, we similarly randomly sample ten functions from the test set and create a single trajectory. We use SGD on the MSE loss with a mini-batch size of 8 for online updates, and Adam (Kingma and Ba, 2014) for optimizing the OML objective. Note that the OML objective involves computing gradients through a network unrolled for  5   Figure 3: Mean squared error across all 10 regression tasks. The x-axis in (a) corresponds to seeing all data points of samples for class 1, then class 2 and so on. These learning curves are averaged over 50 runs, with error bars representing 95% conﬁdence interval drawn by 1,000 bootstraps. We can see that the representation trained on iid data—Pre-training—is not effective for online updating. Notice that in the ﬁnal prediction accuracy in (b), Pre-training and SR-NN representations have accurate predictions for task 10, but high error for earlier tasks. OML, on the other hand, has a slight skew in error towards later tasks in learning but is largely robust. Oracle uses iid sampling and multiple epochs and serves as a best case bound.  400 steps. At evaluation time, we use the same learning rate as used during the inner updates in the meta-training phase for OML. For our baselines, we do a grid search over learning rates and report the results for the best performing parameter.  We found that having a deeper representation learning network (RLN) improved performance. We use six layers for the RLN and two layers for the PLN. Each hidden layer has a width of 300. The RLN is only updated with the meta-update and acts as a ﬁxed feature extractor during the inner updates in the meta-learning objective and at evaluation time.  Split-Omniglot: We learn an encoder – a deep CNN with 6 convolution and two FC layers – using the MAML-Rep and the OML objective. We treat the convolution parameters as θ and FC layer parameters as W . Because optimizing the OML objective is computationally expensive for H = 1000 (It involves unrolling the computation graph for 1,000 steps), we approximate the two objectives. For MAML-Rep we learn the φθ by maximizing fast adaptation for a 5 shot 5-way classiﬁer. For OML, instead of doing |Strain| no of inner-gradient steps as described in Algorithm 2, we go over Strain ﬁve steps at a time. For kth ﬁve steps in the inner loop, we accumulate our meta-loss on Stest[0 : 5 × k], and update our meta-parameters using these accumulated gradients at the end as explained in Algorithm 3 in the Appendix. This allows us to never unroll our computation graphs for more than ﬁve steps (Similar to truncated back-propagation through time) and still take into account the effects of interference at meta-training.  Finally, both MAML-Rep and OML use 5 inner gradient steps and similar network architectures for a fair comparison. Moreover, for both methods, we try multiple values for the inner learning rate α and report the results for the best parameter. For more details about hyper-parameters see the Appendix. For more details on implementation, see Appendix B.  4.3 Baselines  We compare MAML-Rep and OML – the two Meta-learneing based Representations Leanring methods to three baselines.  Scratch simply learns online from a random network initialization, with no meta-training.  Pre-training uses standard gradient descent to minimize prediction error on the meta-training set. We then ﬁx the ﬁrst few layers in online training. Rather than restricting to the same 6-2 architecture for the RLN and PLN, we pick the best split using a validation set.  SR-NN use the Set-KL method to learn a sparse representation (Liu et al., 2019) on the meta-training set. We use multiple values of the hyper-parameter β for SR-NN and report results for one that performs the best. We include this baseline to compare to a method that learns a sparse representation.  6  PretrainingSR-NNOMLOracle MeanSquared ErrorNo of functions learned0.00.51.01.513795108642Continual Regression ExperimentError Distribution13791086425123012301230Task ID MeanSquared Error Figure 4: Comparison of representations learned by the MAML-Rep, OML objective and the baselines on Split-Omniglot. All curves are averaged over 50 CLP runs with 95% conﬁdence intervals drawn using 1,000 bootstraps. At every point on the x-axis, we only report accuracy on the classes seen so far. Even though both MAML-Rep and OML learn representations that result in comparable performance of classiﬁers trained under the IID setting (c and d), OML out-performs MAML-Rep when learning online on a highly correlated stream of data showing it learns representations more robust to interference. SR-NN, which does not do meta-learning, performs worse even under the IID setting showing it learns worse representations.  4.4 Meta-Testing  We report results of LCLP (Wonline, θmeta) for fully online updates on a single Sk for each CLP problem. For each of the methods, we separately tune the learning rate on a ﬁve validation trajectories and report results for the best performing parameter.  Incremental Sine Waves: We plot the average mean squared error over 50 runs on the full testing set, when learning online on unseen sequences of functions, in Figure 3 (left). OML can learn new functions with a negligible increase in average MSE. The Pre-training baseline, on the other hand, clearly suffers from interference, with increasing error as it tries to learn more and more functions. SR-NN, with its sparse representation, also suffers from noticeably more interference than OML. From the distribution of errors for each method on the ten functions, shown in Figure 3 (right), we can see that both Pre-training and SR-NN have high errors for functions learned in the beginning whereas OML performs only slightly worse on those.  Split-Omniglot:  We report classiﬁcation accuracy on the training trajectory (Strain) as well as the test set in Figure 4. Note that training accuracy is a meaningful metric in continual learning as it measures forgetting. The test set accuracy reﬂects both forgetting and generalization error. Our method can learn the training trajectory almost perfectly with minimal forgetting. The baselines, on the other hand, suffer from forgetting as they learn more classes sequentially. The higher training accuracy of our method also translates into better generalization on the test set. The difference in the train and test performance is mainly due to how few samples are given per class: only 15 for training and 5 for testing.  Figure 5: OML scales to more complex datasets such a Mini-imagenet. We use the existing metatraining/meta-testing split of mini-imagenet. At meta-testing, we learn a 20 way classiﬁer using 30 samples per class.  As a sanity check, we also trained classiﬁers by sampling data IID for 5 epochs and report the results in Fig. 4 (c) and (d). The fact that OML and MAML-Rep do equally well with IID sampling indicates that the quality of representations (φθ = Rd) learned by both objectives are comparable and the higher performance of OML is indeed because the representations are more suitable for incremental learning.  Moreover, to test if OML can learn representations on more complex datasets, we run the same experiments on mini-imagenet and report the results in Figure 5.  7  Accuracy0.00.20.40.60.81.00.00.20.40.60.81.0No of classes learned incrementally0.00.20.40.60.81.0IID, Multiple Epochs, Train ErrorIID, Multiple Epochs, Test ErrorMAML-RepSRNN020010050150020010050150(c)(d)OMLScratchAll of themOnline, Single Pass, Train ErrorOnline, Single Pass, Test ErrorOMLMAML-RepSRNN0200100501500200100501500.00.20.40.60.81.0(a)(b)PretrainingScratchOMLMAML-RepSRNNPretrainingScratch1618161862010812140.60.40.20.30.20.50.40.1PretrainingOMLOMLPretraining62010812141.00.8Meta-testing: Train AccuracyAccuracyNo of classes learned incrementallySR-NNSR-NNMeta-testing: Test Accuracy Figure 6: We reshape the 2304 length representation vectors into 32x72, normalize them to have a maximum value of one and visualize them; here random instance means representation for a randomly chosen input from the training set, whereas average activation is the mean representation for the complete dataset. For SR-NN, we re-train the network with a different value of parameter β to have the same instance sparsity as OML. Note that SR-NN achieves this sparsity by never using a big part of representation space. OML, on the other hand, uses the full representation space. In-fact, OML has no dead neurons whereas even pre-training results in some part of the representation never being used.  4.5 What kind of representations does OML learn?  As discussed earlier, French (1991) proposed that sparse representations could mitigate forgetting. Ideally, such a representation is instance sparse–using a small percentage of activations to represent an input– while also utilizing the representation to its fullest. This means that while most neurons would be inactive for a given input, every neuron would participate in representing some input. Dead neurons, which are inactive for all inputs, are undesirable and may as well be discarded. An instance sparse representation with no dead neurons reduces forgetting because each update changes only a small number of weights which in turn should only affect a small number of inputs. We hypothesize that the representation learned by OML will be sparse, even though the objective does not explicitly encourage this property.  We compute the average instance sparsity on the Omniglot training set, for OML, SR-NN, and Pre-training. OML produces the most sparse network, without any dead neurons. The network learned by Pre-training, in comparison, uses over 10 times more neurons on average to represent an input. The best performing SR-NN used in Figure 4 uses 4 times more neurons. We also re-trained SR-NN with a parameter to achieve a similar level of sparsity as OML, to compare representations of similar sparsity rather than representations chosen based on accuracy. We use β = 0.05 which results in an instance sparsity similar to OML.  We visualize all the solutions in Figure 6. The plots highlight that OML learns a highly sparse and well-distributed representation, taking the most advantage of the large capacity of the representation. Surprisingly, OML has no dead neurons, which is a well-known problem when learning sparse representations (Liu et al., 2019). Even Pre-training, which does not have an explicit penalty to enforce sparsity, has some dead neurons. Instance sparsity and dead neurons percentage for each method are reported in Table 1.  Table 1: Instance sparisty and dead neuron percentage for different methods. OML learns highly sparse representations without any dead neurons. Even Pre-training, which does not optimize for sparsity, ends up with some dead neurons, on the other hand.  Method  Instance Sparsity Dead Neurons  OML SR-NN (Best) SR-NN (Sparse) Pre-Training  3.8% 15% 4.9% 38%  0% 0.7% 14% 3%  5  Improvements by Combining with Knowledge Retention Approaches  We have shown that OML learns effective representations for continual learning. In this section, we answer a different question: how does OML behave when it is combined with existing continual  8  OMLSR-NN (Sparse)Pre-trainingRandom Instance 1Random Instance 2Average Activation0.01.0Random Instance 3SR-NN Table 2: OML combined with existing continual learning methods. All memory-based methods use a buffer of 200. Error margins represent one std over 10 runs. Performance of all methods is considerably improved when they learn from representations learned by OML moreover, even online updates are competitive with rehearsal based methods with OML. Finally, online updates on OML outperform all methods when they learn from other representations. Note that MER does better than approx IID in some cases because it does multiple rehearsal-based updates for every sample.  Split-Omniglot  One class per task, 50 tasks  Five classes per task, 20 tasks  Method  Standard  OML  Pre-training  Standard  OML  Pre-training  Online Approx IID ER-Reservoir MER EWC  04.64 ± 2.61 53.95 ± 5.50 52.56 ± 2.12 54.88 ± 4.12 05.08 ± 2.47  64.72 ± 2.57 75.12 ± 3.24 68.16 ± 3.12 76.00 ± 2.07 64.44 ± 3.13  21.16 ± 2.71 54.29 ± 3.48 36.72 ± 3.06 62.76 ± 2.16 18.72 ± 3.97  01.40 ± 0.43 48.02 ± 5.67 24.32 ± 5.37 29.02 ± 4.01 02.04 ± 0.35  55.32 ± 2.25 67.03 ± 2.10 60.92 ± 2.41 62.05 ± 2.19 56.03 ± 3.20  11.80 ± 1.92 46.02 ± 2.83 37.44 ± 1.67 42.05 ± 3.71 10.03 ± 1.53  learning methods? We test the performance of EWC (Lee et al., 2017), MER (Riemer et al., 2019) and ER-Reservoir (Chaudhry et al., 2019), in their standard form—learning the whole network online—as well as with pre-trained ﬁxed representations. We use pre-trained representations from OML and Pre-training, obtained in the same way as described in earlier sections. For the Standard online form of these algorithms, to avoid the unfair advantage of meta-training, we initialize the networks by learning iid on the meta-training set.  As baselines, we also report results for (a) fully online SGD updates that update one point at a time in order on the trajectory and (b) approximate IID training where SGD updates are used on a random shufﬂing of the trajectory, removing the correlation.  We report the test set results for learning 50 tasks with one class per task and learning 20 tasks with 5 tasks per class in Split-Omniglot in Table 2. For each of the methods, we do a 15/5 train/test split for each Omniglot class and test multiple values for all the hyperparameters and report results for the best setting. The conclusions are surprisingly clear. (1) OML improves all the algorithms; (2) simply providing a ﬁxed representation, as in Pre-training, does not provide nearly the same gains as OML and (3) OML with a basic Online updating strategy is already competitive, outperforming all the continual learning methods without OML. There are a few additional outcomes of note. OML outperforms even approximate IID sampling, suggesting it is not only mitigating interference but also making learning faster on new data. Finally, the difference in online and experience replay based algorithms for OML is not as pronounced as it is for other representations.  6 Conclusion and Discussion  In this paper, we proposed a meta-learning objective to learn representations that are robust to interference under online updates and promote future learning. We showed that using our representations, it is possible to learn from highly correlated data streams with signiﬁcantly improved robustness to forgetting. We found sparsity emerges as a property of our learned representations, without explicitly training for sparsity. We ﬁnally showed that our method is complementary to the existing state of the art continual learning methods, and can be combined with them to achieve signiﬁcant improvements over each approach alone.  An important next step for this work is to demonstrate how to learn these representations online without a separate meta-training phase. Initial experiments suggest it is effective to periodically optimize the representation on a recent buffer of data, and then continue online update with this updated ﬁxed representation. This matches common paradigms in continual learning—based on the ideas of a sleep phase and background planning—and is a plausible strategy for continually adapting the representation network for a continual stream of data. Another interesting extension to the work would be to use the OML objective to meta-learn some other aspect of the learning process – such as a local learning rule (Metz et al., 2019) or an attention mechanism – by minimizing interference.  9   References  Al-Shedivat, Maruan, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel (2018). Continuous adaptation via meta-learning in nonstationary and competitive environments. International Conference on Learning Representations.  Aljundi, Rahaf, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars (2018). Memory aware synapses: Learning what (not) to forget. In European Conference on Computer Vision.  Aljundi, Rahaf, Min Lin, Baptiste Goujaud, and Yoshua Bengio (2019). Gradient based sample selection for online continual learning. Advances in Neural Information Processing Systems.  Aljundi, Rahaf, Marcus Rohrbach, and Tinne Tuytelaars (2019). Selﬂess sequential learning. Interna tional Conference on Learning Representations.  Bengio, Yoshua, Tristan Deleu, Nasim Rahaman, Rosemary Ke, Sébastien Lachapelle, Olexa Bilaniuk, Anirudh Goyal, and Christopher Pal (2019). A meta-transfer objective for learning to disentangle causal mechanisms. arXiv preprint arXiv:1901.10912.  Chaudhry, Arslan, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny (2019). Efﬁcient lifelong learning with a-gem. International Conference on Learning Representations.  Chaudhry, Arslan, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS Torr, and Marc’Aurelio Ranzato (2019). Continual learning with tiny episodic memories. arXiv:1902.10486.  Finn, Chelsea (2018, Aug). Learning to Learn with Gradients. Ph. D. thesis, EECS Department,  University of California, Berkeley.  Finn, Chelsea, Pieter Abbeel, and Sergey Levine (2017). Model-agnostic meta-learning for fast  adaptation of deep networks. In International Conference on Machine Learning.  French, Robert M (1991). Using semi-distributed representations to overcome catastrophic forgetting  in connectionist networks. In Annual cognitive science society conference. Erlbaum.  French, Robert M (1999). Catastrophic forgetting in connectionist networks. Trends in cognitive  Kingma, Diederik P and Jimmy Ba (2014). Adam: A method for stochastic optimization.  sciences.  arXiv:1412.6980.  Kirkpatrick, James, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. (2017). Overcoming catastrophic forgetting in neural networks. National academy of sciences.  Lake, Brenden M, Ruslan Salakhutdinov, and Joshua B Tenenbaum (2015). Human-level concept  learning through probabilistic program induction. Science.  Lee, Sang-Woo, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang (2017). Overcoming catastrophic forgetting by incremental moment matching. In Advances in Neural Information Processing Systems.  Li, Zhizhong and Derek Hoiem (2018). Learning without forgetting. IEEE Transactions on Pattern  Analysis and Machine Intelligence.  Li, Zhenguo, Fengwei Zhou, Fei Chen, and Hang Li (2017). Meta-sgd: Learning to learn quickly for  few-shot learning. arXiv:1707.09835.  Lin, Long-Ji (1992). Self-improving reactive agents based on reinforcement learning, planning and  teaching. Machine learning.  Liu, Vincent, Raksha Kumaraswamy, Lei Le, and Martha White (2019). The utility of sparse representations for control in reinforcement learning. AAAI Conference on Artiﬁcial Intelligence.  10   Liu, Xialei, Marc Masana, Luis Herranz, Joost Van de Weijer, Antonio M Lopez, and Andrew D Bagdanov (2018). Rotate your networks: Better weight consolidation and less catastrophic forgetting. In International Conference on Pattern Recognition.  Lopez-Paz, David and Marc’Aurelio Ranzato (2017). Gradient episodic memory for continual  learning. In Advances in Neural Information Processing Systems.  Metz, Luke, Niru Maheswaranathan, Brian Cheung, and Jascha Sohl-dickstein (2019). Meta-learning update rules for unsupervised representation learning. International Conference on Learning Representations.  Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. (2015). Human-level control through deep reinforcement learning. Nature.  Nagabandi, Anusha, Chelsea Finn, and Sergey Levine (2019). Deep online learning via meta-learning: Continual adaptation for model-based rl. International Conference on Learning Representations.  Rebufﬁ, Sylvestre-Alvise, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert (2017). icarl: Incremental classiﬁer and tation learning. In Conference on Computer Vision and Pattern Recognition.  Riemer, Matthew, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro (2019). Learning to learn without forgetting by maximizing transfer and minimizing interference. International Conference on Learning Representations.  Schmidhuber, Jurgen (1987). Evolutionary principles in self-referential learning, or on learning how  to learn. Ph. D. thesis, Institut fur Informatik,Technische Universitat Munchen.  Shin, Hanul, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim (2017). Continual learning with deep  generative replay. In Advances in Neural Information Processing Systems.  Sutton, Richard (1990). Integrated architectures for learning planning and reacting based on approxi mating dynamic programming. In International Conference on Machine Learning.  Zenke, Friedemann, Ben Poole, and Surya Ganguli (2017). Continual learning through synaptic  intelligence. In International Conference on Machine Learning.  11   Figure 7: Flowchart elucidating a single gradient update for representation learning. (1) We sample trajectory Sk from our stream of data for inner updates in the meta-training, and another trajectory Stest for evaluation. (2) We use Sk to do k gradient updates on the PLN (Prediction learning network). (3) We then use this updated network to compute loss on the Sk + Stest and compute gradients for this loss with respect to the initial parameters θ1, W1. (4) Finally, we update our initial parameters θ, W0 to θ(cid:48), W (cid:48)0.  Table 3: Parameters for Sinusoidal Regression Experiment  Parameter  Description  Learning rate used for the meta-update  Meta LR Meta Update Optimizer Optimizer used for the meta-update Inner LR Inner LR Search Steps-per-function Inner steps Total layers Layer Width Non-linearly RLN Layers Pre-training set  LR used for the inner updates for meta-learning Inner LRs tried before picking the best Number of gradient updates for each of the ten tasks Number of inner gradient steps Total layers in the fully connected NN Number of neurons in each layer Non-linearly used Number of layers used for learning representation Number of functions in the meta-training set  Value  1e-4 Adam 0.003 [0.1, 1e-6] 40 400 9 300 relu 6 400  Appendix  A Discussion on the Connection to Few-Shot Meta-Learning  Our approach is different from gradient-based meta-learning in two ways; ﬁrst, we only update PLN during the inner updates whereas maml (and other gradient-based meta-learning techniques) update all the parameters in the inner update. By not updating the initial layers in the inner update, we change the optimization problem from "ﬁnding a model initialization with xyz properties" to "ﬁnding a model initialization and learning a ﬁxed representation such that starting from the learned representation it has xyz properties." This gives our model freedom to transform the input into a more desirable representation for the task—such as a sparse representation.  Secondly, we sample trajectories and do correlated updates in the inner updates, and compute the meta-loss with respect to a batch of data representing the CLP problem at large. This changes the  12  ...Sample a trajectory from the streamRLNPLNW0UseL(Yi+1,Y0i+1)toupdateW0toW1UseL(Yi+k,Y0i+k)toupdateWk 1toWkW1RLNPLNRLNPLNStest=(Xrand,Yrand)Sample a random batch of data2RLNPLN‘Data streamT=(X0,Y0),(X1,Y1),...,(Xk,Yk),...,(Xn,Yn),...,RLNPLNW00413Update RLN and PLN using gradients from random batch✓0✓✓✓✓Sk=(Xi+1,Yi+1),(Xi+2,Yi+2),...,(Xi+k,Yi+k)WkWkXi+1Y0i+1Xi+2Y0i+2L(Yi+1,Y0i+1)Xrand+XtrajY0rand+Y0trajL(Yrand+Ytraj,Y0rand+Y0traj)BackpropogationMinimize loss on a randombatch with respect to initial parameters Table 4: Parameters for Omniglot Representation Learning  Parameter  Description  Learning rate used for the meta-update  Meta LR Meta update optimizer Optimizer used for the meta-update Inner LR Inner LR Search Inner steps Conv-layers FC Layers RLN Kernel Non-linearly Stride # kernels Input  LR used for the inner updates for meta-learning Inner LRs tried before picking the best Number of inner gradient steps Total convolutional layers Total fully connected layers Layers in RLN Size of the convolutional kernel Non-linearly used Stride for convolution operation in each layer Number of convolution kernels in each layer Dimension of the input image  Value  1e-4 Adam 0.03 [0.1, 1e-6] 20 6 2 6 3x3 relu [2,1,2,1,2,2] 256 each 84 x 84  optimization from "ﬁnding an initialization that allows for quick adaptation" (such as in maml Finn (2018)) to "ﬁnding an initialization that minimizes interference and maximizes transfer." Note that we learn the RLN and the initialization for PLN using a single objective in an end-to-end manner.  We empirically found that having an RLN is extremely important for effective continual learning, and vanilla maml trained with correlated trajectories performed poorly for online learning.  B Reproducing Results  We release our code, and pretrained OML models for Split-Omniglot and Incremental Sine Waves available at https://github.com/Khurramjaved96/mrcl. In addition, we also provide details of hyper-parameters used from learning the representations of Incremental Sine Waves experiment and Split-Omniglot in Table 3 and 4 respectively.  For online learning experiments in Figure 3 and 4, we did a sweep over the only hyper-parameter, learning rate, in the list [0.3, 0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.0001, 0.00003, 0.00001] for each method on a ﬁve validation trajectories and reported result for the best learning rate on 50 random trajectories.  B.1 Computing Infrastructure  We learn all representations on a single V100 GPU; even with a deep neural network and meta-updates involving roll-outs of length up to 400, OML can learn representations in less than ﬁve hours for both the regression problem and omniglot experiments. For smaller roll-outs in Omniglot, it is possible to learn good representations with-in an hour. Note that this is despite the fact that we did not use batch-normalization layers or skip connections which are known to stabilize and accelerate training.  C Representations  We present more samples of the learned representations in Figure 8. We also include the averaged representation for the best performing SR-NN model (15% instance sparsity) in Figure 10 which was excluded from Figure 6 due to lack of space.  13   Figure 8: More samples of representations for random input images for different methods. Here SR-NN (4.9%) is trained to have similar sparsity as OML whereas SR-NN (15%) is trained to have the best performance on Split-Omniglot benchmark.  Figure 9: Instead of learning an encoder φθ , we learn an initialization by updating both θ and W in the inner loop of meta-training. In "OML without RLN," we also update both at meta-test time whereas in "OML without RLN at test time," we ﬁx theta at meta-test time just like we do for OML . For each of the methods, we report the training error during meta-testing. It’s clear from the results that a model initialization is not an effective bias for incremental learning. Interestingly, "OML with RLN at test time" doesn’t do very poorly. However, if we know we’ll be ﬁxing θ at meta-test time, it doesn’t make sense to update it in the inner loop of meta-training (Since we’d want the inner loop setting to be as similar to meta-test setting as possible.  Figure 10: Average activation map for the best performing SR-NN with 15% sparsity. Scale goes from 0 to max (Light to dark green.)  14  OMLSR-NN (4.9%)SR-NN (15%)PretrainingOML: Learning RLNOML: Learning an InitializationNumber of classes learnedAccuracy2550751001251501750.00.20.40.60.81.010Omniglot Training Trajectory Performance itrain from p(Sk|Ti)  Algorithm 3: Meta-Training : Approximate Implementation of the OML Objective Require: p(T ): distribution over tasks Require: α, β: step size hyperparameters Require: m: No of inner gradient steps per update before truncation 1: randomly initialize θ, W 2: while not done do Sample task Ti ∼ p(T ) 3: Sample S 4: 5: W0 = W 6: ∇accum = 0 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end while  end for Sample Sitest from p(Sk|Ti) θ = θ + ∇θ(cid:96)i(fθ,Wj [Stest[0 : j, 0]], Sitest[0 : j, 1]) Stop Gradients(fθ,Wj ))  Wj = Wj−1 − α∇Wj−1(cid:96)i(fθ,Wj−1(X ij), Y ij ) j = j + 1  while j ≤ |Strain| do  for k in 1, 2, . . . , m do  end while  C.1 Why Learn an Encoder Instead of an Initialization  We empirically found that learning an encoder results in signiﬁcantly better performance than learning just an initialization as shown in Fig 9. Moreover, the meta-learning optimization problem is more well-behaved when learning an encoder (Less sensitive to hyper-parameters and converges faster). One explanation for this difference is that a global and greedy update algorithm – such as gradient descent – will greedily change the weights of the initial layers of the neural network with respect to current samples when learning on a highly correlated stream of data. Such changes in the initial layers will interfere with the past knowledge of the model. As a consequence, an initialization is not an effective inductive bias for incremental learning. When learning an encoder φθ, on the other hand, it is possible for the neural network to learn highly sparse representations which make the update less global (Since weights connecting to features that are zero remain unchanged).  15  
2.2 Brief Overview of ML Workflows ML workflows commonly consist of three major components: Data Pre-processing (DPR). This stage contains all the data manipulation operations, such as data cleaning and feature extraction, used to turn raw data into a format compatible with ML algorithms. Learning/Inference (L/I). Once the data is transformed into a learnable representation, such as feature vectors, learning takes place, using the transformed data to derive an ML model via optimization. Inference refers to the processing by which the learned model is used to make predictions on unseen data, and is often performed after learning. Post Processing (PPR). Post processing is the all-encompassing term for operations following learning and inference. Bruha et al. [5] classifies PPR operations in to four categories: 1) rule-based knowledge filtering, 2) and knowledge integration, 3) interpretation and explanation, 4) evaluation. While 1) and 2) involve transformations of the L/I output, 3) and 4) are about the analysis of the L/I output. Mentions of 1) and 2) are sparse in our corpus and thus excluded from our study.  In the context of ML application development, an iteration involves creating a version of the workflow, either from scratch or by copying/modifying a previous version, and executing this version end to end to obtain some results. Program termination marks the end of an iteration, and any results that are not written to disk during execution can only be obtained by modifying the workflow to explicitly save the results and rerunning the workflow.  2.3 Statistics Collection Our goal in this survey is to collect statistics on how users iterate on ML workflows. However, iterations are often not explicitly reported in publications. To overcome this challenge, we design a set of statistics that allow us to infer the iterative process leading to the results reported in each paper. We introduce the statistics for each individual component of the ML workflow below. DPR. As mentioned above, DPR encompasses all operations involved in transforming raw data into learnable representations, such as feature engineering, data cleaning, and feature value normalization. We record D, the set of distinct DPR operation types found in each paper and collect n D = |D |. Mentions of DPR operations are usually found in the data and methods sections in the paper. L/I. Workflow modifications concerning L/I fall into one of three categories: 1) hyperparameter tuning for a model (e.g., increasing learning rate, changing the architecture of a neural net) and 2) switching between model classes (e.g., from decision tree to SVM). For each paper, we record M, the set of all model classes and P, the set of distinct hyperparameters tuned across all model classes, and collect nM = |M| and n P = |P |. Evidence for these statistics is usually found in the algorithms section, as well as result tables and figures. PPR. Of the four types of PPR operations enumerated above, evaluation and interpretation/explanation are the most commonly reported in papers, often presented in tables or figures. For each  Figure 1: Paper count per domain by conference.  study iteration using the statistics. In Section 3, we report interesting results and insights discovered from our survey and propose concrete system requirements to support human-in-the-loop ML based on the survey analysis.  2 DATA & METHODOLOGY In this section we describe the dataset and the methods used to collect the statistics that enable analyses of iteration in publications.  2.1 Corpus We surveyed 105 papers published in 2016 on applied data science. To ensure relevance, we selected four venues that specifically publish applied machine learning studies: KDD Applied Data Science Track, Association for Computational Linguistics (ACL), Computer Vision and Pattern Recognition (CVPR), and Nature Biotechnology (NB). We randomly sample 20 papers from ACL, CVPR, and NB each, and 45 papers from KDD. These papers span applications in social sciences (SocS), web applications (WWW), natural sciences (NS), natural language processing (NLP), and computer vision (CV). Paper topics were determined using the ACM Computing Classification System (CCS) 1. Keywords in each paper are matched with entries in the CCS tree, and each paper is assigned as its domain the most appropriate high level entry containing its keywords. Figure 1 illustrates the domain composition of the conferences surveyed. While ACL, CVPR, and Nature specialize in a single domain, KDD embraces many domains, with a focus on web applications and social science. Limitations. Our approach is limited in its ability to accurately model iterations due to several characteristics of the corpus: 1) While the corpus spans multiple domains, the number of paper in each domain is small, which can lead to spurious trends. 2) Papers provide an incomplete picture of the overall iterative process. Machine learning papers are results-driven and focus more on modeling than data pre-processing by convention. Due to space constraints, authors often omit a large number of iterative steps and report only on the small subset that led to the final results.  3) Papers often present results side by side instead of the order they were obtained, making it difficult to determine the exact transitions between the variants studied in the iterative process.  We attempt to overcome some of these limitations by  • Having multiple surveyors and aggregating the results to reduce the change of spurious results, to be elaborated in Section 2.3;  1https://www.acm.org/publications/class-2012  010203040CVPRNATUREACLKDDDomainWebAppSocialSciencesNLPNaturalSciencesCV paper, we record E, the set of evaluation metrics used, and collect n E = |E |. In addition, we collect nt abl e and nf iдur e , the number of tables and figures containing results and case studies, respectively. We refer to D, M, P, E collectively as entity sets in the rest of  the paper 2.  To ensure the quality of the statistics collected, we had three graduate students in data mining, henceforth referred to as surveyors, perform the survey independently on the same corpus. We reference the results collected by each surveyor with a subscript, e.g., M1 is the set of model classes recorded by surveyor 1. To increase the likelihood of consensus, we first had the surveyors discuss and agree on a seed set for each entity set, e.g., E = {Accuracy, RMSE, NDCG}. Surveyors were then asked to remove from and add to this set as they see fit for each paper. Let n′x be the aggregated value of the statistic nx . We aggregate the three sets of results as follows:  • For an entity set S (e.g., M, the set of model classes), let Sa = S1 ∪ S2 ∪ S3. We filter Sa to obtain S ′ ⊆ Sa such that s ∈ S ′ is identified by at least two surveyors. That is, a paper is considered to contain an operation only if it is identified to be in the paper by at least two surveyors independently. We define n′ S for the corresponding statistic as |S ′|. • For nt abl e and nf iдur e , we define n′  t abl e/f iдur e to be the av erage of the values obtained by the three surveyors.  2.4 Estimating Iterations using Statistics The information collected above indicate versions of the workflow studied but not the iterative modifications themselves. To infer the number of iterations using the statistics collected above, we make the following assumptions:  • Each iteration involves a single change. While it is possible for multiple changes to be tested in a single iteration, it is unlikely the case since the interactions can obfuscate the contribution of individual changes.  • Each element in an entity set is tested exactly once. For the authors to report on a variant, there must have been at least one version of the workflow containing that variant. Although it is likely for a variant to be revisited in multiple iterations in the actual research process, papers, by convention, provide little information on this aspect. Due to this lack of evidence, we take the conservative approach by taking the minimum value. Let tDP R , tLI , tP P R be the number of iterations containing changes to the DPR, L/I, and PPR components of the workflow, respectively. Using the two assumptions above, we estimate tDP R , tLI , and tP P R as follows: • ˆtDP R = n′D • ˆtLI = (n′M − 1) + (n′P − 1) • ˆtP P R = min (cid:16)n′E, n′ t abl e + n′ f iдur e For ˆtDP R , we assume that the authors start with the raw data and incrementally add more data pre-processing operations in each iteration. We subtract one from n′M and n′P in ˆtLI to account for the fact that the initial version of the workflow must contain a model, a set of hyperparameters, and an optimization algorithm.  (cid:17)  2The complete entity sets and statistics can be found at https://github.com/gestaltml/AppliedMLSurvey/blob/master/data/combinedCounts.tsv  The estimator ˆtP P R assumes that in a PPR iteration, the authors can either gather all information on a single metric or generate an entire figure/table.  3 RESULTS AND INSIGHTS In this section we share interesting trends about ML workflow development discovered from our survey.  3.1 Iteration Count  Figure 2: Distribution of number of iterations by workflow component.  012345ˆtDPR0.00.20.40.60.81.0Num.Papers012345ˆtLI0.00.20.40.60.81.0012345ˆtPPR0.00.20.40.60.81.0EstimatorValueHistogramsforAllPapers012345ˆtDPR0.00.20.40.60.81.0Num.Papers012345ˆtLI0.00.20.40.60.81.0012345ˆtPPR0.00.20.40.60.81.0EstimatorValueHistogramsforSocialSciences012345ˆtDPR0.00.20.40.60.81.0Num.Papers012345ˆtLI0.00.20.40.60.81.0012345ˆtPPR0.00.20.40.60.81.0EstimatorValueHistogramsforNaturalSciences012345ˆtDPR0.00.20.40.60.81.0Num.Papers012345ˆtLI0.00.20.40.60.81.0012345ˆtPPR0.00.20.40.60.81.0EstimatorValueHistogramsforWebApplications012345ˆtDPR0.00.20.40.60.81.0Num.Papers012345ˆtLI0.00.20.40.60.81.0012345ˆtPPR0.00.20.40.60.81.0EstimatorValueHistogramsforNLP012345ˆtDPR0.00.20.40.60.81.0Num.Papers012345ˆtLI0.00.20.40.60.81.0012345ˆtPPR0.00.20.40.60.81.0EstimatorValueHistogramsforComputerVision common belief that ML applications have collectively progressed beyond handcrafted features thanks to the advent of deep learning (DL). In addition to the incompatibilities with DL in some domains mentioned in Section 3.1, the efficacy of features designed using domain knowledge versus using DL to search for the same features without domain knowledge is possibly another contributing factor.  3.3 Learning/Inference by Domain Table 2 lists the most popular model classes for each application domain, with abbreviations expanded in the caption. We have already discussed the disparity between the popularity of DL in CV/NLP and other domains in Section 3.1. Most traditional approaches such as GLM, SVM, and Random Forest are still in favor with most domains, since the large additional computation cost for DL often fails to justify the incremental model performance gain. Matrix factorization, which is highly amenable to parallelization, is popular in web applications for supporting recommendation engines. Interestingly, SVM is the most popular method in natural sciences by a large margin (100% more popular than the second most popular option), possibly due to its ability to support higher order functions through kernels. NS applications experimenting with DL are mostly computer vision related.  Table 3 shows the most popular model tuning operations by domains. The top two operations, learning rate and batch size, are both concerned with the training convergence rate, suggesting that training time is an important factor in all domains. Cross validation and regularization are both mechanisms to control model complexity and overfitting to observed data. Lower complexity models usually result in faster inference time and better ability to generalize to more unseen data.  3.4 Post Processing by Domain Of the evaluation methods listed in Table 4, P/R, accuracy, correlation, and DCG are summary evaluations of model performance while case study, feature contribution, human evaluation, and visualization are fine-grained methods towards insights to improve upon the current model. While the former group can be used automatically such as in grid search, the latter group is aimed purely for human understanding.  3.5 System Desiderata The results in Section 3 suggest a number of properties that a versatile and effective human-in-the-loop ML system should possess:  • Iteration. Developers iterate on their workflows in every application domain and test out changes to all components of the workflow. Understanding the most frequent changes helps us develop systems that anticipate and respond rapidly to iterative changes.  • Fine-grained feature engineering. Handcrafted features designed using domain knowledge is still an indispensable part of the workflow development systems in all domains and should therefore be adequately supported instead of dismissed as an outdated practice.  • Efficient joins. Data is often pooled from multiple sources, thus requiring systems to support efficient joins in the data pre-processing component.  Figure 3: Mean iteration count by domains.  Figure 2 shows the histograms for the three iteration estimators ˆtDP R , ˆtLI , ˆtP P R across the entire corpus (top row) and by domain (rows 2-6). A bin in every histogram represents an integral value for the estimators, and bin heights equal the fraction of papers with the bin value as their estimates. The mean values for the estimators by domains are shown in the stacked bar chart in Figure 3, where the total bar length is equal to the average number of iterations in each domain. From these two figures, we see that 1) most papers use ≥ 1 evaluation methods, evident from the fact that histograms in the third column in Figure 2 are skewed towards ˆtP P R ≥ 2; 2) PPR is the most common iteration type across all domains, evident from the length of the E[ˆtP P R ] bars in Figure 3; and 3) on average, more DPR iterations are reported than L/I iterations in every domain except computer vision, as illustrated by the relative lengths of the E[ˆtDP R ] and E[ˆtLI ] bars in Figure 3.  When grouped by domains, we see that the distributions for certain domains deviate a great deal from the overall trends in Figure 2. Domains dominated by deep neural nets (DNNs), which are designed to replace manual feature engineering for higher order features, tend to skew towards fewer DPR and more L/I iterations, such as NLP and CV. Additionally, there are only a few highly processed datasets studied in all NLP and CV papers, further reducing the need for data pre-processing in these domains. On the other hand, social and natural sciences exhibit the opposite trend in the histograms in Figure 2, biasing towards more DPR iterations. This is largely due to the fact that both domains rely heavily on domain knowledge to guide ML and strongly prefer explainable models. In addition, a large amount of data is required to enable training of DNNs. The scale of data is often much smaller for SocS and NS than NLP and CV, thus preventing effective application of DNNs and requiring more manual features.  3.2 Data Pre-processing by Domain Table 1 shows the most popular DPR operations in each application domain, ordered top to bottom by popularity, with abbreviations expanded in the caption. While the table reaffirms common knowledge such as feature normalization is important, Table 1 also shows two striking results: 1) joining multiple data sources is common in four of the five domains surveyed; 2) 13 of the papers contain fine-grained features defined using domain knowledge across all domains. Result 1) suggest that unlike classroom and data competition settings in which the input data resides conveniently in a single file, data in real-world ML applications is aggregated from multiple sources (e.g., user database and event logs). Result 2) contradicts the  01234567CVNLPWebAppNaturalSciencesSocialSciencesE[ˆtDPR]E[ˆtLI]E[ˆtPPR] SocS Join (31.0%) Feature def. (27.6%) Normalize (17.2%) Impute (6.9%)  WWW Feature def. (36.1%) Join (22.2%) Normalize (13.9%) Discretize (8.3%) Table 1: Common DPR operations ordered top to bottom by popularity. Join = joining multiple data sources; Feat. def. = custom logic for fine-grained feature extraction; Univer. FS = univariate feature selection, using criteria such as support and correlation per feature; BOW = bag of words; PCA = principal component analysis, a common dimensionality reduction technique.  NS Feature def. (40.6%) Univar. FS (18.8%) Normalize (12.5%) PCA (9.4%)  CV Feature def. (37.5%) BOW (25.0%) Interaction (25.0%) Join (12.5%)  NLP Feature def. (32.1%) BOW (17.9%) Join (14.3%) Normalize (10.7%)  SocS GLM (36.0%) SVM (28.0%) RF (20.0%)  NS SVM (32.7%) GLM (15.4%) RF (13.5%)  WWW GLM (37.0%) RF (11.1%) SVM (11.1%)  Decision Tree (12.0%) DNN (13.5%) Matrix Factorization (11.1%)  NLP  CV  RNN (32.4%) CNN (38.2%) SVM (17.6%) GLM (14.7%) RNN (17.6%) SVM (11.8%) RF (5.9%) CNN (8.8%)  Table 2: Common model classes ordered top to bottom by popularity per domain. GLM = generalized linear models (e.g., logistic regression); RF = random forest; SVM = support vector machine; R/CNN = recursive/convolutional neural networks.  SocS Regularize (40.0%) CV (30.0%) LR (10.0%) Batch size (10.0%)  NS CV (31.8%) LR (22.7%) DNN arch. (18.2%) Kernel (9.1%)  WWW Regularize (41.2%) LR (23.5%)  CV NLP LR (46.2%) LR (39.4%) Batch size (30.8%) Batch size (24.2%) Batch size (11.8%) DNN arch. (18.2%) DNN arch. (11.5%) Regularize (11.5%) Kernel (6.1%)  CV (11.8%) Table 3: Most popular model tuning operations by domain. CV = cross validation; LR = learning rate; DNN arch. = DNN architecture modification; Kernel specifically applies to SVM.  SocS P/R (25.7%) Acc. (20.0%) Feat. Contrib. (17.1%) Vis. (14.3%)  NS Acc. (28.6%) P/R (18.6%) Vis. (15.7%) Correlation (11.4%)  WWW Acc. (20.8%) P/R (20.8%) Case (13.2%) DCG (9.4%)  NLP P/R (29.2%) Acc. (27.1%) Case (14.6%) Human Eval. (8.3%)  CV Vis. (33.3%) Acc. (29.8%) P/R (17.5%) Case (12.3%)  Table 4: Most popular evaluation methods by domain. P/R = precision/recall; Acc. = accuracy; Vis. = visualization; Feat. Contrib. = feature contribution to model performance; NCG = discounted cumulative gain, popular in ranking tasks; Case = case studies of individual results.  • Explainable models. Many domains have yet to embrace deep learning due to their needs for explainable models. The system should provide ample support to help developer interpret model behaviors.  • Fast model training. The fact that the most tuned model parameters are related to training time suggests that developers are in need of systems that have fast model training, but also low latency for the end-to-end workflow execution in general. • Fine-grained results analysis. Fine-grained and summary evaluation methods are equally popular across all domains. Thus, model management systems should provide support for not only summary metrics but also more detailed model characteristics.  We are in the process of developing a system, titled Helix [17], that is aimed at accelerating iterations in human-in-the-loop ML  workflow development, using many of the properties listed above as guiding principles. 

II. METHODOLOGY  We discuss the research questions, as well as our study  design, and the data our analysis rely on.   A. Research Question  The goal of this work is to identify the top-performing model to classify user feedback (tweets and app reviews) into problem reports, inquires, and irrelevant by comparing the traditional machine learning approach with deep learning. We, therefore, state the following research questions:  • RQ1. To what extent can we extract problem reports, inquires, and irrelevant information from user feedback using traditional machine learning?  • RQ2. To what extent can we extract problem reports, inquires, and irrelevant information from user feedback using deep learning?  • RQ3. How do the results of the traditional machine learning approach and the deep learning approach compare and what can we learn from it?  B. Study Design  Fig. 1: Overview of the study design.  Figure 1 shows the overall study design. Each white box within the four columns describes a certain step that we performed while the grey boxes show the result that each column produced. The ﬁrst part of the paper is about the Study Data, which we describe later in this section. In the second part, Traditional Approach, we perform traditional machine learning engineering, including feature engineering and hyperparameter tuning. In the part Deep Learning Approach, we design a convolutional neural network architecture, apply transfer learning for the embedding layer, and ﬁnally evaluate the ﬁne-tuned models. In the fourth part Result Comparison, we report on the results of our classiﬁcation experiments (benchmark) comparing the traditional and the deep learning approaches.  C. Study Data  We collected about 5,000,000 English and 1,300,000 Italian Tweets addressing Twitter support accounts of telecommunication companies. From that corpus, we randomly sampled ∼10,000 English tweets and ∼15,000 Italian tweets that were composed by users. As the annotation of so many tweets is very time-consuming, we created coding tasks on the crowdannotation platform ﬁgure eight1. Before starting the crowd TABLE I: Overview of the study data.  App Reviews English  Tweets  English  Italian  n problem report n inquiry n irrelevant  TOTAL  1.437 1.100 3.869  6.406  2.933 1.405 6.026  3.414 2.594 9.794  10.364  15.802  annotation, we ﬁrst wrote a coding guide to describe our understanding of problem reports, inquiries, and irrelevant tweets with the help of the innovation center of a big Italian telecommunication company. Second, we run a pilot study to test the quality of the coding guide and the annotations received. Both coding guides were either written or proofread by at least two native speakers, and we required that the annotators are natives in the language. Each tweet can belong to exactly one of the before mentioned classes and is annotated by at least two persons, three in case of a disagreement. As for the annotated app reviews, we rely on the data and annotations of Maalej et al. [27]. Table I summarizes the annotated data for both languages.  Replication package. To encourage replicability, we uploaded all scripts, benchmark results, and provide the annotated dataset upon request2.  III. MACHINE LEARNING PIPELINES  We describe how we performed the machine learning approaches and explain certain decisions such as for the selected features. To ensure a fair comparison between the traditional and the deep learning approach, we used not only the same datasets but also the same train and test sets.  A. Traditional Machine Learning  1) Preprocessing: We preprocessed the data in three steps to reduce ambiguity. Step 1 turns the text into lower case; this reduces ambiguity by normalizing, e.g., “Feature”, “FEATURE”, and “feature” by transforming it into the same representation “feature”. Step 2 introduces masks to certain keywords. For example, whenever an account is addressed using the “@” symbol, the account name will be masked as “account”. We masked account names, links, and hashtags. Step 3 applies lemmatization, which normalizes the words to their root form. For example, words such as “see”, “saw”, “seen’, and “seeing” become the word “see”.  2) Feature Engineering: Feature engineering describes the process of utilizing domain knowledge to ﬁnd a meaningful data representation for machine learning models. In NLP it encompasses steps such as extracting features from text, as well as selection and optimization. Table II summarizes the groups of features, their representation, as well as the number of features we extracted for that feature group. For instance, the table shows that the feature group “keywords” consists of 37 keywords for the Italian language, each of them being 1 if that keyword exists or 0 if not.  1https://www.ﬁgure-eight.com/  2https://mast.informatik.uni-hamburg.de/replication-packages/  Deep Learning BenchmarkData AnnotationStudy Data ReportData CrawlingDesigning Network ArchitectureTraditional MLBenchmark Transfer LearningClassifier SelectionApproach DescriptionHyperparameter tuningResult ComparisonData CollectionClassificationBenchmarkModel ConfigurationDiscussionHyperparameter tuningApproach DescriptionFeature EngineeringReport with insightsDescriptive Statistics TABLE II: Extracted features before scaling. If not further speciﬁed, the number of features applies to all data sets.  Feature Group  Value Boundaries  Number of Features  n words n stopwords sentimentneg sentimentpos keywords POS tags tense tf-idf  fastText  TOTAL  N N {x ∈ Z | − 5 ≤ x ≤ −1} {x ∈ N | 1 ≤ x ≤ 5} {0, 1} N N {x ∈ R | 0 ≤ x ≤ 1}  {x ∈ R | 0 ≤ x ≤ 1}  1 1 1 1 37 (IT), 60 (EN) 18 (IT), 16 (EN) 4 (IT), 2 (EN) 665 (app reviews, EN) 899 (tweets, EN) 938 (tweets IT) 300  1.047 (app reviews, EN) 1.281 (tweets, EN) 1.301 (tweets IT)  We extracted the length (n words) of the written user feedback as Pagano and Maalej [31] found that most irrelevant reviews are rather short. One example for such a category is rating, which does not contain valuable information for developers as most of the time, such reviews are only praise (e.g., “I love this app.”). Excluding or including stop words, in particular in the preprocessing phase is highly discussed in research. We found papers that reported excluding stop words as an essential step (e.g., [16]), papers that leveraged the inclusion of certain stop words (e.g., [20]), and others that tested both (e.g., [27]). However, the decision for exclusion and inclusion depends on the use case. We decided to use them as a feature by counting their occurrence in each document. Further, we extracted the sentiment of the user feedback using the sentistrength library [35]. We provide the full user feedback (e.g., a tweet) as the input for the library. The library then returns two integer values, one ranging from -5 to -1 indicating on how negative the feedback is, the other ranging from +1 to +5 indicating how positive the feedback is. The sentiment can be an important feature as users might write problem reports in a neutral to negative tone while inquiries tend to be rather neutral to positive [16], [31], [27]. Keywords have proven to be useful features for text classiﬁcation [36], [27], [18] as their extraction allows input of domain experts’ knowledge. However, keywords are prone to overﬁt for a single domain and therefore might not be generalizable. In this work, we use the same set of keywords for the English app reviews and tweets. We extracted our set of keywords by 1) looking into related work [19], [36], [27], and 2) by manually analyzing 1,000 documents from the training set of all three datasets following the approach from Iacob and Harrison [19]. Kurtanovi´c and Maalej [24], [23] successfully used the counts of Part-of-speech (POS) tags for classiﬁcation approaches in requirements engineering. Therefore we also included them in our experiments.  Maalej et al. [27] successfully utilized the tenses of sentences. This feature might be useful for the classiﬁcation as users write problem reports often in the past or present tense, e.g., “I updated the app yesterday. Since then it crashes.”  and inquiries (i.e., feature requests) in the present and future tense, e.g., “I hope that you will add more background colors”. When extracting the tense using spaCy3 the Italian language model supported four tenses while for the English language we had to deduce the tense by extracting the part-of-speech tags. Tf-idf (term frequency-inverse document frequency) [34] is a frequently used technique to represent text in a vector space. It increases proportionally to the occurrence of a term in a document but is offset by the frequency of the term in the whole corpus. Tf-idf combines term frequencies with the inverse document frequency to calculate the term weight in the document.  FastText [21] is an unsupervised approach to learn highdimensional vector representations for words from a large training corpus. The vectors of words that occur in a similar context are close in this space. Although the fastText library provides pre-trained models for several languages, we train our own domain-speciﬁc models based on 5,000,000 English app reviews, 1,300,000 Italian tweets, and 5,000,000 Italian tweets. We represent each document as the average vector of all word vectors of the document, which is also a 300-dimensional vector. We chose fastText for our word embedding models as it composes a word embedding from subword embeddings. In contrast, word2vec [29] learns embeddings for whole words. Thereby, our model is able to 1) recognize words that were not in the training corpus and 2) capture spelling mistakes, which is a typical phenomenon in user feedback.  3) Experiment Conﬁguration: For the experiment setup, we tried to ﬁnd the most accurate machine learning model by varying ﬁve dimensions (no particular order). In the ﬁrst dimension we target to ﬁnd the best-performing features of Table II by testing different combinations. In total, we tested 30 different feature combinations such as “sentiment + fastText” and “n words + keywords + POS tags + tf-idf”.  The second dimension is testing the performance of (not) applying feature scaling. Tf-idf vectors, for example, are represented by ﬂoat numbers between 0 and 1, while the number of words can be any number greater than 0. This could lead to two issues: 1) the machine learning algorithm might give a higher weight to features with a high number meaning that the features are not treated equally. 2) the machine learning model could perform worse if features are not scaled.  In the third dimension, we perform Grid Search [2] for hyper-parameter tuning. In contrast to Random Search, which samples hyper-parameter combinations for a ﬁxed number of settings [1], Grid Search exhaustively combines hyperparameters of a deﬁned grid. For each hyper-parameter combination in the Grid Search, we perform 5-fold cross-validation of the training set. We optimize the hyperparameters for the f1 metric to treat precision and recall as equally important.  The fourth dimension checks whether sampling (balancing) the training data improves the overall performance of the classiﬁers. For unbalanced data the machine learning algorithm might tend to categorize a document as part of the majority  3https://spacy.io/   Fig. 2: Neural network architecture for the classiﬁcation.  class as this is the most likely option. In this work we test both, keeping the original distribution of documents per class and applying random under-sampling on the majority class to create a balanced training set.  Finally, the ﬁfth dimension is about testing different machine learning algorithms. Similar to our reasoning for the feature selection, we tested the following algorithms frequently used in related work: Decision Tree, Random Forest, Naive Bayes, and Support Vector Machine [27], [14], [37]. As for the classiﬁcation, we follow the insights from Maalej et al. [27] and employ binary classiﬁcation (one for each: problem report, inquiry, and irrelevant) instead of multiclass classiﬁcation.  B. Deep Learning  1) Deep Learning: Traditional classiﬁcation approaches require a data representation based on hand-crafted features, which domain experts deem useful criteria for the classiﬁcation problem at hand. In contrast, neural networks, which are used in deep learning approaches, use the raw text as an input, and learn high-level feature representations automatically [11]. In previous work, researchers applied them in diverse applications with remarkable results to different classiﬁcation tasks, including object detection in images, machine translation, sentiment analysis, and text classiﬁcation tasks [6]. However, neural networks are not a silver bullet, and they have also achieved only moderate results in the domain of software engineering [13], [9], [10].  2) Convolutional Neural Networks: Although convolutional neural networks (CNNs) have mainly been used for image classiﬁcation tasks, researchers also applied them successfully to natural language processing problems [22], [26]. In most cases, deep learning approaches require a large amount of training data to outperform traditional approaches. Figure 2 shows the architecture of the neural network that we used for the experiments in this study. H¨aring et al. [18] used this model to identify user comments on online news sites that address either the media house, the journalist, or the forum moderator.  They achieved promising results that partly outperformed a traditional machine learning approach. The input layer requires a ﬁxed size for the text inputs. We choose the size 200, which we found appropriate for both the app review and the Twitter dataset as tweets are generally shorter and we identiﬁed less than 20 app reviews that exceed 200 words. We cut the part, which is longer than 200 words and pad shorter input texts, so they reach the required length. After the input layer our network consists of an embedding layer, a 1D convolution layer, a 1D global max pooling layer, a dense layer, and a concluding output layer with a softmax activation. For the previous layers, we used the tanh activation function. During training, we froze the weights of the embedding layer, whereby 15,000 trainable parameters remain.  3) Transfer Learning: Transfer learning is a method often applied to deep learning using models pre-trained on another task [11]. In natural language processing, a common application of transfer learning is to reuse word embedding models, e.g. word2vec [29] or fastText [21], which were previously trained on a large corpus to pre-initialize the weights of an embedding layer. We applied transfer learning to pre-initialize our embedding layer with three different pre-trained fastText models [21]. During training, we froze the weights of the embedding layer.  4) Hyperparameter Tuning: The network architecture and the hyperparameter conﬁguration can be a crucial factor for the performance of the neural network. Therefore we compared variations of both our CNN architecture as well as training parameters and evaluated the best-performing model on the test set. We performed a grid search and varied the number of ﬁlters and the kernel size of the 1D convolutional layer, the number of units for the dense layer, the number of epochs and the batch size for the training, and the number of units for the ﬁnal dense layer. Due to the small size of our training set, we conducted a stratiﬁed 3-fold cross-validation on the training set for each hyperparameter conﬁguration to acquire reliable results. Subsequently, we evaluated the model with the best ……………………………200300Ihavenoserviceathome!PADDING…64198…64…1601Embedding layer(pre-ﬁlled with fastText model)1D convolution layerﬁlter: 64stride: 1kernel size: 3Global maxpooling layerDense layerunits: 16Input layer(padded)Output layerProblem reportIrrelevant TABLE III: Classiﬁcation benchmark for the traditional machine learning approach (Trad.) and the deep learning approach (DL). The best f1 score per classiﬁcation problem and dataset is marked in bold font.  . problem report d inquiry Tra irrelevant  L D  problem report inquiry irrelevant  app review EN  r  .75 .76 .89  .60 .79 .93  f1  .79 .72 .89  .52 .74 .85  auc  .85 .85 .86  .82 .94 .90  p  .83 .68 .88  .46 .69 .78  tweet EN f1 r  .82 .70 .75  .42 .40 .70  .59 .43 .74  .46 .40 .72  p  .46 .32 .73  .51 .40 .74  auc  .72 .73 .69  .74 .75 .75  p  .51 .47 .78  .62 .51 .85  tweet IT f1 r  .88 .82 .89  .57 .57 .77  .65 .60 .83  .59 .54 .81  auc  .83 .82 .73  .84 .83 .86  performing hyperparameter conﬁguration on the test set. We trained the models with seven epochs and a batch size of 32. We used the Python library Keras [5] for composing, training, and evaluating the models.  IV. RESULTS In this section, we describe and discuss the results of the classiﬁcation experiments. We ﬁrst explain the evaluation metrics. Then, we report on the benchmark in Table III showing the top accuracy. Finally, we explain the conﬁguration of the models leading to the best results from Table IV.  For this work, we report on the classiﬁcation metrics precision, recall, and f1 as presented in related work [16], [36], [27]. For the calculation of these metrics we used sklearn’s strictest parameter setting average=binary, which is only reporting the result for classifying the true class. Additionally, we report on the Area Under the Curve AUC value, which is considered a better metric when dealing with unbalanced data as it is independent of a certain threshold for binary classiﬁcation problems. In machine learning, Area Under the Receiver Operating Characteristics ROC AUC is a metric frequently used to address class imbalance. Davis and Goadrich [7] argue that Precision-Recall AUC (PR AUC) is a more natural evaluation metric for that problem. We optimized and selected the classiﬁcation models based on f1, the harmonic mean of precision and recall. Thereby either precision or recall can have a rather low value compared to the other.  Table III shows the classiﬁcation results of the best model for each of the three data sets and each classiﬁcation problem. traditional machine learning For the English app reviews, generally performs better than deep learning when considering the f1 score. One reason for this difference might be, that for the app reviews, we have only about 6,000 annotated data points while for tweets we have about 10,000 for English tweets and about 15,000 for Italian tweets. For the English tweets, both approaches perform quite similar. While the f1 score seems to be lower for the deep learning approach, the AUC values are similar for both approaches. The results for the Italian tweets show when optimizing towards f1, that deep learning reaches a higher precision, while the traditional approaches achieve a higher recall. The f1 score reveals again that both approaches perform similarly. Based on our results, which are generated by a large series of experiments, we cannot say that for our setup, either of the approaches performs better.  V. DISCUSSION  A. Implications of the Results  In this work, we classiﬁed user feedback for two languages from two different feedback channels. We found that when considering the f1 score as a measure, traditional machine learning performs slightly better in most of the examined cases. We expect that our approaches can also be applied to further feedback channels and languages, although some features are language-dependent and need to be updated. For example, our deep learning model requires on top of a training set a pre-trained word embedding model for each language such as the English and Italian fastText models used. Word embeddings capture the similarity between words depending on the domain and language. They are highly adaptable to language development by retraining the model regularly on current app reviews and tweets. It can capture the meaning of transitory terms like Twitter hashtags or emoticons. In traditional approaches, the language-dependent features are keywords, sentiment, POS tags, and the tf-idf vocabulary. This requires more effort for creating models for multiple languages. The rest remains language and domain-independent. Traditional approaches often perform better on small training sets as domain experts implicitly incorporate signiﬁcant information through hand-crafted features [4]. We assume that for these experiments, the hand-crafted features derived from the domain experts lead to considerably better classiﬁcation results. Deep neural networks derive high-level features automatically by utilizing large training samples. We presume that with more training data, a deeper neural network would outperform the traditional approach.  B. Field of Application  Classifying user feedback is an ongoing ﬁeld in research because of the high amount of feedback companies receive daily. Pagano and Maalej [31] show that, back in 2012, visible app vendors receive, on average, 22 reviews per day in the app stores. Free apps receive a signiﬁcantly higher amount of reviews (∼37 reviews/day) compared to paid apps (∼7 reviews/day). Popular apps such as Facebook receive about 4,000 reviews each day. When considering Twitter as a data source for user feedback for apps Guzman et al. [12] show that popular app development companies receive on average about 31,000 daily user feedback. Such numbers make it difﬁcult for companies – in particular with popular apps – to employ   TABLE IV: Conﬁguration of the best performing classiﬁcation experiments for the traditional machine learning and the deep learning approaches. RF = Random Forest, DT = Decision Tree. CNN = Convolutional Neural Network.  app review EN  inquiry  tweet EN  inquiry  tweet IT  inquiry  problem report  irrelevant  problem report  irrelevant  problem report  irrelevant  problem report inquiry irrelevant problem report inquiry irrelevant problem report inquiry irrelevant  g app review EN  tweet EN  tweet IT  g n i n r a e L  e n i ch a M  l a n o i t i d  Tra  n i n r a Le p e e D  RF(max features:None, n estimators:500). features:sentiment, tﬁdf, sampling:true, scaling:false DT(criterion:gini, max depth:1, min samples leaf:1, min samples split:4, splitter:random). features:tﬁdf, keywords, sampling:false, scaling:false DT(criterion:gini, max depth:8, min samples leaf:2, min samples split:4, splitter:random). features:n words,n stopwords, n tense, n pos, keywords, tﬁdf, sampling:false, scaling:false RF(max features:auto, n estimators:1000). features:sentiment, tﬁdf, sampling:true, scaling:true DT(criterion:gini, max depth:1, min samples leaf:1, min samples split:2, splitter:best). features:n words,n stopwords, n tense, n pos, keywords, tﬁdf, fastText, sampling:true, scaling:true RF(max features:none, n estimators:1000). features:n words,n stopwords, n tense, n pos, keywords, fastText, sampling:true, scaling:false RF(max features:log2, n estimators:1000) features:sentiment, n words,n stopwords, n tense, n pos, tﬁdf, sampling:true, scaling:true DT(criterion:entropy, max depth:8, min samples leaf:10, min samples split:6, splitter:random) features:n words,n stopwords, n tense, n pos, keywords, sampling:true, scaling:false DT(criterion:entropy, max depth:8, min samples leaf:8, min samples split:2, splitter:random) features:sentiment, n words,n stopwords, n tense, n pos, tﬁdf, keywords, sampling:false, scaling:true  CNN(dense number units:32, kernel size:3, number ﬁlters:16). sampling:true, scaling:true CNN(dense number units:32, kernel size:5, number ﬁlters:16). sampling:true, scaling:true CNN(dense number units:32, kernel size:5, number ﬁlters:16). sampling:true, scaling:true CNN(dense number units:32, kernel size:5, number ﬁlters:16). sampling:true, scaling:true CNN(dense number units:16, kernel size:5, number ﬁlters:16). sampling:true, scaling:true CNN(dense number units:32, kernel size:5, number ﬁlters:16). sampling:true, scaling:true CNN(dense number units:32, kernel size:5, number ﬁlters:16). sampling:true, scaling:true CNN(dense number units:32, kernel size:5, number ﬁlters:16). sampling:true, scaling:true CNN(dense number units:32, kernel size:5, number ﬁlters:16). sampling:true, scaling:true  a manual analysis on user feedback [15]. Therefore, gaining a deeper understanding of how to 1) ﬁlter noise and 2) how to extract requirements relevant information from user feedback is of high importance [27]. Recent advances in technology and scientiﬁc work enable new ways to tackle these challenges.  VI. RELATED WORK  In the paper “Toward Data-Driven Requirements Engineering”, Maalej et al. [28] describe the concept of User Feedback Analytics which contains the two sub-categories Implicit Feedback and Explicit Feedback. While Implicit Feedback deals with usage data such as click events that are collected via software sensors on, e.g., a mobile device, Explicit Feedback is concerned with written text such as app reviews. We focus on Explicit Feedback, which in the ﬁeld of requirements engineering often includes either app reviews [17], [16], [27], tweets [14], [37], product reviews such as Amazon reviews [24], [25], a combination of reviews and product descriptions [20], or a combination of platforms [30]. User feedback is essential to practitioners, as it contains valuable insights such as bug reports and feature requests [31]. The classiﬁcation of user feedback [27] was a ﬁrst step towards extracting such information. Further studies [24], [25] looked at classiﬁed feedback to analyze and understand user rationale—the reasoning and justiﬁcation of user decisions, opinions, and beliefs. Once a company decides to integrate, for example, an innovative feature request in the software product, it will be forwarded to the release planning phase [36]. In this work, we focus on the classiﬁcation of user feedback of app reviews and tweets.  App Review Classiﬁcation. Maalej et al. [27] present experiments on classifying app reviews from the Google Play Store and the Apple AppStore using traditional machine learning. In contrast to their work, we also apply deep learning, included tweets and work with two different languages (English and Italian). Chen et al. [3] introduce AR-Miner, a framework focusing on mining and ranking techniques to extract valuable information for developers following the idea of reducing manual effort. Dhinakaran et al. [8] perform app review classiﬁcation and enhance existing approaches with active learning to reduce the annotation effort for experts.  Tweet Classiﬁcation. Guzman et al. [14] and Williams and Mahmoud [37] present studies that assess the technical value of tweets for software requirements. Williams and Mahmoud [37] conclude that—after analyzing 4,000 tweets manually— about 51% of the tweets contain technical information useful for requirements engineering. Similarly, Guzman et al. [14] show that about 42% of their 1,350 manually analyzed tweets contain either bug reports, feature shortcomings, or feature requests. Conceptually, both studies follow similar goals and structure by: ﬁrst preprocessing the data; second classifying tweets into their speciﬁed categories; and third grouping similar tweets. Guzman et al. [14] go one step further and present a weighted function to rank tweets by their relevance. Compared to both papers, we have a strong focus on reporting feature engineering by testing diverse features and feature combinations (see Table II). Further, we perform the classiﬁcation on two different languages and employ deep learning as an addittonal experiment.  
2 | CONTRIBUTION  Our main contribution is a set of interpretable, risk-calibrated linear models that perform better than existing actuarial  risk assessments, and predict speciﬁc crime types. Other important aspects of our contribution are as follows:  • We consider multiple types of recidivism (general, violent, drug, property, felony, and misdemeanor) at two  time scales (six-month, two-year) for a total of 12 prediction problems.  • Our analysis was conducted on two criminal history data sets (one from Broward County, Florida, and the other from the state of Kentucky), which allowed us to understand variability in model performance across locations. We  found that models do not generalize well between locations, and conclude that models should be trained on data  from the location where they are meant to be used.  • We discuss how our models satisfy fairness and interpretability criteria. To our knowledge, beyond the ﬁelds of fair and interpretable machine learning, there are few paper that discuss both fairness and interpretability with the  same attention as predictive performance.  •  The risk models trained as part of this study are interpretable, and could potentially be useful in practice after a  careful, location-speciﬁc evaluation of their accuracy and fairness.  Similar to Zeng et al. [10], we use machine learning techniques optimized for interpretability, and address multiple  prediction problems. This work is an improvement over that of Zeng et al. [10] in the following ways. We use inter pretable machine learning techniques to create risk scores representing probabilities of recidivism, rather than making  binary predictions (the tools we use had not been not invented at the time of Zeng et al. [10]’s publication). We compare  with COMPAS and the Arnold Public Safety Assessment (PSA), two models currently used in the justice system, whereas  Zeng et al. [10] compared only with other machine learning methods. We use data obtained at the pretrial stage rather  than at prison-release. Since many jurisdictions utilize prediction instruments to determine pretrial release, this better  aligns with the use cases of risk scores. Our data come from two locations, and include more detailed information than  in Zeng et al. [10], and are more recent than 1994. Finally, models are assessed for multiple deﬁnitions of fairness (in  addition to performance).   WANG & HAN ET AL.  3 | BACKGROUND  5  Algorithmic risk assessment dates back to the early 1900s [18], and is used today at various stages of the criminal justice  system, such as at pretrial, parole, probation, or even sentencing. In this work, we focus on forecasting recidivism at the  pretrial stage. Though some states have implemented their own tools (Virginia, Pennsylvania, Kentucky), many utilize  systems produced by companies, non-proﬁts and other organizations [19]. These externally-produced risk assessments  and some of the jurisdictions that utilize them include COMPAS (Florida, Michigan, Wisconsin, Wyoming, New Mexico), the Public Safety Assessment (New Jersey, Arizona, Kentucky,2 Phoenix, Chicago, Houston), LSI-R (Delaware, Colorado,  Hawaii), and the Ohio Risk Assessment System [20, 21, 22]. The United States is not alone in using actuarial risk  assessments. Canada uses the Static-2002 to assess risk of violent and sexual recidivism [23]; the Netherlands uses the  Quickscan to assess static and dynamic risks of recidivism [24]; the U.K. uses the Offender Group Reconviction Scale to  predict reoffense while on probation [25].  3.1 | The Debate over Risk Assessments  Since the inception of actuarial risk assessments, there has been debate over whether they should be used in the  criminal justice system at all. Proponents claim that statistical models reduce overall violence levels and ensure the  most efﬁcient use of treatment and rehabilitative resources by helping judges identify the individuals that are truly  dangerous. A large body of evidence appears to support this claim. Various studies have shown that statistical models  are more accurate than human experts [26, 27]. Others have shown that a small percentage of individuals commit the  majority of crimes [28, 29, 30], indicating that correctly identifying dangerous individuals could lead to substantial  decreases in violence levels. Proponents also claim that risk assessments are instrumental to reducing racial/economic  disparity, allocating social services, and reducing mass incarceration [31]. In particular, some jurisdictions have adopted  risk assessments at the pretrial stage to replace cash bail, which is widely viewed as biased against poor defendants  [32, 33].  In practice, reducing overall violence levels, mass incarceration, and racial/economic disparity through actuarial risk  assessment is complex. Critics have argued that as recidivism prediction models always rely on racially-biased features  such as arrest records, actuarial risk assessment will only exacerbate racial and socioeconomic disparity, and should  therefore be abolished [34, 35]. In a well-known incident, ProPublica claimed that COMPAS was biased against African Americans because there was a disparity in false positive rates and false negative rates between African-Americans and  Caucasians [36]. Follow-up research showed that this bias was likely a property of the data generation process rather  than the COMPAS model, and that even a model that only relied on age showed a similar disparity in false positives  and false negatives [4]. Actuarial risk assessment might be vulnerable to feature bias, but it is important to remember  that other parts of the court system (such as bail and sentencing guidelines for judges) are not immune to feature bias  either—they also use criminal history and arrest records. Similarly, in one of the ﬁrst large-scale empirical studies,  Stevenson [37] showed that in Kentucky, the use of the Arnold PSA seemingly increased disparity between whites and  blacks at pretrial release. Because the risk scores were applied differently by judges in different counties, it seemed  that white people beneﬁted more than black people in terms of pretrial release numbers—but within the same county,  white and black defendants saw similar increases in release. Thus, rather than eliminating the use of risk scores, using  them uniformly across counties may have made risk assessments more fair across the state.  Others have argued that a fundamental ﬂaw with risk assessments is that their simple labels obscure the true  uncertainty behind their predictions [7]. This may be true for currently used risk assessments, but merely underscores  2Kentucky created and implemented their own tool in 2006 but transitioned to the Arnold PSA in 2013.   6  WANG & HAN ET AL.  the necessity for researchers to develop models which do quantify uncertainty. While actuarial risk assessments are not  perfect, we must remember that in the absence of risk assessments, judges can only rely on their intuition—and human  intuition has been shown to be less reliable than statistical models [26, 27, 33, 38].  Another problem is that some of the most widely used risk prediction algorithms are for-proﬁt and secret (e.g., COMPAS 3), yielding concerns over due process rights. In the 2017 Wisconsin Supreme Court case, Loomis v. Wisconsin,  Loomis challenged the use of the proprietary risk prediction software, COMPAS, on the grounds that this violated his  due process and equal protection rights [3]. Yet today, there are plenty of equally accurate, transparent risk prediction  tools that publish their guidelines and full models. See Table 4 in the Appendix for examples. In this article, we compare  against the Arnold PSA, an interpretable and publicly available tool which is used in multiple jurisdictions.  There is also a general fear that the use of risk assessments could lead to situations similar to those depicted in the  movie, “Minority Report”. In Minority Report, individuals were punished before they committed a crime based on oracles’  visions of the future. However, one of the major principles common to American criminal justice texts [40, 41] is that  individuals should be punished based on the crimes they committed in the past. This illustrates why risk assessments  have played only a minor role in sentencing. In reality, risk prediction tools are most heavily used in bail, parole, and  social services decisions.  Risk scores are no “magic bullet,” but abolishing risk assessment without a useful alternative plan will not solve the  problems above either. Reducing feature bias requires generations of community investment; jurisdictions must train  judges on how to use risk scores; and communities must provide treatment resources for those deemed high risk. Risk  assessments and other evidence-driven practices can be an important part of this solution. In the most recent revision  of the Model Penal Code, the American Law Institute has supported giving people shorter prison terms or sending them  to the community through the use of risk assessment tools [42, 43]. By providing simple and transparent risk scores,  we hope to mitigate the possibility that risk assessments are miscomputed, and enable judges and defendants to fully  understand their scores.  3.2 | Black-box and Interpretable Machine Learning for Predicting Criminal Recidivism  There is an abundance of past research on using machine learning methods to predict criminal recidivism. However,  many of these studies utilize black-box, non-interpretable models, and only optimize for predictive performance. For  instance, Neuilly et al. [44] used random forests to predict homicide offender recidivism. Other black-box models  applied to this problem include stochastic gradient boosting [45] and neural networks [46].  In comparison, there is relatively little work using interpretable machine learning techniques to forecast recidivism. It is not even clear how interpretability should be deﬁned in this domain4. Berk et al. [47] used classical decision trees to  build a simple screener for forecasting domestic violence for the Los Angeles Sheriff’s Department. In 2016, Goel et al.  [48] created a simple scoring system by rounding logistic regression coefﬁcients, which helped address stop-and-frisk  for the New York Police Department. Zeng et al. [10] was the ﬁrst work using modern machine learning methods that  globally optimized over the space of sparse linear integer models to predict criminal recidivism. Despite the range of  interpretable models that have been applied to the criminal recidivism problem, a common thread among these works is  that simple, interpretable models can do just as well as black-box models, and better than humans. For instance, Angelino  et al. [11] found that COMPAS shows no beneﬁt in accuracy over very simple machine learning models involving age  3While COMPAS’ guidelines are published and validation studies have been performed, the full forms of the models are not available and some of the validation studies do not conform to standards of open science (i.e., the validation data is not publicly available [39], or the studies’ authors are afﬁliated with the  corporations that produced the models. 4See Section 7 for a discussion of what we consider interpretable for the domain of criminal recidivism prediction   WANG & HAN ET AL.  7  and criminal history. Skeem et al. [38] showed that algorithms outperformed humans on predicting criminal recidivism  in three data sets, and demonstrated that the performance gap was especially large when abundant risk factors were  considered for risk prediction.  The approaches outlined above achieved interpretability through training models with interpretable forms. Another  major approach is post-hoc explainability, in which a simpler model provides insights into a black-box model. However,  post-hoc explanations are notoriously unreliable, or are not thorough enough to fully explain the black-box model [49].  Additionally, there seems to be no clear beneﬁt of black-box models over inherently interpretable models in terms of  prediction accuracy on the criminal recidivism problem [10, 24]. Thus, for a high-stakes problem such as predicting  criminal recidivism, we choose not to utilize these methods.  In fact, there have been cases in criminal justice where post-hoc explanations led to incorrect conclusions and  pervasive misconceptions about what information some of the most common recidivism models use. The 2016 COMPAS  scandal—where ProPublica reporters accused the proprietary COMPAS risk scores of an explicit dependence on race  [36]—was caused by a ﬂawed, post-hoc explanation of a black-box model. In particular, ProPublica reasoned that if  a post-hoc explanation of COMPAS depended linearly on race, then COMPAS depended on race (controlling for age  and criminal history). However, as Rudin et al. [4] demonstrated, just because an explanation model depends on a  variable does not mean that the black box model depends on that variable. Thus, ProPublica’s reasoning was incorrect.  In particular, this analysis found that COMPAS does not seem to depend linearly on some of its input variables (age),  and does not seem to depend on race after conditioning on age and criminal history variables. Criminologists have also  criticized the ProPublica work for other reasons [5]. Despite the ﬂaws in the ProPublica article, it is widely viewed as  being a landmark paper on fairness in machine learning.  A notable advantage of interpretable modelling for criminal justice is that some interpretable models allow a  decision-maker to incorporate factors not in the database in a way that black-box models cannot. For instance, scoring  systems (linear models with integer coefﬁcients) place all of the model inputs onto the same scale: every input receives  a number of points. The points of each factor in the model provide clarity on how important each input is relative to the  others.  3.3 | Fair Machine Learning  Fairness is a crucial property of risk scores. As such, the recidivism prediction problem is a key motivator for many of  these works. However, recidivism prediction is rarely the primary focus of fairness papers. Many of these papers seek to  make theoretical contributions by proposing deﬁnitions of fairness and creating algorithms to achieve these deﬁnitions,  using recidivism prediction as a case study [50, 51]. Others have proven fairness impossibility theorems, showing  when different fairness constraints cannot be achieved simultaneously. For instance, the two fairness deﬁnitions at the  heart of the debate over COMPAS’ fairness (calibration and balance for positive/negative class) cannot be achieved simultaneously in nontrivial cases 5 [52, 53]. These theorems show that many fairness deﬁnitions directly conﬂict,  so there cannot be a single universal deﬁnition of fairness [52, 54, 55]. Moreover, there is often a trade-off between  performance and fairness [56, 57, 58]. The emerging consensus is that any decision about the “best” deﬁnition of  fairness must rely heavily on model characteristics and domain-speciﬁc expertise.  The question of what should count as fair in criminal recidivism prediction can be answered by discussion among  ethicists, judges, legislators, and stakeholders in the criminal justice system. Existing American anti-discrimination law  provides a general legal framework for addressing this question. Under Title VII of the Civil Rights Act of 1964, there are  two theories of liability: disparate impact and disparate treatment [59]. In this article, we use the deﬁnitions of fairness  5However, by placing relaxations on the conditions, the fairness deﬁnitions can be approximately satisﬁed simultaneously.   8  WANG & HAN ET AL.  from the ﬁeld of fair machine learning, as they apply directly to machine learning models and are more speciﬁc than the  general legal guidelines of disparate impact and treatment. Moreover, some of the deﬁnitions of fairness proposed by  the ﬁeld of fair machine learning community are inspired by these guidelines. See Corbett-Davies and Goel [60] for a  detailed discussion of the relationship between algorithmic deﬁnitions of fairness and economic/legal deﬁnitions of  discrimination.  4 | DATA  In this study, we used criminal history data sets from Broward County, Florida, and the state of Kentucky, allowing us to  analyze how models perform across regions. The Broward County data set consists of publicly available criminal history  and court data from Broward County, Florida. This data set consists of the full criminal history, probational history,  and demographic data for the 11,757 individuals who received COMPAS scores at the pretrial stage from 2013-2014  (as released by ProPublica [36]). The probational history was computed from public criminal records released by the  Broward Clerk’s Ofﬁce. Though the full data set includes 11,757 individuals, this analysis includes only the 1,954 for  which we could also compute the PSA. We processed the Broward data using the same methods as Rudin et al. [4]. From  the processed data, we computed various features such as number of prior arrests, prior charges, prior felonies, prior  misdemeanors, etc.  The Kentucky pretrial and criminal court data was provided by the Department of Shared Services, Research and  Statistics in Kentucky. The data came from two systems: the Pretrial Services Information Management System (PRIM)  and CourtNet. The PRIM data contain records regarding defendants, interviews, PRIM cases, bonds, etc., that were  connected with the pretrial service interviews conducted between July 1, 2009 and June 30, 2018. The data from  CourtNet provided further information about cases, charges, sentences, dispositions, etc. When constructing features  from the Kentucky data set, we computed features that were as similar as possible to the Broward features (e.g., prior  arrests, prior charges with different types of crimes, age at current charge) in order to compare models between the  two regions. There are several features from Broward data which could not be computed from the Kentucky data, such  as “age at ﬁrst offense” and “prior juvenile charges”. A limitation of the Kentucky data set is that the policies governing  risk assessments changed over the period when the data was gathered, possibly impacting the consistency of the data  collection.  A difference in the data processing between the two data sets is that when constructing prediction features and  predictive labels, we considered non-convicted charges in the Broward data, but considered convicted charges in the  Kentucky data. The reason for this choice is sample size. The processed Broward data contains only 1,954 records, and  limiting the scope to convicted charges would yield only 1,297 records. The use of convicted versus non-convicted  charges between the two regions might explain some discrepancies in the results in Section 8, where we discuss the  generalization of recidivism prediction models between states. Note that many models currently implemented within  the justice system rely on non-convicted charges (for instance, counts of prior arrests), but for the applications such as  bail and parole, the use of non-convicted charges could be problematic—it holds individuals accountable for crimes that  they may not have committed. Please refer to the Appendix (Section 11) for more details on the data processing and a  full list of features for both data sets.   WANG & HAN ET AL.  5 | METHODOLOGY  9  Throughout our analysis, we compare with two tools that are currently used to predict recidivism in the U.S. justice  system: COMPAS (Correctional Offender Management Proﬁling for Alternative Sanctions) and the Arnold PSA (Public Safety Assessment, created by Arnold Ventures6). Although we would have liked to compare against more assessment  tools, many of them use data that are not publicly available, or are owned by for-proﬁt companies that do not release  their models. For a detailed discussion of the other risk assessments we considered and the features we were missing,  please consult the Appendix (Section 11).  More speciﬁcally, we compared our models against the Arnold PSA’s New Criminal Activity (NCA) and New Violent  Criminal Activity (NVCA) scores on the general and violent recidivism problems, respectively. Note that the timeframes and labels for prediction are important here, and our choices distinguish this study from past works on recidivism  prediction. Let us explain the time-frames next.  It is important that we chose ﬁxed time-frames for prediction, in our case, two years or six months past the current  charge dates. In reality, the scores are used to assess risks during the pretrial period. However, there is a huge amount  of variation in pretrial periods, which can span a few days to a few years: the average pretrial time-span in Kentucky is  109 days, and could last upwards of 3-4 years. Since the pretrial period depends on the jurisdiction, we chose to ﬁx  time-spans (of six months and two years) so that the models do not depend on the policy used for determining how long  the pretrial period would be. That way, the risk calculations we produce depend mainly on the inherent characteristics  of the individual, rather than the length of the pretrial period (potentially a characteristic of the jurisdiction). Also, this  way, individuals with the same propensity to commit a new crime within six months (or two years depending on which  risk score) are given identical risk scores, even if they have different expected time periods until their respective trials.  The six-month time-span represents an approximate length of pretrial period. The two-year time-span provides more  balanced labels, since two years provides more time to commit crimes than six months. Additionally, our evaluation  metric is AUC, which is a rank-statistic, and considers relative risk rather than absolute risk; that is, an individual who  actually commits crimes within two years of their current charge date should be ranked higher than an individual who  does not. The relative risk within the two-year time-frame is related to the relative risk for other (shorter or longer)  time-frames, allowing these models to potentially generalize to varying pretrial time-frames.  Another important aspect of our prediction problems is the deﬁnition of recidivism we chose. We predict the  occurrence of a convicted charge within six months/two years for Kentucky. In other words, we would like to predict  whether someone will be arrested, within six month or two years from their current charge, for another crime that they  were later convicted for. This deﬁnition potentially alleviates a due process concern: if we instead include non-convicted  charges, our models might be more likely to predict who will be arrested than who will be convicted, which is tied to  policing practices. For Broward, where we did not have conviction information for later charges, we predicted any charge  within six months/two years, which is the typical approach to recidivism prediction.  In Broward, we directly computed Arnold PSA scores, as the Arnold PSA is publicly available. The features used  by the Arnold PSA are provided in Tables 16 & 17 in the Appendix. In Kentucky, we used the unscaled Arnold PSA scores that came with the data set.7 We compared against COMPAS’ Risk of General Recidivism and Risk of Violent  Recidivism risk scores on the two-year general and two-year violent prediction problems, respectively (both models are designed to predict recidivism within two years). The COMPAS suite is proprietary, but COMPAS General and  Violent scores were provided with the Broward County data set (we do not compare against COMPAS on the Kentucky  data set). The COMPAS General and COMPAS Violent scores appear to have been developed for a parole population  6previously named the Laura and John Arnold Foundation 7In Kentucky, Arnold PSA scores are reported to judges without scaling.   WANG & HAN ET AL.  10  decisions).  [15], but have been applied for pretrial decisions in Broward. In this study, we consider the COMPAS scores for the  outcomes they were actually applied for (pretrial decisions), rather than the outcomes they were developed for (parole  In Sections 6 and 7, we compare the performance of black-box and interpretable algorithms on the Broward  and Kentucky data sets. We caution readers against comparing an algorithm’s performance in Broward with its  performance in Kentucky. An algorithm’s differences in performance between the data sets could be attributed to the  many differences between the two regions. For instance, the Broward data set is at the county level while the Kentucky  data set is at the state level. As the Kentucky data is at the state level, it embeds diverse information about 120 counties  (e.g., demographics, legislation, culture, local policing practices). Thus, in Sections 6 and 7, the comparisons between  baseline models and interpretable models are conducted within each data set. In Section 8, we discus in detail the  regional differences between Broward County and Kentucky, and present a set of experiments that illustrate model  performance gaps resulting from these regional differences.  5.1 | Prediction Labels  In addition to two-year general recidivism and two-year violent recidivism—the two types of criminal recidivism considered by COMPAS and the PSA—we computed recidivism prediction labels speciﬁc to various crime types, such as property, drug related recidivism and recidivism with felony or misdemeanor level charges.8 Note that an individual could have multiple positive labels, indicating that the newly committed crime involves multiple charge types. We  deﬁned recidivism as a recorded charge within a certain time frame. Out of all the possible recidivism prediction tasks  we considered, we selected the six most balanced: general, violent, drug, property, felony, and misdemeanor. To investigate the effect of temporal scale on predictive performance, we generated these six tasks using the time windows  two-years and six-months after the current charge date (or release date, if the individual went to prison for their current charge), for a total of twelve tasks. The summary of prediction tasks and the base rate of recidivism for each  task is provided in Table 1.  Kentucky  Broward  Labels  Two Year P (yi = 1) Six Month P (yi = 1) Two Year P (yi = 1) Six Month P (yi = 1) Explanation  20.4%  5.7 %  45.5%  21.8 %  General  Violent  Drug  Property  Felony  3.4%  8.7%  3.9%  9.6%  21.0%  9.3%  9.0%  17.6%  8.4%  4.0%  5.0 %  8.9 %  0.7%  2.0%  0.9%  2.4%  3.9%  yi = 1 if the defendant had any type of charge within two years (resp. six months) from current charge date/release date  yi = 1 if the defendant had a violent charge within two years (resp. six months) from current charge date/release date  yi = 1 if the defendant had a drug-related charge within two years (resp. six months) from current charge date/release date  yi = 1 if the defendant had a property-related charge within two years (resp. six months) from current charge date/release date  yi = 1 if the defendant had a felony-level charge within two years (resp. six months) from current charge date/release date  yi = 1 if the defendant had a misdemeanor-level charge within two years (resp. six months) from current charge date/release date  Misdemeanor  15.6%  27.2%  12.5 %  TA B L E 1 Label distributions for Broward and Kentucky.  8For clarity, we apply the typewrite font to indicate the prediction tasks.   WANG & HAN ET AL.  5.2 | Problem Setup  11  Due to the binary nature of recidivism tasks, we approached these prediction problems as binary classiﬁcation problems,  but do not binarize the ﬁnal predicted probabilities/scores of the machine learning models for the following reasons.  First, existing risk scores are usually nonbinary. For instance, the Arnold PSA’s unscaled New Criminal Activity (NCA)  score takes integer values from 0 through 13, while the COMPAS Risk of Recidivism and Risk of Violent Recidivism  scores take on integer values from 1 through 10 [15, 20]. Second, we want to create more nuanced risk scores both  by predicting highly-speciﬁc types of recidivism, (in addition to coarser categories like general recidivism), and by  presenting non-binary scores which reﬂect a range of risk values.  Since the predictions are nonbinary, we use Area Under the Curve (AUC) as our evaluation metric. This decision  also impacts the fairness metrics we assess, which we discuss in Section 9. We applied nested cross validation process  to train the models. Please refer to the Appendix (Section 11) for the details.  6 | BASELINE MACHINE LEARNING METHODS  To provide a basis of comparison for the interpretable models (presented in Section 7), we evaluated the performance of  six common, non-interpretable machine learning methods in this section. We present the baseline prediction results for  Broward and Kentucky in Tables 7 and 8 respectively. Baseline models and descriptions are provided below. The tuned  hyperparameters and packages used for each problem are provided in the Appendix (Section 11).  •  •  •  •  •  (cid:96)2 Penalized Logistic Regression: To prevent over-ﬁtting, there is an (cid:96)2 penalty term on the sum of squared coefﬁcients in the loss function for logistic regression. Although this method produces linear models, we consider  (cid:96)2-penalized logistic regression to be non-interpretable because if the number of input features is large, there could be a large number of nonzero terms in the model.  (cid:96)1 Penalized Logistic Regression: To prevent over-ﬁtting, there is an (cid:96)1 penalty term on the sum of absolute values of coefﬁcients in the loss function for logistic regression. This algorithm creates sparser models than (cid:96)2 penalized logistic regression. Notice that the sparsity of the model depends on the magnitude of the penalty and must be  balanced with consideration of prediction performance. In our experiments, (cid:96)1 models with Broward data were sparse yet maintained good predictive performances. However, the best (cid:96)1 models with Kentucky data still had too many features, which made it difﬁcult to interpret the results. Therefore, we classiﬁed (cid:96)1-penalized logistic regression as a non-interpretable algorithm.  SVM with a Linear Kernel [61]: An algorithm that outputs a hyperplane that separates two classes by maximizing  the sum of margins between the hyperplane and all points. Incorrectly classiﬁed points are penalized. Although  SVM with linear kernel yields a linear model, the concerns with (cid:96)1 and (cid:96)2 penalized logistic regressions apply here as well: the number of nonzero terms could be large, making it difﬁcult to interpret the model.  Random Forest [62]: An ensemble method that combines the predictions of multiple decision trees, each of which  is trained on a bootstrap sample of the data. The implementation we use combines individual trees by averaging  the probabilistic prediction of each tree. Random Forest is usually considered a black-box classiﬁer because it is  difﬁcult to understand the individual contribution of each feature (which can be found in many trees), and the joint  relationship between features.  Boosted Decision Trees [63]: An ensemble method where a sequence of weak classiﬁers (decision trees) are ﬁt to  weighted versions of the data. Similar to random forest, boosted decision trees produce black-box models because  it is difﬁcult to understand the joint relationships of the features. We use the XGBoost implementation [64].   12  WANG & HAN ET AL.  Major Findings: We found that all baseline machine learning algorithms performed similarly across recidivism  problems for the Kentucky data set. We also found that models performed better on the six-month prediction problems  than on the two-year problems on Kentucky data, but not on Broward data. These ﬁndings will be discussed throughout  the following subsections.  6.1 | Broward Baseline Results  Table 7 in the Appendix contains the performance of baseline algorithms on the Broward data; the results are visualized  in Figure 1 (presented below). We noticed that no algorithm consistently performs better than the others. Simple linear  models can even outperform black-box models in some prediction problems. For instance, in the two-year prediction  problems, (cid:96)2-penalized logistic regression and LinearSVM tie in performance for the general recidivism prediction. XGBoost performs the best in violent and property prediction problems. (cid:96)1-penalized logistic regression has the best performance in drug and felony prediction tasks, while (cid:96)2-penalized logistic regression has the best performance in misdemeanor recidivism prediction. The largest performance gap is 5.1%, from property recidivism prediction.  In the six month prediction problems, we see the same phenomenon that no single model dominates the others  in performance. Overall, the performance gaps across baseline models for the general, felony, and misdemeanor prediction tasks are small, while other prediction problems have larger gaps.  F I G U R E 1 Visualizations of Broward baseline results from Table 7 in the Appendix. Within each prediction problem, all algorithms performed similarly. No single algorithm consistently outperformed others.  6.2 | Kentucky Baseline Results  In Kentucky, complex and nonlinear baselines perform slightly better than linear models (see Table 8 in the Appendix  and Figure 4, which is presented below), potentially due to the larger size of the Kentucky data set (1,956 records in  Broward county versus 250K records in Kentucky). In particular, Random Forest and XGBoost uniformly perform  slightly better than all the other algorithms on all prediction tasks, over both time periods we examined. XGBoost  performs the best on all tasks. However, performance gaps, across all prediction problems and in both time frames, are  very small. Thus, we conclude that all the baseline algorithms perform similarly over the Kentucky data set . One thing  we noticed from the Kentucky results is that all algorithms perform slightly better on the six-month recidivism period   WANG & HAN ET AL.  than on the two-year period.  13  F I G U R E 2 Visualizations of Kentucky baseline results from Table 8 in the Appendix. Random Forest and XGBoost consistently perform better than other models, but the results are similar across all models.  7 |  INTERPRETABLE MACHINE LEARNING METHODS  For recidivism prediction, we considered several different types of interpretable machine learning methods with  different levels of interpretability, ranging from scoring systems to decision trees, to additive models. Since the Burgess  model in 1928 [65], recidivism risk assessments have traditionally been scoring systems, which are sparse linear models  with positive integer coefﬁcients. A scoring system can be visualized as a simple scoring table or set of ﬁgures. There  have only recently been algorithms designed to optimally learn scoring systems directly from data, without manual  feature selection or rounding. Scoring systems have several advantages: they allow an understanding of how variables  act jointly to form the prediction; they are understandable by non-experts; risks can be computed without a calculator;  and they are consistent with the form of model that criminologists have built over the last century, where “points” are  given to the individual, and the total points are transformed into a risk of recidivism. Further, outside information (such  as risk factors that are not in any database) can be more easily incorporated into the risk score: it is much easier to  determine how many points to assign to a new factor if the points are integer-valued for the known risk factors (e.g., we  could choose to subtract three points for drug treatment, to counteract four points of past drug-related arrests).  While scoring systems appear to be the accepted standard for interpretability in the domain of criminal justice,  imposing the constraints of linearity, sparsity, and integrality of coefﬁcients could potentially be strong enough to reduce  accuracy. Thus, we also consider modern algorithms that satisfy a subset of the conditions of interpretability (sparsity in  features, ability to visualize/explain any variable interactions, linearity, integer coefﬁcients). Speciﬁcally, we tested four  interpretable machine learning algorithms: Classiﬁcation and Regression Trees (CART), Explainable Boosting Machine  (EBM), Additive Stumps, and RiskSLIM (Risk-Calibrated Supersparse Linear Integer Models). Algorithm speciﬁcs are  articulated below and the tested hyperparameters are provided in the Appendix (Section 11). We also tested two  existing risk assessments—the Arnold PSA and COMPAS—and compared their performances to both baseline and  interpretable machine learning models.  •  Classiﬁcation and Regression Trees (CART) [62]: A method to create decision trees by continuously splitting input   WANG & HAN ET AL.  14  •  •  features on certain values until a stopping criterion is satisﬁed. CART constructs binary trees using the feature  and threshold that yields the largest information gain at each node. We constrain the maximum depth of the tree  to ensure that it does not use too many features. CART models are nonlinear. They cannot be written as scoring  systems, but can be written as logical models.  Explainable Boosting Machine (EBM) [12]: An algorithm that uses boosting to train Generalized Additive Models with a few interaction terms (GA2Ms). The contribution by each feature and feature interaction pair can be  visualized. The models are interpretable and modular, thus editable by experts. The models are generally not sparse,  and cannot be written as scoring systems.  RiskSLIM [66]: An algorithm that generates sparse linear models with integer coefﬁcients that have risk-calibrated  probabilities. The models generated by RiskSLIM have form similar to that of models used in criminal justice over  the last century.  • Additive Stumps: An interpretable variation on (cid:96)1-penalized logistic regression: for each feature, we generate multiple binary stumps (this pre-processing technique is discussed further in the next section), and apply (cid:96)1penalized logistic regression to these stumps. Ideally, the features will have monotonically increasing (or decreasing)  contributions to the estimated probability of recidivism. Models constructed using this method generally use  fewer features than those constructed with vanilla (cid:96)1-penalized logistic regression. These models are ﬂexible and nonlinear. These models also cannot be written as scoring systems because they are not sparse in the number of  nonlinearities.  • Arnold PSA [20]: A widely-used, publicly available, interpretable risk assessment system that consists of three scores: New Criminal Activity (NCA), New Violent Criminal Activity (NVCA), and Failure to Appear (FTA). We  compare against the NCA for the general recidivism problem, and against the NVCA for the violent recidivism problem, on both two-year and six-month time scales. The NCA has 7 factors, while the NVCA has 5 factors.  •  COMPAS [15]: A widely-used risk assessment system that consists of several scores, including the three that we  study: Risk of General Recidivism (COMPAS General), Risk of Violent Recidivism (COMPAS Violent), and Risk of  Failure to Appear. We compare against the COMPAS General score for the two-year general recidivism problem, and compare against the COMPAS Violent score for the two-year violent problem.  Major Findings: Overall, the best interpretable models performed approximately as well as the best black-box models  on both regions and both prediction time periods we considered.  7.1 | Pre-processing Features into Binary Stumps  We performed a data pre-processing technique for two of the interpretable machine learning algorithms: RiskSLIM and  Additive Stumps. This technique consists of transforming all original features into binary stumps (dummy variables)  using Equation 1. Pre-processing the features into stumps allows us to include nonlinear interactions between the  features (e.g. age, criminal history) and labels. It also allows us to visualize each Additive Stumps model as a set of  monotonically increasing (or decreasing) curves.  Formally, stumps are binary indicators, which are created by splitting features at pre-speciﬁed thresholds. For a  feature X (j ), and a set of threshold values K ∈ (cid:210), we generate decreasing stumps S (j ) k  for all k ∈ K as follows:  k =  S (j )   1,  for X (j ) ≤ k  0, else  (1)   WANG & HAN ET AL.  15  We can generate increasing stumps analogously by substituting ≥ for ≤ in the deﬁnition above. The rationale behind the naming convention is as follows. Linear models constructed from increasing (respectively, decreasing) stumps  have the nice property that if one sums the contribution from all stumps corresponding to a ﬁxed original feature for the feature X (j )), and the coefﬁcients ck are mostly non-negative 9, the resulting function (i.e., f (X (j )) = (cid:205) k ∈K  ck S (j ) k  f (X (j )) is monotonically increasing (respectively decreasing), which is desirable for interpretability.  More concretely, the “age_at_current_charge” feature ranges from 18 to 70 in our data. For all age-related features,  we construct decreasing stumps for k = {18, 19, ..., , 60}. We chose decreasing stumps for age features because based on past studies [e.g., 4, 67] and criminological theory [16, 68, 69] , the probability of recidivism decreases with age. On  the other hand, intuitively, the probability of recidivism should increase as criminal history increases. Thus, we construct  increasing stumps for the remaining features (which relate to criminal history).  To select a collection of stumps for the RiskSLIM and Additive Stumps model, we selected threshold values for all  features by examining each feature visualization from EBM and choosing the threshold values that correspond to sharp  drops in the predicted scores.  7.2 | Broward Prediction Results for Interpretable Models  Table 9 in the Appendix and Figure 3 show the results of interpretable models on the Broward data set. For all prediction  problems in both two-year and six-month prediction periods that we examined, we observed that CART consistently  performed worse than other algorithms. Additive Stumps and EBM performed similarly on all the prediction tasks and  outperformed other models, including the Arnold PSA and COMPAS, on most of the prediction tasks.  The performances of the best interpretable models are very similar to that of the best baseline models—this is true  for each of the prediction problems we considered. The AUC gaps between the best interpretable models and best  baseline models for all two-year prediction tasks range from 0.3% to 1.7% in absolute value, and range from 0.2% to  2.6% for six-month prediction tasks. The two maximum prediction gaps, 1.7% and 2.6%, both come from drug recidivism prediction tasks. Prediction gaps from all other problems are smaller than 1%.  F I G U R E 3 Broward interpretable model results.  9For decreasing (respectively increasing) stumps, if the coefﬁcient for the largest (respectively smallest) stump is negative, the function f will still be monotonic because the negative value will be subtracted from all values of the remaining stumps   16  WANG & HAN ET AL.  7.3 | Kentucky Prediction Results for Interpretable Models  The Kentucky prediction results are provided in Table 10 in the Appendix, and visualized in Figure 4. For all prediction  problems in both time frames we considered, CART, EBM, and Additive Stumps all had similar performances. RiskSLIM  had relatively lower results compared to other interpretable models. All interpretable models performed better than  the Arnold PSA, with the exception that the Arnold PSA performed slightly better (0.3%) than RiskSLIM on two-year  general recidivism. Once more, we observed that the best interpretable models can perform approximately as well as the best black-box models (XGBoost). For the two-year prediction tasks, the differences in performance between the  best interpretable and the best black-box models ranged from 0.7% to 0.9% in absolute value; for six-month problems,  the difference ranged from 0.4% to 1.5%.  F I G U R E 4 Kentucky interpretable model results.  Summary of Interpretable Models’ Results: We found that the best interpretable models performed approxi mately as well as the best black-box models, on both data sets and both time periods we considered, which is consistent  with previous studies on other data sets [10]. The best interpretable models possess the advantage of being transparent  and interpretable, allowing judges and defendants a better understanding of the predictions that the model outputs.  7.4 | Tables and Visualizations of Interpretable Models  Each of the interpretable machine learning methods produces models that can be visualized, either as a decision tree  (CART), scoring table (RiskSLIM), or as a set of visualizations (EBM, Additive Stumps). In this section, we present these  tables and visualizations for EBM, Additive Stumps and RiskSLIM, to give a clearer understanding of each model’s  interpretability. Here we used the two-year general recidivism prediction problem on Kentucky data as an example.  7.4.1 | EBM Models  The EBM package provides visualizations for each feature in the data set along with a bar chart of feature importance,  both of which are displayed in an interactive dashboard. The dashboard allows users (potentially judges) to see the  scores corresponding to each bar or line by hovering the mouse over it. EBM models are not sparse in the number of  features, so there could be visualizations for all features. Here, we show screenshots of the bar chart and visualizations   WANG & HAN ET AL.  17  for the three most important features. EBM visualizations are similar to those from Additive Stumps, in that each  feature’s contribution to the score can be displayed separately. However, EBM scores do not tend to be monotonically  increasing or decreasing in each feature.  F I G U R E 5 Visualizations from EBM on two-year general recidivism. Top left: overall importance of each feature, ranked from the most important variable to least important. Remaining three: visualization for the contribution of the feature to the overall score (top) and histograms of feature values to show the distribution (bottom). Features contributions are visualized as bar charts if the feature takes binary value. The shaded grey area represents the conﬁdence region. We see that as values get larger, there is more uncertainty in the predictions, which may be because we have fewer data points for such large feature values.  7.4.2 | Additive Stumps  Additive Stumps models are constructed by thresholding the original features, such as age or criminal history, into binary  stumps, followed by running (cid:96)1-penalized logistic regression on the stumps. Choosing an appropriate regularization value for (cid:96)1-penalized logistic regression can give us a model that is sparse in the number of original features—despite the fact that the regularization is directly on the stumps, not on the original features. For the Kentucky two-year  general recidivism problem, the ﬁnal model contains 28 stumps plus an intercept. These stumps are rooted under only 14 original features. Visualizations of the contributions for these 14 features are presented in Figure 6. Table 11,  containing a scoring table that includes all 28 stumps plus an intercept, is provided in the Appendix (Section 11).   18  WANG & HAN ET AL.  F I G U R E 6 Visualizations of the total contribution for each of the original features in the Additive Stumps model on two-year general recidivism. The contribution from each stump feature is the estimated coefﬁcient from (cid:96)1-penalized logistic regression.  7.4.3 | RiskSLIM  RiskSLIM produces scoring tables with coefﬁcients optimized to be integers (“points”), which makes the predictions  easier to calculate and interpret for users, such as judges. The total points are translated into probabilities using the  logistic function provided at the top of the table. By examining a RiskSLIM model, users can easily identify which  features contribute to the ﬁnal score and by how much. We provide scoring tables in Table 2 for two-year general recidivism prediction on both Broward and Kentucky data sets. More tables are provided in the Appendix (Section 11).  We noticed that for each prediction problem, almost all ﬁve of the cross validation folds for the RiskSLIM algorithm  yielded the same model on the (larger) Kentucky data set. In more detail, for Kentucky two-year drug and violent recidivism prediction problems, all ﬁve RiskSLIM models produced during cross validation were identical. For the rest of  the prediction labels, four out of ﬁve cross validation models were the same. For the six-month recidivism prediction  problems, the misdemeanor prediction problem resulted in ﬁve identical RiskSLIM models, and the violent recidivism   WANG & HAN ET AL.  19  prediction problem had four models that were the same. The fact that the Kentucky RiskSLIM models are often the  same, despite being trained on different (albeit overlapping) subsets of data, suggests that they are robust to the exact  subsample used for training.  TA B L E 2 Two-year general recidivism RiskSLIM models for Broward (left) and Kentucky (right). Each feature is given an integer point. The ﬁnal predicted probability is calculated by inputting the total score to the logistic function provided on the top of the tables.  Broward  Kentucky  Pr(Y = +1) = 1 / (1 + exp(-(-2 + score)))  Pr(Y = +1) = 1 / (1 + exp(-(-2 + score)))  age at current charge ≤31  1 points  +...  number of prior arrest≥ 2  number of prior misdemeanor charges ≥4  1 points  number of prior arrest≥ 3  had charge(s) within last three years = Yes  1 points  number of prior arrest≥ 5  ...  ...  1 points  1 points  1 points  +...  +...  +...  ADD POINTS FROM ROWS 1 TO 3  SCORE  = ....  ADD POINTS FROM ROWS 1 TO 3  SCORE  = .....  8 | RECIDIVISM PREDICTION MODELS DO NOT GENERALIZE WELL ACROSS  REGIONS  It is common practice for recidivism prediction systems to be applied across states, or even countries, with only minor  tuning on local populations. Implicit in this practice is the assumption that models trained on data from one collection of  locations will perform well when used in another collection of locations—i.e., that models generalize across locations. For  instance, the Arnold PSA, which was developed on 1.5 million cases from approximately 300 U.S. jurisdictions, has been  adopted in the states of Arizona, Kentucky, New Jersey, and many large cities including Chicago, Houston, Phoenix, etc.  [20]. These systems have remained in place for years without any updates.  However, based on our experimental results, we conjecture that different locations would beneﬁt from specialized  models that conform to the speciﬁc aspects of each location. For instance, let us brieﬂy compare the state of Kentucky  and Broward County in Florida. The demographics are completely different: Kentucky is not a diverse state (87.8%  white, 7.8% black, and 4.4% other groups in 2019 [70]), whereas Broward County is more racially diverse (62.3%, white;  17.1% Hispanic or Latino; 12.2% black or African American; 5.07% Asian and other groups [71]. The geographies of  the locations are drastically different as well: Kentucky is an interior state located in the Upland South with a humid  subtropical climate, whereas Broward County is at the eastern edge of Florida with a tropical climate. Several studies  have indicated an association between climate (temperature, humidity, and precipitation) and crime [72, 73, 74]. There  are many other factors that differ between the locations that might affect the generalization of the recidivism prediction  models, such as different local prosecution practices, laws and the way they are administered, social service programs,  local cultures, educational systems, and judges’ views.  Because models tend to be used broadly across locations, in this section we aim to investigate how well predictive  models generalize between the two locations for which we have data. We trained models on Kentucky and tested on  Broward (and vice versa). We looked more closely at age, and examined how the joint probability distribution of age  and recidivism differs between Broward and Kentucky. We focused on age because of its important relationship to  recidivism [67, 69, 75].  Major Findings: Our analysis shows that models do not generalize well across regions, and the joint probability  distribution of age and recidivism varies across states. Therefore, we suggest that different models be constructed in   20  WANG & HAN ET AL.  different regions, and be updated periodically.  8.1 | Training on One Region and Testing on the Other  In order to construct models on one region and test them on the other, we only used the shared features from both  data sets. Nested cross validation was used to train both the models that were trained in one region and tested in the  other, and the models that were trained and tested in the same region. More details about this procedure can be found  in Section 11.7 in the Appendix. Table 12 and Table 14 in the Appendix respectively show the performance of models  trained on Kentucky and tested on Broward, and models trained on Broward and tested on Kentucky. Table 13 and  Table 15 respectively show the performance of models trained and tested on Broward, and models trained and tested  on Kentucky.  Comparing Table 12 with Table 13, we observed that there is an overall decrease in model performance when  models were trained in Kentucky and tested on Broward. For instance, for the two-year general recidivism problem, the performances drop between 3.5% to 6.0% on the baseline models. A similar pattern can be observed for the  interpretable models. Conversely, when we trained models on Broward and tested on Kentucky, we observed even  larger performance decreases from the models trained and tested on only Kentucky (compare Table 14 to Table 15). For  the two-year general prediction task, performance gaps from baseline models range between 5.1% and 8.6%, while the gaps range from 4.6% to 12.0% for interpretable models.  Through this experimentation, we concluded that for at least the twelve prediction problems in our setup, models  do not generalize across states. This could be attributable to differences in the joint probability distribution of features  and outcomes between locations. To understand the difference in these distributions more closely, we examine the age  feature.  8.2 | Age-Recidivism Probability Distributions by Region  Age has traditionally been a highly predictive factor for recidivism [67, 69, 75]. Therefore, differences in the age  distributions between two regions could signiﬁcantly impact a model’s ability to generalize between regions.  Consider the general recidivism problem as an example. In Kentucky, the probability of general recidivism for both six-month and two-year prediction periods peaks for individuals aged around the early to mid 30s and then  decreases as age increases. In Broward County, the age distribution for the corresponding general recidivism problem is substantially different. From Figure 7, the probabilities seem to peak around ages 18-29, and then decrease after  age 29. There are less data for higher ages, causing greater variance in the probabilities. For the violent recidivism problem, please refer to Figure 11 in the Appendix (Section 11).  Additionally, there is a large gap in the probability magnitudes between the two regions. For instance, the probabili ties of general recidivism from the Broward data set can exceed 0.5, while the probabilities of general recidivism from Kentucky data are all less than 0.4. Thus, the populations of individuals from Broward and Kentucky who recidivate are  different with respect to age.  This difference is directly manifested in the interpretable models presented in Section 7.4. We found that the  selection of features differs between interpretable models trained on Broward and Kentucky data. For instance,  referring to the simple RiskSLIM models listed in Section 11.8 in the Appendix, which show the most important features  in each prediction problem, we noticed that with Broward data, almost all prediction problems contain at least one  age feature, either “age at current charge” or “age at ﬁrst offense.” This suggests that age is important in predicting  recidivism across different problems trained on the Broward data. However, none of the RiskSLIM models trained   WANG & HAN ET AL.  21  F I G U R E 7 Probability of recidivism v. age at current charge—general recidivism  on the Kentucky data set use age features. Almost all the models use “prior arrest” features, reﬂecting the fact that  Kentucky recidivism prediction problems rely more on prior criminal history information than on age.  9 | FAIRNESS  In this section, we conduct a technical discussion of a small fraction of the various fairness deﬁnitions that have emerged  recently, and an evaluation of how well the interpretable models satisfy them on the Kentucky data set. We ﬁrst describe  our rationale for selecting fairness deﬁnitions (calibration, balance for positive/negative class, and balanced group AUC).  Next, we evaluate how well the Arnold PSA, COMPAS, EBM (the best-performing interpretable models) and RiskSLIM  (the most interpretable and most constrained models) satisfy these deﬁnitions on the two-year general recidivism and two-year violent recidivism problems in Kentucky. Finally, we discuss how current fairness-enforcement procedures interact with interpretability.  Major Findings: Empirically, we found no egregious violations of the three fairness deﬁnitions (group calibration,  BPC/BNC, and BG-AUC) for both interpretable machine learning models we assessed (EBM and RiskSLIM) for the  two-year general recidivism problem on the Kentucky data set. We found that the Arnold NCA raw score violated one of the fairness deﬁnitions (BPC/BNC). Overall, we observed a larger gap in fairness (for all three fairness measures we  examined) between the largest and smallest sensitive groups, than between black and white sensitive groups. We also  note that existing techniques to enforce fairness generally require non-interpretable transformations, and therefore do  not work well with interpretable models.                        S U R E D E L O L W \ O R F D W L R Q     . < W Z R B \ H D U V L [ B P R Q W K                                                                                               D J H B D W B F X U U H Q W B F K D U J H                      S U R E D E L O L W \ O R F D W L R Q     ) / * H Q H U D O  5 H F L G L Y L V P 9.1 | Selection of Fairness Metrics: Calibration, Balance for Positive/Negative Class, Bal anced Group AUC  As discussed in Section 5.2, we do not wish to consider binary risk scores in this study. This decision limits us to a much  smaller class of fairness deﬁnitions (e.g., statistical parity would not be relevant). Below, we summarize the deﬁnitions  that apply to regression that we do not consider and the reasons why:  WANG & HAN ET AL.  Fairness through unawareness states that a model should not use any sensitive features [55]. However, if there  are proxies for sensitive features present in the data set, the model can still learn an association between a sensitive  group and the outcome. Fairness through unawareness could be used if one decides that a proxy feature is  permissible to use (e.g., if one decided that age could be used, despite its correlation with race), but we do not  presume that this is what is desired for this application. Of course, if fairness through unawareness is desired, it is  easy to construct models that satisfy this deﬁnition.  Individual fairness intuitively requires that “similar” individuals are treated “similarly” by the model—individuals  with similar features should be given similar model scores. This type of fairness requires manually (and thus  subjectively) deﬁning a notion of similarity between individuals [76]. This type of subjective choice goes beyond the  scope of this paper.  Once we limited ourselves to real-valued outcomes and eliminated the above deﬁnitions, only a few deﬁnitions  remained. In a literature search for non-binary fairness deﬁnitions, we found the following: calibration, balance for  positive class/balance for negative class (BPC/BNC) and balanced group AUC (BG-AUC).  Below, G denotes a (categorical) sensitive attribute such as race, and gi denotes one of the sensitive groups in G (e.g. African-American, Caucasian, and Hispanic, for the sensitive attribute of race). Y ∈ {0, 1} denotes the ground-truth label (recidivism status) and S denotes the predicted score from a model.  Calibration: We consider two notions of calibration. The ﬁrst, group calibration, requires that for all predicted  scores, the fraction of positive labels is the same across all groups. Mathematically, group calibration over the  sensitive attribute G requires:  P (Y = 1|S = s, G = gi ) = P (Y = 1|S = s, G = gj ), (cid:91)i , j  where s is the given value of a risk score.10 In practice, it is common to bin the score S if there are many possible values. The second, monotonic calibration, requires that if s1 < s2, then P (Y = 1|S = s1) < P (Y = 1|S = s2). 11 These types of calibration are of particular concern to designers of current recidivism risk models. Group calibration  means that a risk score holds the same “meaning” for each race. Monotonic calibration means that if the score  increases, the risk also increases. These notions are important because human decision-makers expect risk scores  to have these intuitive properties (but not all algorithms produce calibrated models) [77].  Balance for Positive Class (BPC) requires that for all individuals with a positive label, the expected values of the  22  •  •  •  •  10In the case where scores are binary, group calibration is equivalent to requiring conditional use accuracy equality. 11We note that a real-valued score S between 0 and 1 is well-calibrated if P (Y = 1|S = s) = s . Well-calibration says that the predicted probability of recidivism should be the same as the true probability of recidivism [55]. Although well-calibration is the deﬁnition of calibration that is standard in the statistics  community, we consider monotonic-calibration here because any score that is monotonically-calibrated can be transformed to be well-calibrated.   WANG & HAN ET AL.  23  predicted scores are the same across groups. Mathematically, a risk score S satisﬁes BPC if:  E [S |Y = 1, G = gi ] = E [S |Y = 1, G = gj ], (cid:91)i , j .  Similarly, a risk score S satisﬁes Balance for Negative Class (BNC) if:  E [S |Y = 0, G = gi ] = E [S |Y = 0, G = gj ], (cid:91)i , j .  BPC and BNC differ only in the labelY .12 Intuitively, BPC means that the average score for recidivists is the same in each group, while BNC means that the average score for non-recidivists is the same in each sensitive group.  BPC/BNC is an intuitive notion of fairness, which says that it is permissible to give consistently higher (respectively  lower) scores to individuals who truly belong to the positive (respectively negative) class. However, BPC/BNC  limits the set of attributes where it is permissible to “discriminate” between individuals, to the label Y . Suppose the count of prior offenses is an important feature for a recidivism prediction model—higher prior counts lead  to higher scores. This is a reasonable model assumption because a higher prior count is correlated with higher  recidivism rates. If on average, African-Americans have higher prior counts than Caucasians, the model will not  satisfy BPC/BNC. For a model to satisfy BPC/BNC, it must give the same average score to individuals from a certain  race and with a certain recidivism label, regardless of distributional differences in prior counts. Those who believe  that prior counts and arrests are racially biased against African-Americans might ﬁnd this a desirable property of  a fairness deﬁnition. On the other hand, those who ﬁnd this undesirable can ﬁx this by conditioning on the prior  counts attribute as well.  •  Balanced Group AUC (BG-AUC) requires that the AUC of the risk score is the same for each sensitive group. This  deﬁnition is our adaptation of overall accuracy equality [56], which asks that the score’s accuracy is the same for  each sensitive group. Our risk scores are not binary so we do not assess accuracy in this work, but assessing the  AUC for each group is the natural analog.  Sensitive attributes: The two sensitive attributes that are available in the Kentucky data sets are race and gender.  In the Kentucky data set, all individuals are partitioned into Caucasian, African-American, Indian, Asian, and Other, but we group the Indian and Asian attributes into Other because there are very few individuals with these attributes. See Table 19 for the distribution of sensitive attributes in Kentucky. The Kentucky data set also partitions  individuals into the genders Female and Male. To summarize,  races in Kentucky = {Caucasian, African-American, Other}  sexes in Kentucky = {Female, Male}.  We remark that the binary versions of calibration and BPC/BNC deﬁnitions conﬂicted during the COMPAS scandal.  Investigative journalists from ProPublica found that COMPAS had a higher false negative rate for Caucasians and a higher false positive rate for African-Americans.13 In response, however, Northpointe claimed that COMPAS scores  were calibrated.  Kleinberg et al.’s [52] impossibility theorem demonstrated that the conﬂict between calibration and BPC/BNC holds  in general. In particular, they show that if a model does not satisfy either of the two trivial cases—a model that always  12In the binary case, BPC/BNC is equivalent to equalized odds [50], which requires that false positive rates and false negative rates are equal for each group. 13To determine false negative/positive rates, ProPublica binned COMPAS scores into binary scores,   24  WANG & HAN ET AL.  makes perfect predictions, or a data set where the base rates of recidivism are equal for each sensitive group—then the  model cannot satisfy all three fairness deﬁnitions simultaneously. However, a relaxed version of the theorem states  that, if either of the two conditions approximately hold (approximately perfect predictions, or approximately equal base  rates), then the three fairness deﬁnitions can be approximately satisﬁed at the same time.  Figure 10 in the Appendix shows that the base rates for all sensitive attributes under each prediction problem on  the Kentucky data. We noticed that for the two-year general and violent recidivism problems, the base rates are similar to each other (less than 3% in differences) across gender and race categories (except for “Other”). Given the  relaxed version of the impossibility theorem, together with the relaxed criteria of the 3% difference we considered, we  expect that the three fairness deﬁnitions can all be approximately satisﬁed.  9.2 | Fairness Results  We assessed model fairness only on the Kentucky data because the Broward data has a limited sample size, potentially  making the fairness results unreliable. (We attempted the evaluation on Broward data, but conditioning on race/gender  and the true label/score in the Broward data led to subgroups that were too small, and therefore noisy results.) We  compared the interpretable models, EBM and RiskSLIM, to the Arnold PSA on Kentucky. EBM has the best performance  on most of the prediction problems on the Kentucky data set. RiskSLIM performs relatively worse, but is considerably  simpler as there are no more than ﬁve features in each model, coefﬁcients are integers, and the model is linear.  We evaluated the two-year general and two-year violent problems, as they are the primary problems that the Arnold PSA is used for. For the two-year general problem, we evaluated the unscaled Arnold New Criminal Activity (NCA) score; for the two-year violent problem, we assessed the unscaled Arnold New Violent Criminal Activity (NVCA) score. Although Arnold Ventures provides a table to scale the Arnold scores, in Kentucky, judges are presented with the  unscaled scores along with a categorization of the scores as low, medium, and high risk. Results for two-year general recidivism are presented directly in this section; results for two-year violent recidivism can be found in the Appendix.  Note that each of the fairness conditions implicitly has a threshold parameter. For instance, the fairness condition  BPC is strictly satisﬁed if the mean scores between multiple groups are “equal” to each other. However, whether two  numbers are approximately equal is subjective and requires a threshold. So one must always issue a disclaimer when  stating that any of these fairness conditions are satisﬁed. Hence we remark that subjective thresholds were used to  determine whether fairness conditions were approximately satisﬁed in what follows.  | Calibration  As Figure 8 shows, the Arnold NCA raw score does not satisfy monotonic calibration for race or gender groups. The  score approximately satisﬁes group calibration for race (excluding the “Other” group) for all score values except for  13, and approximately satisﬁes group calibration for gender for all score values less than 11. The reason why higher  Arnold NCA raw scores fail the calibration deﬁnitions may be that there are few individuals with higher scores in  the data set , thus making the results less stable. Interestingly, we found that the scaled version of Arnold NCA fully  satisﬁed monotonic and group calibration, but had slightly worse predictive performance. EBM and RiskSLIM both  satisfy monotonic calibration and group calibration for all gender and race groups (excluding the “Other” group).   WANG & HAN ET AL.  25  (a) For the Arnold NCA raw score, none of the curves are mono tonically increasing—violating monotonic calibration. Cali bration curves for the Caucasian and African-American race  groups lie close to each other (except for the score 13), ap proximately satisfying group calibration. Curves for the male  and female groups lie close to each other, but diverge for  scores greater than 10, indicating that the score satisﬁes  group calibration for gender for scores less than or equal to  10.  (b) For EBM, except for the “Other” group, the calibration  curves are monotonically increasing and approximately  equal to each other (satisfying monotonic calibration and  group calibration).  (c) For RiskSLIM, the domain of the graph goes up to only 0.3 − 0.4 (unlike the other graphs). The curves are monotonically increasing and overlap with each other with the “Other” cat egory being lower than all the other ones. Thus, RiskSLIM  is approximately group calibrated and monotonically cali brated.  F I G U R E 8 Calibration results for the Arnold NCA raw, EBM and RiskSLIM for two-year general recidivism on Kentucky.  0.02.55.07.510.012.5Arnold NCA Raw Score0.00.20.40.60.81.0P(Y = 1 | Score = score, Attr = attr)Calib. of Arnold NCA Raw on general_two_year in KentuckyAll individualsAfrican-AmericanCaucasianOtherfemalemale0.0-0.10.1-0.20.2-0.30.3-0.40.4-0.50.5-0.60.6-0.70.7-0.80.8-0.90.9-1.0EBM Score0.00.20.40.60.81.0P(Y = 1 | Score = score, Attr = attr)Calib. of EBM on general_two_year in Kentucky0.0-0.10.1-0.20.2-0.30.3-0.40.4-0.50.5-0.60.6-0.70.7-0.80.8-0.90.9-1.0RiskSLIM Score0.00.20.40.60.81.0P(Y = 1 | Score = score, Attr = attr)Calib. of RiskSLIM on general_two_year in Kentucky 26  WANG & HAN ET AL.  | Balance for Positive/Negative Class (BPC/BNC)  For models that provide risk probabilities as output (namely EBM and RiskSLIM models), we apply a 3% rule to determine  whether BPC and BNC conditions are satisﬁed. The unscaled Arnold NCA produces scores between 0 and 13 rather  than probabilities, so we use a 0.4 difference threshold to determine whether BPC and BNC are satisﬁed. That is, if the  difference in scores between the two groups is greater than the threshold, then we conclude that the model violates  BPC/BNC. All conclusions we present below exclude the “Other” race group, because of its small sample size.  Figure 9(a) displays the BPC/BNC results for the Arnold NCA raw, and shows that Arnold NCA satisﬁes neither  BNC nor BPC on gender or race groups. Figures 9(b) shows that EBM satisﬁes both BPC and BNC on race groups. EBM  also satisﬁes BNC, but not BPC, on gender groups. Figure 9(c) displays the results for RiskSLIM, which satisﬁes both  BPC and BNC on race and gender groups.  | Balanced Group AUC (BG-AUC)  We determine whether the models satisfy BG-AUC using a 3% rule. In Kentucky, AUC values are stable across sensitive  attributes for all models, satisfying BG-AUC for both gender and race. The discrepancies in AUC between African Americans and Caucasians range from 0.3% (RiskSLIM) to 2.1% (Arnold NCA raw). The range gets smaller for gender  groups, lying between 0.5% (Arnold NCA) to 1.3% (RiskSLIM). Hence, we found that the Arnold NCA raw, EBM and  RiskSLIM all satisfy Balanced Group AUC for the race (excluding the “Other” group) and gender groups.  TA B L E 3 AUCs of the Arnold NCA Raw, EBM and RiskSLIM on Kentucky, conditioned on sensitive attributes. AUC ranges are given for each sensitive attribute.  Kentucky  Race  Sex  Model  Label  Afr-Am.  Cauc.  Other Race  race_range  Female  Male  sex_range  Arnold NCA Raw  general_two_year  0.692  0.713  0.653  0.059  0.714  0.709  0.005  EBM  general_two_year  0.742  0.751  0.696  0.055  0.745  0.753  0.008  RiskSLIM  general_two_year  0.705  0.708  0.620  0.088  0.699  0.712  0.013  Summary of Fairness Results: For the two-year general recidivism problem on the Kentucky data set, we found no egregious violations of the three fairness deﬁnitions (group calibration, BPC/BNC, and BG-AUC) for either of the  interpretable machine learning models we assessed (EBM and RiskSLIM), but we did ﬁnd small violations. We found the  Arnold NCA raw score violated one of the fairness deﬁnitions (BPC/BNC).  In more detail, we found that balanced group AUC were approximately satisﬁed for all three models with respect  to both gender and race groups (except for “Other”). With respect to calibration, both EBM and RiskSLIM satisﬁed  monotonic and group calibration on both gender and race groups (except for the “Other”). Arnold PSA approximately  satisﬁed group calibration on race (excluding risk score 13) and gender groups for scores less than 11. Additionally, EBM  satisﬁed both BPC and BNC on race categories, while it only satisﬁed BNC on gender categories. RiskSLIM satisﬁed  both deﬁnitions on both sensitive attributes. The Arnold NCA satisﬁed neither BPC nor BNC on race and gender groups.  A caveat is that we limited the discussion of the race groups to Caucasians and African-Americans—otherwise the  “Other” group would have caused all models to fail all deﬁnitions of fairness (calibration curves for the “Other” group are  signiﬁcantly beneath curves for other groups, average predicted scores vary substantially from the other groups, and  prediction AUC is signiﬁcantly lower for the “Other” group). This may be because we have the least data for the “Other”   WANG & HAN ET AL.  27  (a) Differences in expected scores for African-Americans  and Caucasians are greater than the threshold (0.4):  0.79 (race, negative class), 0.61 (race, positive class).  Differences in expected scores for gender are also  greater than the threshold: 0.7 (gender, negative  class) and 0.84 (gender, positive class).  (b) Differences in expected scores for African-Americans  (c) Difference in expected scores for African-Americans and  and Caucasians are less than 0.03: 0.01 (race, negative  Caucasians are less than .03: 0.01 (race, negative class),  class), 0.01 (race, positive class). Differences in expected  0 (race, positive class). Differences in expected prob scores for gender satisfy the threshold in negative class  abilities for the male and female groups are less than  but not positive class: 0.01 (gender, negative class), 0.04  0.03: 0.00 (gender, negative class), 0.02 (gender, positive  (gender, positive class).  class).  F I G U R E 9 Balance for Positive and Negative Class for the Arnold NCA raw score, EBM and RiskSLIM on the two-year general prediction problem in Kentucky. Red line indicates the maximum value output by models.  general_bncgeneral_bpc0.02.55.07.510.012.5E(Arnold | Attr = attr, Y = i)3.745.682.955.071.833.392.574.573.275.41BPC/BNC for Arnold on general_two_year in KentuckyAfr-Am.Cauc.Other RaceFemaleMalegeneral_bncgeneral_bpc0.00.20.40.60.81.0E(EBM | Attr = attr, Y = i)0.090.280.080.270.030.120.070.240.080.28BPC/BNC for EBM on general_two_year in KentuckyAfr-Am.Cauc.Other RaceFemaleMalegeneral_bncgeneral_bpc0.00.20.40.60.81.0E(RiskSLIM | Attr = attr, Y = i)0.20.350.190.350.140.220.190.330.190.35BPC/BNC for RiskSLIM on general_two_year in KentuckyAfr-Am.Cauc.Other RaceFemaleMale 28  WANG & HAN ET AL.  race group, which is only 2.49% of the total sample. To ensure fairness, it is important that comparable amounts of data  are gathered for each sensitive group when possible. However, in non-diverse states such as Kentucky, there may not  be enough individuals in minority groups to create a large enough statistical sample.  9.3 | A Discussion on the Interaction between Fairness and Interpretability  There are signiﬁcant hurdles to using current fairness techniques with interpretable models. Moreover, the vast  majority of the work on fairness has focused on the binary classiﬁcation case. Thus, few deﬁnitions of fairness (let alone  algorithms) work for problems where predictions are nonbinary.  We did not attempt to use fairness-enforcement techniques because many fairness techniques require a non interpretable transformation (further discussed below). Once these transformations are made, there is no way to correct  them to produce an interpretable model afterwards. There are generally three approaches to fairness algorithms: pre processing of features [78], altering the training loss function [79, 80], and post-processing of predictions [50, 51, 53].  The pre-processing steps are generally complicated transformations of the input features, which shreds the data’s  natural meaning. Similarly, post-processing approaches either transform the predictions in some way, performing  “fairness corrections” [53] (which are non-interpretable), or require threshold selection, which is contrary to our goals of  providing non-binary risk assessments [50]. The approaches to modify training loss functions are the most promising,  but model optimization for both fairness and interpretability constraints would require new algorithms and is beyond  the scope of this work.  probability estimation.  In problems where fairness is a signiﬁcant concern, machine learning outputs are likely to be used as decision tools  rather than decision-makers, so it is surprising that so little work has thoroughly examined fairness for regression or 
2 Multi-Category and Multi-Task Learning  We extend the notion of Rademacher complexity to the vector-valued setting.  Deﬁnition 2.1 Let T, N ∈ N, let X be any set, F a class of functions f : X → RT , x = (x1, . . . , xN ) ∈ N , and let I : {1, . . . , T } → 2{1,...,N } be a function which assigns to every t ∈ {1, . . . , T } a subset X It ⊂ {1, . . . , N }. We deﬁne RI (F, x) = 1  N E sup  ǫtift (xi) ,  T  f ∈F  independent Rademacher variables (uniformly distributed on  Xt=1 Xi∈It where the ǫti are doubly indexed, {−1, 1}).  In this section we show that the estimation problem for both multi-category and multi-task learning can be reduced to the problem of bounding RI (F, x) for appropriate choices of the function I.  2.1 Multi-Category Learning  Let C ∈ N be the number of categories. There is an unknown distribution µ on H × {1, . . . , C}, a classiﬁcation rule cl : RT → {1, . . . , C}, and for each label y ∈ {1, . . . , C} a surrogate loss function ℓy : RT → R+. The loss function ℓy is designed so as to upper bound or approximate the indicator function of the set . Here we consider the simple case, where T = C. For the construction of appropriate loss functions see [8, 15, 23]. These loss functions are Lipschitz on RT relative to the Euclidean norm, with some Lipschitz constant Lmc, often interpretable as an inverse margin.  z ∈ RT : cl (z) 6= y  (cid:8)  (cid:9)  3   Given a class F of functions f : H → RT we want to ﬁnd f ∈ F so as to approximately minimize  the surrogate risk  E(x,y)∼µℓy (f (x)) .  Since we do not know the distribution µ, this is done on the basis of a sample of N = nT observations (x, y) = ((x1, y1) , . . . , (xN , yN )) ∈ (H × {1, . . . , C})N , drawn i.i.d. from the distribution µ. We then solve the problem  To give a performance guarantee for ˆf we would like to know how far the empirical minimum above is from the true surrogate risk of ˆf . This difference is upper bounded by  ˆf = arg minf ∈F  ℓyi (f (xi)) .  1 N  N  Xi=1  sup f ∈F "  E(x,y)∼µℓy (f (x)) −  ℓyi (f (xi))  .  #  1 N  N  Xi=1  2 N E sup  f ∈F  N  Xi=1  ǫiℓyi (f (xi)) ,  It is by now well known (see e.g. [4]) that the above expression has, with high probability in the sample, a bound, whose dominant term is given by  where the ǫi are independent Rademacher (uniform {−1, 1}-distributed) variables. We now apply the following result [21, Corollary 6].  n, let F be a class of functions f : X → RT and let  Theorem 2.2 Let X be any set, (x1, . . . , xn) ∈ X hi : RT  → R have Lipschitz norm bounded by L. Then E sup ǫtift (xi) , f ∈F  ǫihi (f (xi)) ≤ √2LE sup  n  Xi=1  f ∈F Xt,i  where ǫti is an independent doubly indexed Rademacher sequence and ft is the t-th component of f .  Using this theorem and the Lipschitz property of the loss functions ℓyi, we upper bound (1) by  2√2 N Lmc E sup  f ∈F  T  N  Xt=1  Xi=1  ǫtift (xi) .  A similar argument can be based on Slepian’s inequality with a passage to Gaussian complexities [15]. In this case the ǫti have to be replaced by independent standard normal variables γti, and √2 π/2. The approach chosen here is simpler and allows us to improve some results of replaced by [15] in the linear case. For our ﬁnal result (Theorem 3.3 below) however we also need Gaussian complexities.  p  We deﬁne I mc : {1, . . . , T } → 2{1,...,N } by I mct = {1, . . . , N } for all t . With Deﬁnition 2.1 the  quantity (2) then becomes 2√2LmcRI mc (F, x) .  4  (1)  (2)  (3)   2.2 Multi-Task Learning  In this setting there is an output space Y, and for each task t ∈ {1, . . . , T } a distribution µt on H ×Y and a loss function ℓt : R × Y → [0, 1], which is assumed to be Lipschitz with constant at most Lmt in the ﬁrst argument for every value of the second. Given a class F of functions f : H → RT we want to ﬁnd f ∈ F so as to approximately minimize the task-average risk  1 T  T  Xt=1  E(x,y)∼µtℓt (ft (x) , y) ,  where ft is the t-th component of the function f . For each task t there is a sample (xt, yt) = ((xt1, yt1) , . . . , (xtn, ytn)) drawn i.i.d. from µt. One solves the problem  ˆf = arg minf ∈F  1 nT  T  n  Xt=1  Xi=1  ℓt (ft (xti) , yi) .  As before we are interested in the supremum of the estimation difference  1 T  sup f ∈F  T  Xt=1 "  E(x,y)∼µt ℓt (ft (x) , y) −  ℓt (ft (xti) , yi)  .  #  1 n  n  Xi=1  As shown in [2] or [16] there is again a high probability bound, whose dominant term is given by the vector-valued Rademacher complexity  2 nT E sup  f ∈F  T  n  Xt=1  Xi=1  ǫtiℓt (ft (xti) , yi) ≤  2 nT Lmt E sup  f ∈F  T  n  Xt=1  Xi=1  ǫtift (xti) ,  where we eliminated the Lipschitz functions with a standard contraction inequality as in [22]. We now collect all the tasks input samples xt in a big sample x = (x1, . . . , xN ) ∈ H N with N = nT , and deﬁne I mt : {1, . . . , T } → 2{1,...,N } so that I mtt is the set of all indices of the examples for task t. Thus xt = (xi)i∈It and n = 2LmtRI mt (F, x) .  . The right hand side above again becomes  I mtt  (4)  (cid:12)(cid:12)  (cid:12)(cid:12)  2.3 A Common Expression to Bound  Comparing (3) and (4) we can summarize: Let F be a class of functions with values in RT . The empirical Rademacher complexity of F as used in multi-category learning and the empirical Rademacher complexity of F as used in multi-task learning are up to (Lipschitz-) constants, bounded by RI (F, x), where the function I is either I mc in the multi-category case or I mt in the multi-task case and I mct = {1, . . . , N } while I mtt ⊆ {1, . . . , N } is the set of indices of examples for task t.  With appropriate deﬁnitions of the function I, bounds on RI (F, x) also lead to learning bounds in hybrid situations where there are several multi-category tasks, potentially with classes occurring  5   in more than one task. In the case of 1-vs-1 voting schemes T = C (C − 1) /2, so there is a component for every unordered pair of distinct classes (c1, c2). Then we deﬁne a I(c1,c2) to be the set of indices of all examples for the classes c1 and c2.  In general It should be the set indices of those examples, which occur as arguments of ft in the expression of the empirical error. For reasons of space however we will stay with the cases of multi-task and 1-vs-all multi-category learning as explained above. We refer to the appendix for the most general statements of our results.  To lighten notation we write RI mc = Rmc and RI mt = Rmt. We also use the notation Rα, where the variable α can be either “mc” or “mt”. It will also be useful to observe that for (a1, . . . , aN ) ∈ RN  T  Xt=1 Xi∈I αt  ai = θ2α  ai,  N  Xi=1  where θmc = √T and θmt = 1.  3 Speciﬁc Bounds  We show how the quantity Rα (F, x) may be bounded, ﬁrst by a simple and general method of reduction to the Rademacher complexities of scalar function classes, then for certain linear classes, and ﬁnally we state and prove our main results for composite classes.  3.1 Component Classes and Independent Learning  Given a class F of functions with values in RT we can deﬁne for each t ∈ {1, . . . , T } the scalar valued component class Ft = {ft : f ∈ F}. By bringing the supremum inside the ﬁrst sum in ( 4) we obtain the bound  RI (F, x) ≤  T  1 N  E sup f ∈Ft Xi∈It  Xt=1  ǫif (xi) ,  which is just a sum of standard, scalar case, empirical Rademacher averages.  In the case of independent learning the components of the members of F are chosen indepen dently, so that  F =  Yt Ft = {(f1, . . . , fT ) : ∀t, ft ∈ Ft} ,  and the above bound becomes an identity and unimprovable. In most cases E supf ∈Ft |It| so the above implies a bound of the order θα/√N . is of the order  P  i∈It ǫif (xi)  p  6   3.2 Linear Classes  Before proceeding we require some more notation. Given a sequence of input vectors, (x1, . . . , xN ) ∈ H N we deﬁne the empirical covariance operator ˆC by  N  h ˆC v, wi = 1  N  Xi=1hv, xiihxi, wi for every v, w ∈ H .  Furthermore, given a function I : {1, . . . , T } → 2{1,...,N }, we deﬁne the empirical covariance operator ˆCt by  h ˆCtv, wi = 1  |It| Xi∈Ithv, xiihxi, wi.  We consider linear transformations W : H → RT of the form x 7→ (hw1, xi , . . . , hwT , xi)  with weight-vectors wt ∈ H. Corresponding function classes will be deﬁned by constraints on the norms of such transformations. We use the mixed (2, p)-norms which are deﬁned as  kW k2,p =  (kw1k, . . . , kwT k) (cid:13)(cid:13) √W ∗W  p  (cid:13)(cid:13)  and the trace norm k·ktr = tr or, for ﬁnite-dimensional H, as the Frobenius norm kW k2,2 = (cid:0) the classes,  . The norm k·k2,2 is also known as the Hilbert-Schmidt norm 2. For B > 0 we consider t kwtk (cid:1)  pP  W2,p =  W : kW k2 ≤ BT 1/p n  o  and  Wtr =  W : kW ktr ≤ B√T  .  n  o The class Wtr can be deﬁned alternatively as Wtr = {V W : W ∈ W, V ∈ V}, where W ={W : H → → RT , kV k2,2 ≤ B√T }, see for example [26] and references RT , kW k2,2 ≤ 1} and V ={V : RT therein. This exhibits Wtr as a composite vector-valued function class.  The factor T 1/p in the deﬁnition of W2,p is essential when discussing the dependence on T . If it were absent then by Jensen’s inequality the average norm allowed to the weight vectors would be bounded by B/T 1/p, so the class is regularized to death as T increases. This applies in particular to the case of multi-category learning, where each component needs to be able to win over all the others by some margin. The same argument applies to the √T in the constraint of the trace-norm class. In this sense it is not quite correct to speak of rates in T if the constraint on the norm is held constant as in [15].  7   For simplicity we assume that kxik = 1 for all i (as with a Gaussian RBF-kernel) for the rest of this subsection. Note that this implies tr( ˆC) = tr( ˆCt) = 1. We also consider only the cases of multi-category and multi-task learning. Statements and proofs for general index sets It and general values of the kxik are given in the appendix. We ﬁrst give some lower and upper bounds for W2,∞ and W2,p. Theorem 3.1 For p ∈ [2, ∞] 1 2n ≤ Rα (W2,∞, x) ≤ Rα (W2,p, x) ≤ B θα  B θα  1 n  r  r  and for p ∈ [1, 2] and 1/p + 1/q = 1  Rα (W2,2, x) ≤ Rα (W2,p, x) ≤ 21/qB θα  q n .  r  The lower bound in the [2, ∞]-regime is simply 1/√2 times the upper bound. If we set Λ = T 1/pB, then the multi-category bound for the [1, 2]-regime can be compared to the one given in [15], which is larger by a factor of O√q. This improvement is however exclusively due to our trick of staying with Rademacher variables when eliminating the loss functions.  The norms in the lemma above are not very useful for multi-task learning, as the bounds show no improvement as the number of tasks increases. This is different for the trace-norm constrained class Wtr, for which we have the following result, which already exhibits a typical behaviour of composite classes. The proof of a more general version is given in the appendix.  Theorem 3.2  Rα (Wtr, x) ≤ B θα    r  2 (ln (nT ) + 1) nT  + s  λmax( ˆC)  .  n     If we divide this bound by the above lower bound for regularization with the Hilbert Schmidt  norm, we obtain  Rα (Wtr, x) Rα (W2,2, x) ≤ 2  r  ln (nT ) + 1 T  2λmax( ˆC) tr( ˆC)  ,  + s  a quotient, which highlights the potential beneﬁts of composite classes. As T increases the second term becomes dominant. The quotient λmax( ˆC)/tr( ˆC) can be seen as the inverse of an effective Indeed for whitened data tr( ˆC) = d λmax( ˆC), if d is the number of nonzero data-dimension. eigenvalues of ˆC. The relative estimation beneﬁt of the intermediate representation increases with the number T of classes or tasks and with the effective dimensionality of the data. This appears to be a rather general feature of composite vector-valued classes, also in the nonlinear case.  8   3.3 Composite Classes and Representation Learning  We now consider function classes V ◦ φ ◦ W of the form  W x ∈ H −→ RK  φ −→ H ′  V → RT .  Here inputs x ∈ H are ﬁrst mapped to RK by a linear function W from a class W. The vector W x → H ′. Finally  is then mapped to another Hilbert-space H ′ by a ﬁxed Lipschitz feature map φ : RK φ (W x) is mapped to the T -dimensional vector V φ (W x) by the linear map V chosen from V.  For W ∈ W we consider the constraints kW k2,∞ ≤ b∞, kW k2,2 ≤ b2 and kW k2,1 ≤ b1, denoting the respective classes by W2,∞, W2,2, and W2,1. For V we take the constraint kV k2,∞ ≤ a. This choice allows us to vary T and keep a ﬁxed at the same time. For the “activation function” φ we assume a Lipschitz constant Lφ. We make the simplifying assumption that φ (0) = 0.  The function φ makes the model quite general. Suppose ﬁrst that H ′ = RK. If φ is the identity function we obtain a linear class, deﬁned through its factorization, much like the case of trace-norm regularization discussed earlier. If the components of φ are sigmoids or the popular rectilinear activation functions, we obtain a rather standard neural network with hidden layer, but φ could also include inter-unit interactions, such as poolings or lateral inhibitions (see, e.g. [10, 13]) as long as it observes the Lipschitz condition.  However, the dimension of H ′ need not be K and φ could be deﬁned by a radial basis function network with ﬁxed centers or it could also be the feature-map induced by some kernel on RT , say a Gaussian kernel of width ∆, in which case Lφ = 2/∆. To enforce φ (0) = 0 we need to translate the original feature map ψ of the Gaussian kernel as φ (x) = ψ (x) − ψ (0).  Here the underlying assumption is, that there is a common K-dimensional representation of the data in which the data has sufﬁcient separation properties, but the separating functions may be highly nonlinear.  Theorem 3.3 There are universal constants c1 and c2 such that under the above conditions  Rα (Vφ (W2,∞) , x) ≤ Lφab∞θα    Rα (Vφ (W2,2) , x) ≤ Lφab2θα  c1s  Rα (Vφ (W2,1) , x) ≤ Lφab1θα  c1s   c1Ks  tr( ˆC) nT + c2s  Kλmax( ˆC) n  K tr( ˆC) nT  + c2vuut  λmax  ˆC  (cid:16) n    (cid:17)  2tr( ˆC) + 8λmax( ˆC) ln K nT       + c2s  9  λmax( ˆC)  .  n      We highlight some implications of the above theorem.  1. The bounds differ in their dependence on the dimension K of the hidden layer which is linear, radical and logarithmic respectively. For W2,1 the dependence on K is logarithmic and scales only with λmax( ˆC).  2. In the case of multi-task learning with W2,2 and W2,1 the dependence on K vanishes in the limit T → ∞. In this limit the ﬁrst term in parenthesis vanishes in all three cases, leaving only the second term.  3. Multi-category learning requires more data with θ = √T , but if we take a simultaneous limit in T and n such that T /n remains bounded, then the behaviour is the same as for multi-task learning with T → ∞.  4. In both cases the second term becomes dominant for large T . For the ﬁrst bound crudely setting λmax( ˆC) = 1/d this term scales with K/d and exhibits the beneﬁt of the shared representation as that of dimensional reduction. A similar interpretation holds for the other bounds with some implicit dependence of b2 and b1 on the dimension of the representation.  p  The proof uses the following recent result on the expected suprema of Gaussian processes [18].  For a set Y ⊆ Rm the Gaussian width G (Y ) is deﬁned as  G (Y ) = E sup  y∈Y hγ, yi = E sup  y∈Y  γiyi,  m  Xi=1  where γ = (γ1, . . . , γm) is a vector of independent standard normal variables. Theorem 3.4 Let Y ⊆ Rn have (Euclidean) diameter D (Y ) and let F be a class of functions f : Y → Rm, all of which have Lipschitz constant at most L (F). Let F (Y ) = {f (y) : f ∈ F, y ∈ Y }. Then for any y0 ∈ Y  (5)  G (F (Y )) ≤ c1L (F) G (Y ) + c2D (Y ) Q (F) + G (F (y0)) ,  where c1 and c2 are universal constants and  Q (F) =  sup y,y′∈Y, y6=y′  E sup f ∈F  hγ, f (y) − f (y′)i ky − y′  k  .  We refer to the appendix for statement and proof of a more general version going beyond  1-vs-all multi-category and multi-task learning.  Idea of proof for Theorem 3.3. We use Theorem 3.4 by setting  Y =  W x = (hwk, xii)k≤K, i≤N : W ∈ Wo ⊆ RKN  n  10   (yki) ∈ RKN  where W will be either W2,∞, W 2,2 or W2,1. Note that the cardinality |It| is either N or n in the cases considered here. For F we take the set of functions 7→ (hvt, φ (yi)i)t≤T,i∈It ∈ RT |It| : v ∈ Vo  restricted to Y , so F (Y ) is a subset of RT 2n for multi-category and RT n for multi-task learning. This again accounts for the additional factor of √T for the complexity of multi-category learning. By a well known bound on Rademacher averages in terms of Gaussian averages [14]  n  E  sup  W ∈V,W ∈W Xt Xi∈It  ǫtiV φ (W xi) ≤ r  π 2 E  sup  γtiV φ (W xi)  W ∈V,W ∈W Xt Xi∈It  π 2 G (F (Y )) .  =  r  (6)  To bound G (F (Y )) we then just need to bound the individual components of the right hand side of equation (7), namely the largest Lipschitz constant L (F), the differential Gaussian width Q (F), the diameter D (Y ) and the Gaussian width G (Y ). We needn’t worry about G (F (y0)), because we are free to choose y0, so we can set it to 0. Then f (0) = 0 for all f ∈ F, whence G (F (y0)) = 0. For the bounds on L (F), Q (F), D (Y ) and G (Y ) we refer to the appendix. 