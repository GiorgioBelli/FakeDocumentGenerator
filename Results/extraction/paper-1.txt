PAPER PATH: C:\Users\GIORGIO-DESKTOP\Documents\Universita\Tesi\datasets\downloaded_papers\0201009v1.The_performance_of_the_batch_learner_algorithm.pdf

====================ABSTRACT====================
Abstract. We analyze completely the convergence speed of the
batch learning algorithm, and compare its speed to that of the
memoryless learning algorithm and of learning with memory (as
analyzed in [KR2001b]). We show that the batch learning algorithm is never worse than the memoryless learning algorithm (at
least asymptotically). Its performance vis-a-vis learning with full
memory is less clearcut, and depends on certain probabilistic assumptions.

========================================
====================INTRODUCTION====================
Introduction

The original motivation for the work in this paper was provided
by research in learning theory, speciﬁcally in various models of language acquisition (see, for example, [KNN2001, NKN2001, KN2001]).
In the paper [KR2001b], we had studied the speed of convergence of the
memoryless learner algorithm, and also of learning with full memory.
Since the batch learning algorithm is both widely known, and believed
to have superior speed (at the cost of memory) to both of the above
methods by learning theorists, it seemed natural to analyze its behavior under the same set of assumptions, in order to bring the analysis
in [KR2001a] and [KR2001b] to a sort of closure. It should be noted
that the detailed analysis of the batch learning algorithm is performed
under the assumption of independence, which was not explicitly present
in our previous work. For the impatient reader we state our main result
(Theorem 6.1) immediately (the reader can compare it with the results
on the memoryless learning algorithm and learning with full memory,
as summarized in Theorem 2.1):

Theorem A. Let N∆ be the number of steps it takes for the student
to have probability 1 − ∆ of learning the concept using the batch learner
algorithm. Then we have the following estimates for N∆:

1991 Mathematics Subject Classiﬁcation. 60E07, 60F15, 60J20, 91E40, 26C10.
Key words and phrases.
The author would like to think the EPSRC and the NSF for support, and Natalia

learning theory, zeta functions, asymptotics.

Komarova and Ilan Vardi for useful conversations.

1


2

IGOR RIVIN

• if the distribution of overlaps is uniform, or more generally, the
density function f (1 − x) at 0 has the form f (x) = c + O(xδ),
δ, c > 0, then there exist positive constants C1, C2 such that

limn→∞ P

C1 <

(1 − ∆)2n < C2

= 1

N∆

(cid:19)

(cid:18)

• if the probability density function f (1 − x) is asymptotic to cxβ +

O(xβ+δ),

δ, β > 0, as x approaches 0, then

limn→∞ P

 c1 <

N∆
| log ∆|n 1

1+β

< c2! = 1,

for some positive constants c1, c2;

• if the asymptotic behavior is as above, but −1 < β < 0, then

limx→∞ P

1
x <

(cid:18)

N∆

| log ∆|n1/(1+β) < x
(cid:19)

= 1

The plan of the paper is as follows: in this Introduction we recall the
learning algorithms we study; in Section 1 we deﬁne our mathematical
model; in Section 2 we recall our previous results, in Section 3 we begin
the analysis of the batch learning algorithm, and introduce some of the
necessary mathematical concepts; in Sections 4-6 we analyze the three
cases stated in Theorem A, and we summarize our ﬁndings in Section
7.

Memoryless Learning and Learning with Full Memory. The
general setup is as follows: There is a collection of concepts R0, . . . , Rn
and words which refer to these concepts, sometimes ambiguously. The
teacher generates a stream of words, referring to the concept R0. This is
not known to the student, but he must learn by, at each step, guessing
some concept Ri and checking for consistency with the teacher’s input.
The memoryless learner algorithm consists of picking a concept Ri at
random, and sticking by this choice, until it is proven wrong. At this
point another concept is picked randomly, and the procedure repeats.
Learning with full memory follows the same general process with the
important diﬀerence that once a concept is rejected, the student never
goes back to it. It is clear (for both algorithms) that once the student
hits on the right answer R0, this will be his ﬁnal answer. We would
like to estimate the probability of having guessed the right answer is
after k steps, and also the expected number of steps before the student
settles on the right answer.


THE PERFORMANCE OF THE BATCH LEARNING ALGORITHM

3

Batch Learning. The batch learning situation is similar to the above,
but here the student records the words w1, . . . , wk, . . . he gets from the
teacher. For each word wi , we assume that the student can ﬁnd (in his
textbook, for example) a list Li of concepts referred to by the word. If
we deﬁne

k

Lk =

Li,

\i=1
then we are interested in the smallest value of k such that Lk = {R0}.
This value k0 is the time it has taken the student to learn the concept
R0. We think of k0 as a random variable, and we wish to estimate its
expectation.

1. The mathematical model

We think of the words referring to the concept R0 as a probability
space P. The probability that one of these words also refer to the
concept Ri shall be denoted by pi; the probability that a word refers
to concepts Ri1, . . . , Rik shall be denoted by pi1...ik. All the results
described below (obviously) depend in a crucial way on the p1, . . . , pn
and (in the case of the batch learning algorithm) also on the joint
probabilities. Since there is no a priori reason to assume speciﬁc values
for the probabilities, we shall assume that all of the pi are themselves
independent, identically distributed random variables. We shall refer
to their common distribution as F , and to the density as f . It turns
out that the convergence properties of the various learning algorithms
depend on the local analytic properties of the distribution F at 1 –
some moments reﬂection will convince the reader that this is not really
so surprising.

Sharper analysis of the batch learning algorithm, depends on the

independence hypothesis:

pi1...ik = pi1 . . . pik .

It is again not too surprising that some such assumption on correlations ought to be required for precise asymptotic results, though it is
obviously the subject of a (non-mathematical) debate as to whether
assuming that the various concepts are truly independent is reasonable
from a cognitive science point of view.

2. Previous results

In previous work [KR2001a] and [KR2001b] we obtained the follow
ing result.


4

IGOR RIVIN

Theorem 2.1. Let N∆ be the number of steps it takes for the student
to have probability 1 − ∆ of learning the concept. Then we have the
following estimates for N∆:

• if the distribution of overlaps is uniform, or more generally, the
density function f (1 − x) at 0 has the form f (x) = c + O(xδ),
δ, c > 0, then there exist positive constants C1, C2, C ′1, C ′2 such
that

limn→∞ P

C1 <

| log ∆|n log n < C2

= 1

(cid:18)
for the memoryless algorithm and

limn→∞ P

C ′1 <

(1 − ∆)2n log n < C ′2

= 1

(cid:18)
when learning with full memory;

(cid:19)

(cid:19)

• if the probability density function f (1 − x) is asymptotic to cxβ +
δ, β > 0, as x approaches 0, then for the two algo
O(xβ+δ),
rithms we have respectively

limn→∞ P

c1 < N∆
(cid:18)

| log ∆|n < c2

(cid:19)

= 1,

and

limn→∞ P

c′1 <

(1 − ∆)2n < c′2

= 1

(cid:18)
for some positive constants c1, c2, c′1, c′2;

(cid:19)

• if the asymptotic behavior is as above, but −1 < β < 0, then

limx→∞ P

1
x <

(cid:18)

| log ∆|n1/(1+β) < x
(cid:19)

= 1

for the memoryless learning algorithm, and similarly

limx→∞ P

1
x <

(cid:18)

(1 − ∆)2n1/(1+β) < x
(cid:19)

= 1

for learning with full memory.

N∆

N∆

N∆

N∆

N∆

Recall that f (x) = Θ(g(x)) means that for suﬃciently large x, the ratio f (x)/g(x) is bounded between two strictly positive constants. The
distribution of overlaps referred to above is simply the distribution F .
Notice that the theorem says nothing about the situation when F is
supported in some interval [0, a], for a < 1. That case is (presumably)
of scientiﬁc interest, but mathematically it is relatively trivial: we replace the arguments of all the Θs above by 1, though, of course, we are
thereby hiding the dependence on a.


THE PERFORMANCE OF THE BATCH LEARNING ALGORITHM

5

3. General bounds on the batch learner algorithm

Consider a set of words w1, . . . , wk. The probability that they all

refer to the concept Ri is, obviously pki .
Lemma 3.1. The probability qk that we still have not learned the conn
i=1 pki , and below by maxi pki .
cept R0 after k steps is bounded above by

Proof. Immediate.

P

We will ﬁrst use these upper and lower bounds to get corresponding
bounds on the convergence speed of the batch learner algorithm, and
then invoke the independence hypothesis to sharpen these bounds in
many cases.

We begin with a trivial but useful lemma.

Lemma 3.2. Let G be a game where the probability of success (respectively failure) after at most k steps is sk (respectively fk = 1 − sk).
Then the expected number of steps until success is
∞

∞

∞

k(sk − sk−1) =

sk = 1 −

fk,

Xk=1
if the corresponding sum converges.

Xk=1

Xk=1

Proof. The proof is immediate from the deﬁnition of expectation and
the possibility of rearrangment of terms of positive series.

We can combine Lemma 3.2 and Lemma 3.1 to obtain:

Theorem 3.3. The expected time T of convergence of the batch learner
algorithm is bounded as follows:

(1)

1
1 − pi

n

Xi=1

≥ T ≥ max
1≤i≤n

1
1 − pi

.

The leftmost term in equation (1) has been studied at length in

[KR2001a]. We state a version of the results of [KR2001a] below:

n
i=1

P

1
Theorem 3.4. Let S =
1−pi , where the pi are independently
identically distributed random variables with values in [0, 1], with probability density f , such that f (1−x) = xβ +O(xβ+δ),
δ > 0 for x → 0.
Then If β > 0, then there exists a mean m, such that limn→∞ P(|S/n −
m| > ǫ) = 0, for any ǫ > 0. If β = 0, then limn→∞ P(|S/(n log n) − 1| >
ǫ) = 0). Finally, if −1 ≤ β < 0, then limn→∞ P(S/n1/β+1 − C > a) =
g(a), where lima→∞ g(a) = 0, and C is an arbitrary (but ﬁxed) constant,
and likewise

P(S/n1/(β+1) < b) = h(b),

where lima→0 h(a) = 0,


6

IGOR RIVIN

The right hand side of Eq. (1) is easier to understand. Indeed, let
p1, . . . , pn be distributed as usual (and as in the statement of Theorem
3.4). Then

Theorem 3.5.

limn→∞ n 1
for some positive constant C.

1+β E

(cid:18)

1 − max

1≤i≤n pi

= C,

(cid:19)

Proof. First, we change variables to qi = 1 − pi. Obviously, the statement of the Theorem is equivalent to the statement that

E = E( min1≤i≤n qi) = Cn−1/1+β.
We also write h(x) = f (1 − x), and let H be the distribution function
whose density is h, so that H(x) = 1 − F (1 − x). Now, the probability
of that all of the qi are greater than t equals 1 − (1 − H(t))n, so that

1

1

E =

t d [1 − (1 − H(t))n] =

(1 − H(t))ndt.

0
Z
We change variables t = u/n1/(1+β), to obtain

Z

0

n 11+β

E = 1
n1+β

0
Z

========================================
====================CORPUS====================

========================================
====================CONCLUSION====================

========================================
====================COMPLETE TEXT====================
2
0
0
2
 
an

J
 

4
1
 
 

G]
L
.
s
c
[
 
 
1
v
9
0
0
1
0
2
0
/
s
c
:
v
i
X
r
a

THE PERFORMANCE OF THE BATCH LEARNING
ALGORITHM

IGOR RIVIN

Abstract. We analyze completely the convergence speed of the
batch learning algorithm, and compare its speed to that of the
memoryless learning algorithm and of learning with memory (as
analyzed in [KR2001b]). We show that the batch learning algorithm is never worse than the memoryless learning algorithm (at
least asymptotically). Its performance vis-a-vis learning with full
memory is less clearcut, and depends on certain probabilistic assumptions.

Introduction

The original motivation for the work in this paper was provided
by research in learning theory, speciﬁcally in various models of language acquisition (see, for example, [KNN2001, NKN2001, KN2001]).
In the paper [KR2001b], we had studied the speed of convergence of the
memoryless learner algorithm, and also of learning with full memory.
Since the batch learning algorithm is both widely known, and believed
to have superior speed (at the cost of memory) to both of the above
methods by learning theorists, it seemed natural to analyze its behavior under the same set of assumptions, in order to bring the analysis
in [KR2001a] and [KR2001b] to a sort of closure. It should be noted
that the detailed analysis of the batch learning algorithm is performed
under the assumption of independence, which was not explicitly present
in our previous work. For the impatient reader we state our main result
(Theorem 6.1) immediately (the reader can compare it with the results
on the memoryless learning algorithm and learning with full memory,
as summarized in Theorem 2.1):

Theorem A. Let N∆ be the number of steps it takes for the student
to have probability 1 − ∆ of learning the concept using the batch learner
algorithm. Then we have the following estimates for N∆:

1991 Mathematics Subject Classiﬁcation. 60E07, 60F15, 60J20, 91E40, 26C10.
Key words and phrases.
The author would like to think the EPSRC and the NSF for support, and Natalia

learning theory, zeta functions, asymptotics.

Komarova and Ilan Vardi for useful conversations.

1


2

IGOR RIVIN

• if the distribution of overlaps is uniform, or more generally, the
density function f (1 − x) at 0 has the form f (x) = c + O(xδ),
δ, c > 0, then there exist positive constants C1, C2 such that

limn→∞ P

C1 <

(1 − ∆)2n < C2

= 1

N∆

(cid:19)

(cid:18)

• if the probability density function f (1 − x) is asymptotic to cxβ +

O(xβ+δ),

δ, β > 0, as x approaches 0, then

limn→∞ P

 c1 <

N∆
| log ∆|n 1

1+β

< c2! = 1,

for some positive constants c1, c2;

• if the asymptotic behavior is as above, but −1 < β < 0, then

limx→∞ P

1
x <

(cid:18)

N∆

| log ∆|n1/(1+β) < x
(cid:19)

= 1

The plan of the paper is as follows: in this Introduction we recall the
learning algorithms we study; in Section 1 we deﬁne our mathematical
model; in Section 2 we recall our previous results, in Section 3 we begin
the analysis of the batch learning algorithm, and introduce some of the
necessary mathematical concepts; in Sections 4-6 we analyze the three
cases stated in Theorem A, and we summarize our ﬁndings in Section
7.

Memoryless Learning and Learning with Full Memory. The
general setup is as follows: There is a collection of concepts R0, . . . , Rn
and words which refer to these concepts, sometimes ambiguously. The
teacher generates a stream of words, referring to the concept R0. This is
not known to the student, but he must learn by, at each step, guessing
some concept Ri and checking for consistency with the teacher’s input.
The memoryless learner algorithm consists of picking a concept Ri at
random, and sticking by this choice, until it is proven wrong. At this
point another concept is picked randomly, and the procedure repeats.
Learning with full memory follows the same general process with the
important diﬀerence that once a concept is rejected, the student never
goes back to it. It is clear (for both algorithms) that once the student
hits on the right answer R0, this will be his ﬁnal answer. We would
like to estimate the probability of having guessed the right answer is
after k steps, and also the expected number of steps before the student
settles on the right answer.


THE PERFORMANCE OF THE BATCH LEARNING ALGORITHM

3

Batch Learning. The batch learning situation is similar to the above,
but here the student records the words w1, . . . , wk, . . . he gets from the
teacher. For each word wi , we assume that the student can ﬁnd (in his
textbook, for example) a list Li of concepts referred to by the word. If
we deﬁne

k

Lk =

Li,

\i=1
then we are interested in the smallest value of k such that Lk = {R0}.
This value k0 is the time it has taken the student to learn the concept
R0. We think of k0 as a random variable, and we wish to estimate its
expectation.

1. The mathematical model

We think of the words referring to the concept R0 as a probability
space P. The probability that one of these words also refer to the
concept Ri shall be denoted by pi; the probability that a word refers
to concepts Ri1, . . . , Rik shall be denoted by pi1...ik. All the results
described below (obviously) depend in a crucial way on the p1, . . . , pn
and (in the case of the batch learning algorithm) also on the joint
probabilities. Since there is no a priori reason to assume speciﬁc values
for the probabilities, we shall assume that all of the pi are themselves
independent, identically distributed random variables. We shall refer
to their common distribution as F , and to the density as f . It turns
out that the convergence properties of the various learning algorithms
depend on the local analytic properties of the distribution F at 1 –
some moments reﬂection will convince the reader that this is not really
so surprising.

Sharper analysis of the batch learning algorithm, depends on the

independence hypothesis:

pi1...ik = pi1 . . . pik .

It is again not too surprising that some such assumption on correlations ought to be required for precise asymptotic results, though it is
obviously the subject of a (non-mathematical) debate as to whether
assuming that the various concepts are truly independent is reasonable
from a cognitive science point of view.

2. Previous results

In previous work [KR2001a] and [KR2001b] we obtained the follow
ing result.


4

IGOR RIVIN

Theorem 2.1. Let N∆ be the number of steps it takes for the student
to have probability 1 − ∆ of learning the concept. Then we have the
following estimates for N∆:

• if the distribution of overlaps is uniform, or more generally, the
density function f (1 − x) at 0 has the form f (x) = c + O(xδ),
δ, c > 0, then there exist positive constants C1, C2, C ′1, C ′2 such
that

limn→∞ P

C1 <

| log ∆|n log n < C2

= 1

(cid:18)
for the memoryless algorithm and

limn→∞ P

C ′1 <

(1 − ∆)2n log n < C ′2

= 1

(cid:18)
when learning with full memory;

(cid:19)

(cid:19)

• if the probability density function f (1 − x) is asymptotic to cxβ +
δ, β > 0, as x approaches 0, then for the two algo
O(xβ+δ),
rithms we have respectively

limn→∞ P

c1 < N∆
(cid:18)

| log ∆|n < c2

(cid:19)

= 1,

and

limn→∞ P

c′1 <

(1 − ∆)2n < c′2

= 1

(cid:18)
for some positive constants c1, c2, c′1, c′2;

(cid:19)

• if the asymptotic behavior is as above, but −1 < β < 0, then

limx→∞ P

1
x <

(cid:18)

| log ∆|n1/(1+β) < x
(cid:19)

= 1

for the memoryless learning algorithm, and similarly

limx→∞ P

1
x <

(cid:18)

(1 − ∆)2n1/(1+β) < x
(cid:19)

= 1

for learning with full memory.

N∆

N∆

N∆

N∆

N∆

Recall that f (x) = Θ(g(x)) means that for suﬃciently large x, the ratio f (x)/g(x) is bounded between two strictly positive constants. The
distribution of overlaps referred to above is simply the distribution F .
Notice that the theorem says nothing about the situation when F is
supported in some interval [0, a], for a < 1. That case is (presumably)
of scientiﬁc interest, but mathematically it is relatively trivial: we replace the arguments of all the Θs above by 1, though, of course, we are
thereby hiding the dependence on a.


THE PERFORMANCE OF THE BATCH LEARNING ALGORITHM

5

3. General bounds on the batch learner algorithm

Consider a set of words w1, . . . , wk. The probability that they all

refer to the concept Ri is, obviously pki .
Lemma 3.1. The probability qk that we still have not learned the conn
i=1 pki , and below by maxi pki .
cept R0 after k steps is bounded above by

Proof. Immediate.

P

We will ﬁrst use these upper and lower bounds to get corresponding
bounds on the convergence speed of the batch learner algorithm, and
then invoke the independence hypothesis to sharpen these bounds in
many cases.

We begin with a trivial but useful lemma.

Lemma 3.2. Let G be a game where the probability of success (respectively failure) after at most k steps is sk (respectively fk = 1 − sk).
Then the expected number of steps until success is
∞

∞

∞

k(sk − sk−1) =

sk = 1 −

fk,

Xk=1
if the corresponding sum converges.

Xk=1

Xk=1

Proof. The proof is immediate from the deﬁnition of expectation and
the possibility of rearrangment of terms of positive series.

We can combine Lemma 3.2 and Lemma 3.1 to obtain:

Theorem 3.3. The expected time T of convergence of the batch learner
algorithm is bounded as follows:

(1)

1
1 − pi

n

Xi=1

≥ T ≥ max
1≤i≤n

1
1 − pi

.

The leftmost term in equation (1) has been studied at length in

[KR2001a]. We state a version of the results of [KR2001a] below:

n
i=1

P

1
Theorem 3.4. Let S =
1−pi , where the pi are independently
identically distributed random variables with values in [0, 1], with probability density f , such that f (1−x) = xβ +O(xβ+δ),
δ > 0 for x → 0.
Then If β > 0, then there exists a mean m, such that limn→∞ P(|S/n −
m| > ǫ) = 0, for any ǫ > 0. If β = 0, then limn→∞ P(|S/(n log n) − 1| >
ǫ) = 0). Finally, if −1 ≤ β < 0, then limn→∞ P(S/n1/β+1 − C > a) =
g(a), where lima→∞ g(a) = 0, and C is an arbitrary (but ﬁxed) constant,
and likewise

P(S/n1/(β+1) < b) = h(b),

where lima→0 h(a) = 0,


6

IGOR RIVIN

The right hand side of Eq. (1) is easier to understand. Indeed, let
p1, . . . , pn be distributed as usual (and as in the statement of Theorem
3.4). Then

Theorem 3.5.

limn→∞ n 1
for some positive constant C.

1+β E

(cid:18)

1 − max

1≤i≤n pi

= C,

(cid:19)

Proof. First, we change variables to qi = 1 − pi. Obviously, the statement of the Theorem is equivalent to the statement that

E = E( min1≤i≤n qi) = Cn−1/1+β.
We also write h(x) = f (1 − x), and let H be the distribution function
whose density is h, so that H(x) = 1 − F (1 − x). Now, the probability
of that all of the qi are greater than t equals 1 − (1 − H(t))n, so that

1

1

E =

t d [1 − (1 − H(t))n] =

(1 − H(t))ndt.

0
Z
We change variables t = u/n1/(1+β), to obtain

Z

0

n 11+β

E = 1
n1+β

0
Z

1 − H

u
n1/(1+β)

n du.

(cid:16)

(cid:16)

(cid:17)(cid:17)

Let us write E = E1(n) + E2(n), where

(2)

(3)

(4)

(5)

Let

(6)

Recall that

E1(n) =

1 − H

13(β+1)

n

0
Z

n
Z

n 11+β

13(β+1)

h

h

u
n1/(1+β)

u
n1/(1+β)

n du,

(cid:17)i

n du,

(cid:17)i

(cid:16)

(cid:16)

E2(n) =

1 − H

H(x) = cxβ+1 + O(xβ+δ+1).

I =

exp

cx1+β

dx.

∞

0

Z

We now show:

(cid:0)

(cid:1)

(7)

limn→∞ E1(n) = I.
This is an immediate consequence of Lemma 3.7 and Eq. (5). Also,
limn→∞ E2(n) = 0.

(8)


THE PERFORMANCE OF THE BATCH LEARNING ALGORITHM

7

Since H is a monotonically increasing function, it is suﬃcient to show
that

limn→∞ n 1
This is immediate from Eq. (5) and Lemma 3.7.

n 2
(cid:16)

1 − H

(cid:17)i

3(1+β)

1+β

h

n = 0.

Remark 3.6. The argument shows that C = I, where C is the constant in the statement of lemma, and I is the integral introduced in Eq.
(6).

Lemma 3.7. Let fn(x) = (1 − x/n)n, and let 0 ≤ z < 1/2.

fn(x) = exp(−x)

1 − x2

2n + O

x3
n2 (cid:19)(cid:21)

.

(cid:18)

(cid:20)

Proof. Note that

log fn(x) = n log(1 − x/n) = −x −

xk
knk−1 .

∞

Xk=2

The assertion of the lemma follows by exponentiating the two sides of
the above equation.

We need one ﬁnal observation:

Theorem 3.8. The variable n1/(1+β) minni=1 qi has a limiting distribution with distribution function G(x) = 1 − exp(−x1+β).

Proof. Immediate from the proof of Theorem 3.5.

We can now put together all of the above results as follows.

Theorem 3.9. Let p1, . . . , pk be independently distributed with common density function f , such that f (1 − x) = cxβ + O(xβ+δ), δ > 0.
Let T be the expected time of the convergence of the batch learning
algorithm with overlaps p1, . . . , pk. Then, if β > 0, then there exist C1, C2, such that C1n1/(1+β) ≤ T ≤ C2n, with probability tending
to 1 as n tends to ∞.
If β = 0, then there exist C1, C2, such that
C1n ≤ T ≤ C2n log n, with probability tending to one as n tends to ∞.
If β > 0, then C −1n1/(β+1) ≤ T ≤ Cn1/(β+1) with probability tending to
0 as C goes to inﬁnity.

The reader will remark that in the case that β > 0, the upper and

lower bounds have the same order of magnitude as functions of n.


8

IGOR RIVIN

4. Independent concepts

We now invoke the independence hypothesis, whereby an application

of the inclusion-exclusion principle gives us:

Lemma 4.1. The probability lk that we have learned the concept R0
after k steps is given by

lk =

(1 − pki ).

n

Yi=1

T =

ksk,

∞

Xk=1

∞

T =

(1 − lk)

Note that the probability sk of winning the game on the k-th step
is given by sk = lk − lk−1 = (1 − lk−1) − (1 − lk). Since the expected
number of steps T to learn the concept is given by

we immediately have

Xk=1
Lemma 4.2. The expected time T of learning the concept R0 is given
by

(9)

∞

n

T =

1 −

1 − pki

.

Xk=1  

Yi=1 (cid:0)
Since the sum above is absolutely convergent, we can expand the
products and interchange the order of summation to get the following
formula for T :

(cid:1)!

Notation. Below, we identify subsets of {1, . . . , n} with multindexes
(in the obvious way), and if s = {i1, . . . , il}, then

ps def= pi1 · · · pil.

Lemma 4.3. The expression Eq. (9) can be rewritten as:

(10)

T =

(−1)|s|−1

Proof. With notation as above,

Xs⊆{1,...,n}

1
1 − ps

(cid:18)

− 1

,

(cid:19)

m

Yi=1 (cid:0)

1 − pki

=

(−1)|s|pks,

(cid:1)

Xs⊆{1,...,n}


THE PERFORMANCE OF THE BATCH LEARNING ALGORITHM

9

so

∞

n

T =

1 −

1 − pki

Yi=1 (cid:0)

1 −

Xk=1  

∞

Xk=1 



(cid:1)!
(−1)|s|pks 


∞

Xs⊆{1,...,n}
(−1)|s|−1

(−1)|s|−1

pks

Xk=1

1
1 − ps

(cid:18)

Xs⊆{1,...,n}

Xs⊆{1,...,n}

=

=

=

− 1

,

(cid:19)

where the change in the order of summation is permissible since all
sums converge absolutely.

Formula (10) is useful in and of itself, but we now use it to analyse the
statistical properties of the time of success T under our distribution
and independence assumptions. For this we shall need to study the
moment zeta function of a probability distribution, introduced below.
Its detailed properties are investigated in my paper [Rivin2002], where
Theorems 4.9, 4.10 and 4.11 below are proved. Below we summarize
the deﬁnitions and the results.

4.1. Moment zeta function.

Deﬁnition 4.4. Let F be a probability distribution on a (possibly inI xkF (dx) be the k-th moment of
ﬁnite) interval I, and let mk(F ) =
F . Then the moment zeta function of F is deﬁned to be
R
∞

ζF (s) =

msk(F ),

Xk=1

whenever the sum is deﬁned.

The deﬁnition is, in a way, motivated by the following:

Lemma 4.5. Let F be a probability distribution as above, and let x1, . . . , xn
be independent random variables with common distribution F . Then

(11)

E

1
1 − x1 . . . xn (cid:19)

(cid:18)

= ζF(n).

In particular, the expectation is undeﬁned whenever the zeta function
is undeﬁned.


10

IGOR RIVIN

Proof. Expand the fraction in a geometric series and apply Fubini’s
theorem.

Example 4.6. For F the uniform distribution on [0, 1], ζF is the familiar Riemann zeta function.

Using standard techinques of asymptotic analysis, the following can

be shown (see [Rivin2002]):
Theorem 4.7. Let F be a continuous distribution supported in [0, 1],
let f be the density of the distribution F , and suppose that f (1 − x) =
cxβ + O(xβ+δ), for some δ > 0. Then the k-th moment of F is asymptotic to Ck−(1+β), for C = cΓ(β).

Corollary 4.8. Under the assumptions of Theorem 4.7, ζF (s) is deﬁned for s > 1/(1 + β).

The moment zeta function can be used to two of the three situations
occuring in the study of the batch learner algorithm: In the sequel, we
set α = β + 1.

4.2. α > 1. In this case, we use our assumptions to rewrite Eq. (10)
as

(12)

E(T ) = −

(−1)kζF (k).

n

Xk=1 (cid:18)

n
k(cid:19)

This, in turn, can be rewritten (by expanding the deﬁnition of zeta) as

(13)

E(T ) = −

[(1 − mj(F ))n − 1] =

[1 − (1 − mj(F ))n]

∞

∞

Xj=1
Using the moment zeta function we can show:

Xj=1

Theorem 4.9. Let F be a continuous distribution supported on [0, 1],
and let f be the density of F . Suppose further that

limx→1

f (x)
(1 − x)β = c,

for β, c > 0. Then,

= −

n

limn→∞ n− 1

1+β

"
Xk=1 (cid:18)
1 − exp

∞

n
k(cid:19)

Z

0

u2
= − (cΓ(β + 1)) 1

(cid:0)

β+1 Γ

(−1)kζF (k)

#

du

(cid:1)

β
β + 1(cid:19)

.

(cid:18)

−cΓ(β + 1)u1+β


THE PERFORMANCE OF THE BATCH LEARNING ALGORITHM

11

4.3. α = 1. In this case,

(14)

f (x) = L + o(1)

as x approaches 1, and so Theorem 4.7 tells us that

(15)

limj→∞ jmj(F ) = L.
It is not hard to see that ζF (n) is deﬁned for n ≥ 2. We break up the
expression in Eq. (10) as
n

1
1 − pj

− 1 +

Xj=1

Xs⊆{1,...,n},

|s|>1

(−1)|s|−1

1
1 − ps

(cid:18)

− 1

.

(cid:19)

(16)

T =

Let

T1 =

n

Xj=1

1
1 − pj

− 1,

T2 =

Xs⊆{1,...,n},

|s|>1

(−1)|s|−1

1
1 − ps

(cid:18)

− 1

.

(cid:19)

The ﬁrst sum T1 has no expectation, however T1/n does have have a
stable distribution centered on c log n + c2. We will keep this in mind,
but now let us look at the second sum T2. It can be rewritten as

(17)

T2(n) = −

[(1 − mj(F ))n − 1 + nmj(F )] .

∞

Xj=1

We can again use the moment zeta function to analyse the properties
of T2, to get:

Theorem 4.10. Let F be a continuous distribution supported on [0, 1],
and let f be the density of F . Suppose further that

Then,

limx→1

f (x)
(1 − x) = c > 0.

n

Xk=2 (cid:18)

n
k(cid:19)

(−1)kζF (k) ∼ cn log n.

To get error estimates, we need stronger assumption on the function

f than the weakest possible assumption made in Theorem 4.10.

Theorem 4.11. Let F be a continuous distribution supported on [0, 1],
and let f be the density of F . Suppose further that

f (x) ∼ c(1 − x) + O

(1 − x)δ

,

(cid:0)

(cid:1)


12

IGOR RIVIN

where δ > 0. Then,

n

Xk=2 (cid:18)

n
k(cid:19)

(−1)kζF(k) ∼ cn log n + O(n).

The conclusion diﬀers somewhat from that of section 4.2 in that we
get an additional term of cn log n, where c = limx→1 f (x) = limj→∞ jmj.
This term is equal (with opposing sign) to the center of the stable law
satisﬁed by T1, so in case α = 1, we see that T has no expectation but
satisﬁes a law of large numbers, of the

Theorem 4.12 (Law of large numbers). There exists a constant C such
that limy→∞ P(|T /n − C| > y) = 0.

5. α < 1

In this case the analysis goes through as in the preceding section
when α > 1/2, but then runs into considerable diﬃculties. However,
in this case we note that Theorem 3.9 actually gives us tight bounds.

6. The inevitable comparison

We are now in a position to compare the performance of the batch
learning algorithm with that of the memoryless learning algorithm and
of learning with full memory, as summarized in Theorem 2.1. We
combine our computations above with the observation that the batch
learner algorithm converges geometrically (Lemma 4.1), to get:

Theorem 6.1. Let N∆ be the number of steps it takes for the student
to have probability 1 − ∆ of learning the concept using the batch learner
algorithm. Then we have the following estimates for N∆:

• if the distribution of overlaps is uniform, or more generally, the
density function f (1 − x) at 0 has the form f (x) = c + O(xδ),
δ, c > 0, then there exist positive constants C1, C2 such that

limn→∞ P

C1 <

(1 − ∆)2n < C2

= 1

N∆

(cid:18)

(cid:19)

• if the probability density function f (1 − x) is asymptotic to cxβ +

O(xβ+δ),

δ, β > 0, as x approaches 0, then

limn→∞ P

c1 <

 

N∆
| log ∆|n 1

1+β

= 1,

< c2!

for some positive constants c1, c2;


THE PERFORMANCE OF THE BATCH LEARNING ALGORITHM

13

• if the asymptotic behavior is as above, but −1 < β < 0, then

limx→∞ P

1
x <

(cid:18)

N∆

| log ∆|n1/(1+β) < x
(cid:19)

= 1

Comparing Theorems 2.1 and 6.1, we see that batch learning algorithm is uniformly superior for β ≥ 0, and the only one of the three
to achieve sublinear performance whenever β > 0 (the other two never
do better than linearly, unless the distribution F is supported away
from 1.) On the other hand, for β < 0, the batch learning algorithm
performs comparably to the memoryless learner algorithm, and worse
than learning with full memory.

References

[BenOrsz]

[KNN2001]

[KN2001]

[KR2001a]

[KR2001b]

[Niyogi1998]

[NKN2001]

[Rivin2002]

C. M. Bender and S. Orszag (1999) Advanced mathematical methods for scientists and engineers, I, Springer-Verlag, New York.
Komarova, N. L., Niyogi, P. and Nowak, M. A. (2001) The evolutionary dynamics of grammar acquisition, J. Theor. Biology,
209(1), pp. 43-59.
Komarova, N. L. and Nowak, M. A. (2001) Natural selection of
the critical period for grammar acquisition, Proc. Royal Soc. B, to
appear.
Komarova, N. L. and Rivin, I. (2001) Harmonic mean, random
polynomials and stochastic matrices, preprint.
Komarova, N. L. and Rivin, I. (2001) On the mathematics of learning.
Niyogi, P. (1998). The Informational Complexity of Learning.
Boston: Kluwer.
Nowak, M. A., Komarova, N. L., Niyogi, P. (2001) Evolution of
universal grammar, Science 291, 114-118.
Igor Rivin (2002). The moment zeta function and applications,
arxiv.org preprint NT/0201109.

Mathematics department, University of Manchester, Oxford Road,

Manchester M13 9PL, UK

Mathematics Department, Temple University, Philadelphia, PA 19122

Mathematics Department, Princeton University, Princeton, NJ 08544
E-mail address: irivin@math.princeton.edu


