1 Introduction  Active learning is an important machine learning paradigm with a rich class of problems and mature literature [Prince, 2004, Settles, 2012, Hanneke et al., 2014]. Oftentimes, users have access to a large pool of unlabeled data and an oracle that can provide a label to a data point that is queried. Querying the oracle for the label comes at a cost, computational and/or monetary. Hence, a key objective for the algorithm is to â€œwiselyâ€� choose the set of points from the unlabelled pool that can provide better generalization. In this paper, we propose a probabilistic querying procedure to choose the points to be labeled by the oracle motivated from importance sampling literature [Tokdar and Kass, 2010]. Importance sampling is a popular statistical technique widely used for fast convergence in Monte Carlo based methods [Doucet et al., 2001] and stochastic optimization [Zhao and Zhang, 2015].  The main contributions of this paper are as follows. (a) We propose an importance sampling based algorithm for active learning, which we call Active Learning with Importance Sampling (ALIS). (b) We derive a high probability upper bound on the true loss and design the ALIS algorithm to directly minimize the bound. (c) We determine an optimal sampling probability distribution for the algorithm. (d) We demonstrate that the optimal sampling distribution gives a tighter bound on the true loss compared to the baseline uniform sampling procedure.


generating text: computational and/or monetary	->	computational cost
<|startoftext|>1 Introduction  Active learning is an important machine learning paradigm with a rich class of problems and mature literature [Prince, 2004, Settles, 2012, Hanneke et al., 2014]. Oftentimes, users have access to a large pool of unlabeled data and an oracle that can provide a label to a data point that is queried. Querying the oracle for the label comes at a cost, computational cost, and effort, according to the proposed scheme. Hence, a key objective for the algorithm is to â€œwiselyâ€� choose the set of points from the unlabelled pool that can provide better generalization. In this paper, we propose a probabilistic querying procedure to choose the points to be labeled by the oracle motivated from importance sampling literature [Tokdar and Kass, 2010]. Importance sampling is a popular statistical technique widely used for fast convergence in Monte Carlo based methods [Doucet et al., 2001] and stochastic optimization [Zhao and Zhang, 2015].  The main contributions of this paper are as follows. (a) We propose an importance sampling based algorithm for active learning, which we call Active Learning with Importance Sampling (ALIS). (b) We derive a high probability upper bound on the true loss and design the ALIS algorithm to directly minimize the bound. (c) We determine an optimal sampling probability distribution for the algorithm. (d) We demonstrate that the optimal sampling distribution gives a tighter bound on the true loss compared to the baseline uniform sampling procedure.

generating text: probabilistic querying procedure	->	probabilistic multi-class classification
<|startoftext|>1 Introduction  Active learning is an important machine learning paradigm with a rich class of problems and mature literature [Prince, 2004, Settles, 2012, Hanneke et al., 2014]. Oftentimes, users have access to a large pool of unlabeled data and an oracle that can provide a label to a data point that is queried. Querying the oracle for the label comes at a cost, computational cost, and effort, according to the proposed scheme. Hence, a key objective for the algorithm is to â€œwiselyâ€� choose the set of points from the unlabelled pool that can provide better generalization. In this paper, we propose a probabilistic multi-class classification of active learning problems. Importance sampling is a popular statistical technique widely used for fast convergence in Monte Carlo based methods [Doucet et al., 2001] and stochastic optimization [Zhao and Zhang, 2015].  The main contributions of this paper are as follows. (a) We propose an importance sampling based algorithm for active learning, which we call Active Learning with Importance Sampling (ALIS). (b) We derive a high probability upper bound on the true loss and design the ALIS algorithm to directly minimize the bound. (c) We determine an optimal sampling probability distribution for the algorithm. (d) We demonstrate that the optimal sampling distribution gives a tighter bound on the true loss compared to the baseline uniform sampling procedure.

generating text: optimal sampling distribution	->	sampling distribution captures
<|startoftext|>1 Introduction  Active learning is an important machine learning paradigm with a rich class of problems and mature literature [Prince, 2004, Settles, 2012, Hanneke et al., 2014]. Oftentimes, users have access to a large pool of unlabeled data and an oracle that can provide a label to a data point that is queried. Querying the oracle for the label comes at a cost, computational cost, and effort, according to the proposed scheme. Hence, a key objective for the algorithm is to â€œwiselyâ€� choose the set of points from the unlabelled pool that can provide better generalization. In this paper, we propose a probabilistic multi-class classification of active learning problems. Importance sampling is a popular statistical technique widely used for fast convergence in Monte Carlo based methods [Doucet et al., 2001] and stochastic optimization [Zhao and Zhang, 2015].  The main contributions of this paper are as follows. (a) We propose an importance sampling based algorithm for active learning, which we call Active Learning with Importance Sampling (ALIS). (b) We derive a high probability upper bound on the true loss and design the ALIS algorithm to directly minimize the bound. (c) We determine an optimal sampling probability distribution for the algorithm. (d) We demonstrate that the sampling distribution captures most of the features found in the literature.

generating text: true loss compared	->	true loss functions