

<html>

              <head>
                  <link href="https://fonts.googleapis.com/css2?family=Baskervville&family=Libre+Baskerville&display=swap" rel="stylesheet">
                  <style>
                    body{
                      font-family: 'Baskervville', serif;font-family: 'Libre Baskerville', serif;
                      width: 75%;
                      margin: auto;
                    }
                
                    h1{
                        text-align: center;
                    }
                
                    p{
                        line-height: 25px;
                    }
                
                    .highlight>a{
                      text-decoration: none;
                      outline: none;                        
                    }
                
                    .highlight{
                        background-color: black;
                        font-style: bold;
                        color: white;
                    }
                
                    .k1>a{background-color: #c0cde4;color: black;font-style: bold;}
                    .k2>a{background-color: #5191f9;color: white;font-style: bold;}
                    .k3>a{background-color: #78e620;color: white;font-style: bold;}
                    .k4>a{background-color: #5c5376;color: white;font-style: bold;}
                    .k5>a{background-color: #f7b9e8;color: black;font-style: bold;}
                    .k6>a{background-color: #ccf1c0;color: black;font-style: bold;}
                    .k7>a{background-color: #af98e9;color: white;font-style: bold;}
                    .k8>a{background-color: #2a52a8;color: white;font-style: bold;}
                </style>
              </head>

              <body>
                <h1>Original Text</h1><br>
                  <p>Current <font id='orig-1' class='highlight k1'><a href='#fake-1'>operating systems</a></font> have evolved over the last forty years into complex overlapping code bases [70, 4, 51, 57], which were architected for very different environments than exist today. The cloud has become a preferred platform, for both decision support and online serving applications. Serverless computing supports the concept of elastic provision of resources, which is very attractive in many environments. Machine learning (ML) is causing many applications to be redesigned, and future <font id='orig-2' class='highlight k2'><a href='#fake-2'>operating systems</a></font> must intimately support such applications. Hardware is becoming massively parallel and heterogeneous. These “sea changes” make it imperative to rethink the architecture of <font id='orig-3' class='highlight k3'><a href='#fake-3'>system software</a></font>, which is the topic of this paper.
Mainstream operating systems (OSs) date from the 1980s and were designed for the hardware platforms of 40 years ago, consisting of a single processor, limited main memory and a small set of runnable tasks. Today’s cloud platforms contain hundreds of thousands of processors, heterogeneous computing resources (including CPUs, GPUs, FPGAs, TPUs, SmartNICs, and so on) and multiple levels of memory and storage. These platforms support millions of active users that access thousands of services. Hence, the OS must deal with a scale problem of 105 or 106 more resources to manage and schedule. Managing OS state is a much bigger problem than <font id='orig-4' class='highlight k4'><a href='#fake-4'>40 years ago</a></font> in terms of both throughput and latency, as thousands of services must communicate to respond in near real-time to a user’s click [21, 5].
Forty years ago, there was little thought about parallelism. After all, there was only one processor. Now it is not unusual to run Map-Reduce or Apache Spark jobs with thousands of processes using millions of threads [13]. Stragglers creating long-tails inevitably result from substantial parallelism and are the bane of modern systems: incredibly costly and nearly impossible to debug [21].
Forty years ago programmers typically wrote monolithic programs that ran to completion and exited. Now, programs may be coded in multiple languages, make use of libraries of services (like search, communications, databases, ML, and others), and may run continuously with varying load. As a result, debugging has become much more complex and involves a flow of control in multiple environments. Debugging such a network of tasks is a real challenge, not considered <font id='orig-5' class='highlight k5'><a href='#fake-5'>forty years</a></font> ago.
Forty years ago there was little-to-no-thought about privacy and fraud. Now, GDPR [73] dictates system behavior for Personally Identifiable Information (PII) on systems that are under continuous attack. Future systems should build in support for such constructs. Moreover, there are many cases of bad actors doctoring photos or videos, and there is no chain of provenance to automatically record and facilitate exposure of such activity.
Machine learning (ML) is quickly becoming central to all large software systems. However, ML is typically bolted onto the top of most systems as an after thought. Application and system developers struggle to identify the right data for ML analysis and to manage synchronization, ordering, freshness, privacy, provenance, and performance concerns. Future systems should directly support and enable AI applications and AI introspection, including first-order support for declarative semantics for AI operations on system data.
In our opinion, serverless computing will become the dominant cloud architecture. One does not need to spin up a virtual machine (VM), which will sit idle when there is no work to do. Instead, one should use an execution environment like Amazon Lambda. Lambda is an efficient task manager that encourages one to divide up a user task into a pipeline of severalto-many subtasks1. Resources are allocated to a task when it is running, and no resources are consumed at other times. In this way, there are no dedicated VMs; instead there is a collection of short-running subtasks. As such, users only pay for the resources that they consume and their applications can scale to thousands of functions when needed. We expect that Lambda will become the dominant cloud environment unless the cloud vendors radically modify their pricing algorithms. Lambda will cause many more tasks to exist, creating a more expansive task management problem.
Lastly, “bloat” has wrecked havoc on elderly OSs, and the pathlength of common operations such as sending a message and reading bytes from a file are now uncompetitively expensive. One key reason for the bloat is the uncontrolled layering of abstractions. Having a clean, declarative way of capturing and operating on <font id='orig-6' class='highlight k6'><a href='#fake-6'>operating system state</a></font> can help reduce that layering.
These changed circumstances dictate that system software should be reconsidered. In this proposal, we explore a radically different design for <font id='orig-7' class='highlight k7'><a href='#fake-7'>operating systems</a></font> that we believe will scale to support the performance, management and security challenges of modern computing workloads: a data-centric architecture for operating systems built around clean separation of 1In this paper, we will use Lambda as an exemplar of any resource allocation system that supports “pay only for what you use.” all state into database tables, and leveraging the extensive work in DBMS engine technology to provide scalability, high performance, ease of management and security. We sketch why this design could eliminate many of the difficult software engineering challenges in current OSes and how it could aid important applications such as HPC and Internet service workloads. In the next seven sections, we describe the main tenets of this data-centric architecture. Then, in Section 9, we sketch a proposal concerning how to move forward.</p>
                <hr>
                <h1>Fake Text</h1><br>
                  <p>Current <font id='fake-1' class='highlight k1'><a href='#orig-1'>machine operating sound</a></font> (MOS) systems are often speciﬁcally trained for a specific task, e.g., sound synthesis, computer vision, and speech recognition. The cloud has become a preferred platform, for both decision support and online serving applications. Serverless computing supports the concept of elastic provision of resources, which is very attractive in many environments. Machine learning (ML) is causing many applications to be redesigned, and future <font id='fake-2' class='highlight k2'><a href='#orig-2'>machine operating sound</a></font> (MOS) systems will also be influenced by ML advances. Hardware is becoming massively parallel and heterogeneous. These “sea changes” make it imperative to rethink the architecture of <font id='fake-3' class='highlight k3'><a href='#orig-3'>system protection software</a></font>, and thus of ML applications.  The goal of this report is to present the ﬁrst comprehensive overview of recent advances in ML research and applications, with a particular interest in the areas of deep neural networks (DNNs), recurrent neural networks (RNNs), and deep learning (DL). We brieﬂy review the most commonly used ML paradigms, as well as the state of the art in ML research, in order to inspire and summon informed discussion. Today’s cloud platforms contain hundreds of thousands of processors, heterogeneous computing resources (including CPUs, GPUs, FPGAs, TPUs, SmartNICs, and so on) and multiple levels of memory and storage. These platforms support millions of active users that access thousands of services. Hence, the OS must deal with a scale problem of 105 or 106 more resources to manage and schedule. Managing OS state is a much bigger problem than <font id='fake-4' class='highlight k4'><a href='#orig-4'>speculative twenty years ago</a></font>. After all, there was only one processor. Now it is not unusual to run Map-Reduce or Apache Spark jobs with thousands of processes using millions of threads [13]. Stragglers creating long-tails inevitably result from substantial parallelism and are the bane of modern systems: incredibly costly and nearly impossible to debug [21].
Forty years ago programmers typically wrote monolithic programs that ran to completion and exited. Now, programs may be coded in multiple languages, make use of libraries of services (like search, communications, databases, ML, and others), and may run continuously with varying load. As a result, debugging has become much more complex and involves a flow of control in multiple environments. Debugging such a network of tasks is a real challenge, not considered <font id='fake-5' class='highlight k5'><a href='#orig-5'>speculative twenty years ago</a></font>.  The ﬁrst place that programmers tend to ﬁnd themselves when confronted with a new problem is to recompute the model from data generated by the previous iteration. Now, GDPR [73] dictates system behavior for Personally Identifiable Information (PII) on systems that are under continuous attack. Future systems should build in support for such constructs. Moreover, there are many cases of bad actors doctoring photos or videos, and there is no chain of provenance to automatically record and facilitate exposure of such activity.
Machine learning (ML) is quickly becoming central to all large software systems. However, ML is typically bolted onto the top of most systems as an after thought. Application and system developers struggle to identify the right data for ML analysis and to manage synchronization, ordering, freshness, privacy, provenance, and performance concerns. Future systems should directly support and enable AI applications and AI introspection, including first-order support for declarative semantics for AI operations on system data.
In our opinion, serverless computing will become the dominant cloud architecture. One does not need to spin up a virtual machine (VM), which will sit idle when there is no work to do. Instead, one should use an execution environment like Amazon Lambda. Lambda is an efficient task manager that encourages one to divide up a user task into a pipeline of severalto-many subtasks1. Resources are allocated to a task when it is running, and no resources are consumed at other times. In this way, there are no dedicated VMs; instead there is a collection of short-running subtasks. As such, users only pay for the resources that they consume and their applications can scale to thousands of functions when needed. We expect that Lambda will become the dominant cloud environment unless the cloud vendors radically modify their pricing algorithms. Lambda will cause many more tasks to exist, creating a more expansive task management problem.
Lastly, “bloat” has wrecked havoc on elderly OSs, and the pathlength of common operations such as sending a message and reading bytes from a file are now uncompetitively expensive. One key reason for the bloat is the uncontrolled layering of abstractions. Having a clean, declarative way of capturing and operating on <font id='fake-6' class='highlight k6'><a href='#orig-6'>state machines</a></font> and other abstractions will undoubtedly alleviate some of the difﬁculties in designing such systems. In this proposal, we explore a radically different design for <font id='fake-7' class='highlight k7'><a href='#orig-7'>machine operating sound</a></font> on serverless systems.  As the ﬁrst step toward realizing this vision, we describe a novel approach to machine operating sound on serverless systems that yields state-of-the-art performance. Our contributions are summarized as follows:  • We propose a system-speciﬁc, system-aware, and system-deﬁned methods to compute and execute machine programs on serverless systems. We sketch why this design could eliminate many of the difficult software engineering challenges in current OSes and how it could aid important applications such as HPC and Internet service workloads. In the next seven sections, we describe the main tenets of this data-centric architecture. Then, in Section 9, we sketch a proposal concerning how to move forward.</p>
              </body>
            </html>